[{"content":"\n在 macOS 上进行开发，经常会遇到这样的问题：不同的项目依赖不同版本的 Python、Node.js 或 Java，而 macOS 自带的系统版本往往过时且权限受限。如果直接安装全局版本，很容易弄脏系统环境。\n因此，版本隔离 和 工具链管理 是每个 macOS 开发者的必修课。\n本文档旨在作为一份速查表 (Cheat Sheet)，汇总了 macOS 上最主流的包管理工具（Homebrew）以及各语言的专用版本管理工具（nvm, uv, sdkman）的常用命令，涵盖安装、版本切换、以及工具自身的维护更新。\n系统级包管理：Homebrew Homebrew 是 macOS 的必备工具，主要用于管理系统级工具（如 wget, ffmpeg）和 GUI 应用（Cask）。\n记住，只用它装工具，不要用它直接装语言环境（如 Python, Node），否则多版本切换会很麻烦。\n核心操作：安装与卸载 最基础的增删查改。\n场景 命令 说明 安装软件 brew install \u0026lt;包名\u0026gt; 例如 brew install wget 安装 App brew install --cask \u0026lt;软件名\u0026gt; 例如 brew install --cask picgo 卸载软件 brew uninstall \u0026lt;包名\u0026gt; 删除已安装的包，保留配置文件 彻底卸载 brew uninstall --zap \u0026lt;软件名\u0026gt; (Cask 专用) 卸载应用并尝试删除相关的配置文件/偏好设置 查看已安装 brew list 列出所有安装的包 搜索软件 brew search \u0026lt;关键词\u0026gt; 进阶管理：依赖与清理 随着时间推移，系统中会残留很多不再使用的“孤儿依赖”，这一部分命令用于保持系统干净。\n场景 命令 说明 查看依赖树 brew deps --tree \u0026lt;包名\u0026gt; 推荐。以树状图显示该软件依赖了哪些库，清晰直观。 查看被谁依赖 brew uses --installed \u0026lt;包名\u0026gt; 反向查询：查看这个包被谁用到了（防止误删重要组件）。 自动移除 brew autoremove 重要。自动卸载那些“作为依赖被安装但现在不再需要”的孤儿包。 清理缓存 brew cleanup 删除旧版本的安装包（.dmg/.tar.gz）和临时文件，释放磁盘空间。 系统维护：更新与诊断 场景 命令 说明 更新 Brew brew update 更新 Homebrew 自身和配方库（Formulae）。 升级软件 brew upgrade 将已安装的所有软件升级到最新版。 系统体检 brew doctor 排错首选。检查系统环境、权限、路径冲突等问题，并给出修复建议。 查看信息 brew info \u0026lt;包名\u0026gt; 查看软件的版本、安装路径、依赖关系及注意事项（Caveats）。 Node.js 管理 参考官方文档，使用 nvm 和 npm 方式安装 Node.js。nvm 管理 Node 版本，而 Node 版本内部管理着自己的 npm 版本。\nNode.js 版本控制 (nvm) 场景 命令 说明 安装版本 nvm install \u0026lt;版本号\u0026gt; 例如 nvm install 24 (最新 v24) 或 nvm install --lts 切换版本 nvm use \u0026lt;版本号\u0026gt; 仅在当前终端窗口生效，例如 nvm use 24 设置默认 nvm alias default \u0026lt;版本号\u0026gt; 重要：设置新开终端时的默认 Node 版本 查看远程 nvm ls-remote 列出所有可供安装的 Node 版本 查看本地 nvm ls 列出本机已安装的版本 卸载版本 nvm uninstall \u0026lt;版本号\u0026gt; 删除不再使用的版本及其全局包 npm 自身管理 (升级与维护) 当你安装 Node 时，它带有一个默认的 npm。但 npm 自身更新频率很高，可以通过手动升级。\n注意： npm 的升级是绑定在当前 Node 版本下的。如果你切换了 Node 版本，npm 版本也会变回那个 Node 自带的版本。\n场景 命令 说明 升级 npm (最新) npm install -g npm@latest 推荐。将当前 Node 环境下的 npm 升到最新版。 升级 npm (指定) npm install -g npm@10.9.3 降级或锁定特定版本时使用。 查看版本 npm -v 查看当前正在使用的 npm 版本。 全局包管理 (Global Packages) 虽然我们推荐项目依赖都装在本地 (npm install)，但某些 CLI 工具 (如 yarn, pnpm, typescript, nest) 可能需要全局安装。\n场景 命令 说明 查看全局包 npm list -g --depth 0 检查全局装了哪些工具，避免混乱。 安装全局包 npm install -g \u0026lt;包名\u0026gt; 无需 sudo。在 nvm 环境下，权限属于当前用户。 卸载全局包 npm uninstall -g \u0026lt;包名\u0026gt; 迁移全局包 nvm install \u0026lt;新版本\u0026gt; --reinstall-packages-from=\u0026lt;旧版本\u0026gt; 安装新 Node 时，自动把旧 Node 的全局包也装一份过来。 nvm 自身升级 nvm 不支持 nvm update 这种命令。升级它本质上就是下载最新的脚本覆盖旧脚本。\n场景 命令 说明 升级步骤 curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash 必须手动指定最新版本号。运行后重启终端即可。 检查版本 nvm --version 确认当前 nvm 的版本。 注意：\n升级 nvm 不会 删除你已经安装的 Node.js 版本或全局包，放心执行。 请务必去 nvm GitHub 查看最新的版本号（上面的 v0.40.3 只是示例）。 Python 管理：uv uv 是一个用 Rust 编写的 Python 打包和项目管理器，它将多个 Python 工具（如 pip、venv、pyenv 等）的功能整合到一个工具中，提供了一个统一、高效的 Python 开发流程。\n我曾经写过一篇技术博客《Python 项目管理最佳实践：uv 使用指南》，已经覆盖 uv 的常用场景，这里只强调 uv 自身的升级：\n1 2 # 更新 uv 自身到最新版 uv self update Java: SDKMAN! 对于 Java 开发者，SDKMAN! 是管理 JDK (Java Development Kit) 以及 Maven, Gradle, Kotlin 等 JVM 生态工具的神器。它彻底解决了配置 JAVA_HOME 的痛苦。\n常用命令速查 操作 命令 说明 列出可用版本 sdk list java 查看所有厂商（Temurin, GraalVM 等）的版本 安装版本 sdk install java 21.0.2-tem 安装特定版本 切换版本 (临时) sdk use java 17.0.10-tem 仅当前终端有效 设置默认版本 sdk default java 21.0.2-tem 全局永久生效 查看当前版本 sdk current java 查找安装路径 sdk home java \u0026lt;版本号\u0026gt; 如果 IDE 需要指定 JDK 路径时很有用 维护与更新 SDKMAN 拥有完善的自更新机制：\n1 2 3 4 5 # 1. 更新 SDKMAN 客户端自身 sdk selfupdate # 2. 更新元数据（获取最新的 Java 版本列表） sdk update 总结 在 macOS 上保持环境整洁的秘诀就是：各司其职。\nBrew: 管系统工具。 NVM: 管 Node.js。 uv: 管 Python。 SDKMAN: 管 Java。 掌握以上命令，基本能覆盖 90% 的日常开发环境维护需求。建议将此文加入书签，随时查阅。\n","date":"2026-02-13T15:28:25+08:00","permalink":"https://mahaoliang.tech/p/macos-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86/","title":"macOS 开发环境管理"},{"content":"2022 年 7 月 23 日，高二暑假，我在自己的第一台 MacBook Pro（M1） 上，发布了第一篇博客《打造一个漂亮的命令行终端》 ，四个月后，也就是 2022 年 11 月 30 日，改变世界的ChatGPT 发布。那时候完全不会想到，AI 的发展竟然对终端的使用产生了极大的影响。那时我使用 macOS 上原生 terminal，Oh My Zsh + Powerlevel10k 的组合配置，足够漂亮，足够满足我在终端运行 git 命令，ssh 登录云服务器等操作。\n2025 年 2 月 25 日，Claude Code 低调发布。经过两年的飞速发展，大家似乎已经习惯了 AI 可以做出任何令人震惊的飞跃。在编程领域，从最初的代码补全，进化到了 agentic coding，一开始只是用 AI 生成 demo App，然后在社交媒体上炫耀自己用一句话生成的贪吃蛇游戏，严肃的程序员对 AI 编程持有批判怀疑态度。\n2026 年初，形势悄然发生转变。\nRuby on Rails的创造者，知名程序员 DHH(David Heinemeier Hansson) 在 2025 年中的时候还认为“AI can’t code”，现在改变了看法，在最新的博客《Promoting AI agents》中，DHH 认为在 2025 年 AI agent 产生跨越式发展，使用终端工具（Claude Code) 和 AI 协作式开发，已经能写生产级代码。\nRedis 的作者，多才多艺的顶尖程序员 Salvatore Sanfilippo 也在最新的博客《Don\u0026rsquo;t fall into the anti-AI hype》中表达，AI 将永远彻底改变编程，而且比他预期的快得多。对大多数项目而言，亲自写代码已不再是明智选择（除非只为乐趣）。antirez 亲自实践的证据，几小时内完成了原本需要数周的 4 个任务：修改 linenoise 库支持 UTF-8 并构建测试框架；修复 Redis 测试中时序竞争/死锁等偶发故障；5 分钟生成 700 行纯 C BERT 嵌入推理库（性能仅比 PyTorch 低 15%）；20 分钟内让 AI 复现自己对 Redis Streams 的内部修改。\n最令人震惊的，是 Linux 之父，传奇程序员 Linus Torvalds 在上周末发布了自己的 Vibe Coding 项目AudioNoise ，使用 Google 的 Antigravity 实现音频样本的可视化功能。在提交信息中，Linus Torvalds 表示：“Is this much better than I could do by hand? Sure is.”\n程序大神们都已经拥抱 AI coding，我们还有什么理由拒绝呢。\n但这和终端有什么关系？因为目前最好用的 agentic coding 工具仍然是Claude Code，或者有人认为是其开源平替版 OpenCode，它们都是命令行终端工作方式，程序员从每天面对 IDE，变成每天大多数时间都在终端中工作。原先的软件选择和设置，已经不能满足当前高强度使用的需求。\n例如 macOS 原生 terminal，仅支持 ANSI 16 色与扩展 256 色，不支持 24 位真彩（True Color），渲染方式为 CPU 绘制，无 GPU 加速，大量输出或快速滚动时易出现卡顿。而现代终端，真彩和 GPU 加速都是基本功能。既然每天都要面对终端工作，当然不能凑合，必须找一个颜值高，使用流畅的现代终端。\n原生 terminal 运行真彩测试的效果：\nGhostty 中运行真彩测试的效果：\n铺垫了这么多，终于要进入正题了，打造 2026 面向 agentic coding 的终端开发环境。\n字体 首先准备基础环境，安装必要的字体。\nJetBrainsMono Nerd Font Nerd Fonts 是一个面向开发者的图标集合，包含了 10390+ 个来自主流图标集的图标。使用 Nerd Fonts，我们可以在单调的文本终端，显示丰富的图标，美观不输 GUI 界面，而且极具极客范儿。\nJetBrains Mono 非常流行，也是我最喜欢的编程字体，将 JetBrains Mono 和 Nerd Fonts 打包在一起，就得到新的字体 JetBrainsMono Nerd Font，兼顾编程可读性和图标可视化需求。nerdfonts.com 提供了 Nerd Fonts和各种流行编程字体的打包字体，找到 JetBrainsMono Nerd Font，下载并安装。\nSarasa Term SC 使用 Claude code 时必然要用到中文和 AI 交流，默认会使用系统自带的中文字体。macOS 上默认中文字体是 PingFang SC，虽然美观大方，但它不是等宽字体，和英文混排时，在终端上会显得有些凌乱。所以我们需要一款面向终端和代码编辑器的等宽中文字体。\nSarasa Gothic 是一款开源的多语言复合字体，基于 Inter、Iosevka 和 思源黑体（Source Han Sans） 三款经典字体整合而成，适配中日韩等多语言混排场景。Sarasa Gothic发布了一系列字体，其中 Sarasa Term SC 是面向终端（Term）的等宽（Mono）简体中文（SC）字体，强制保证 中文宽度 = 2 × 英文宽度，适合在终端显示中英文混排内容。\n在 Sarasa Gothic 的 relase 页面，Single Family \u0026amp; Language TTF Package 表格中，找到 SC 行，Term 列对应的字体，下载并安装。\nGhostty 现代终端我选择了 Ghostty。除了真彩、GPU 加速等基本特性外，说一个 agentic coding 的刚需：给 Claude code 布置完任务后，它吭哧吭哧在那儿干活，你刷会儿微博娱乐。Claude code 完成当前任务后，需要等你下一步的指示，这时 Ghostty 会自动通知提醒，太有用了。\n理解 AI agent 说个题外话，如果你对 AI agent 没什么概念，用一下 Claude code 你马上就会明白。AI，或者说大语言模型，本质还是根据你的输入，预测下一个 token。最初的 AI 使用方式，就是对话，你输入提示词，AI 回答。具体到编程领域，你描述要求，AI 完成一段代码。但这样工作效率太低了，agent 出现了。\n你可简单认为 agent 就是具有某种技能的员工，你想做一个 App，产品经理 agent 会和你讨论，明确需求和功能，输出产品功能描述文档，然后架构师根据产品文档，拆分任务，输出具体实现方案，程序员根据实现方案，开始编码干活，测试也是根据产品文档，对程序员的输出进行测试。除了第一步和产品经理讨论需要你参与，明确产品功能后，后面的角色，都会根据自己的职责，自动干活。相当于你雇佣了一堆人（agents），分工协作，听你指挥，帮你干活。\n现在的 AI coding，早已不是一问一答，帮你完成个函数/测试，已经具备了一定的自主性。你像一个老板，去指挥这些 agents 干活。甚至现在已经有了手机上使用 Claude code 的方案，例如 happy。想象一下，下班前你布置好任务，然后去健身，agents 开始干活，在完成了一个阶段的任务后，agents 需要请示老板，这时你手机收到通知，你看了一眼 AI 的汇报，然后下达下一步的指示。你也可以告诉它：今晚不要再打扰我，请根据计划，通宵工作，明早我检查成果。\nAI agent 就是听你指挥，协作帮你完成任务的员工。\nGhostty 配置 Ghostty 信奉 Zero Configuration 理念，但我还是做了少量配置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 font-family = \u0026#34;\u0026#34; font-family = \u0026#34;JetBrainsMono Nerd Font Mono\u0026#34; font-family = \u0026#34;Sarasa Term SC\u0026#34; font-size = 16 font-thicken = true window-height = 40 window-width = 160 window-padding-x = 5 theme = \u0026#34;TokyoNight\u0026#34; macos-titlebar-style = tabs macos-option-as-alt = true background-opacity = 0.95 background-blur = true shell-integration-features = ssh-terminfo,ssh-env,sudo 主字体设置为 JetBrainsMono Nerd，Ghostty 会首先尝试使用 JetBrainsMono Nerd 来渲染字符，当 Ghostty 遇到一个 JetBrainsMono Nerd 里面没有的字符，比如汉字，它会去查看配置列表里的下一个字体，即 Sarasa Term SC。\nShell 仍然选择 zsh + Oh My Zsh，安装配置不变，请参考我以前的博客《打造一个漂亮的命令行终端》和 《Zsh 的安装和配置》。\n命令行提示 命令行提示原先一直使用 Powerlevel10k，最近发现更好的替代者 Starship。\nPowerlevel10k 已经不再更新，进入维护阶段，而 Starship 还非常活跃，同时 Starship 支持 Linux、MacOS、Windows 等多个平台，这样我可以在各个平台上获得一致的体验。\n先禁用 Powerlevel10k，并设置 ZSH_THEME=\u0026ldquo;bira\u0026rdquo;\n安装 1 curl -sS https://starship.rs/install.sh | sh 主题配置 主题选择 Gruvbox Rainbow，使用下面的命令安装主题：\n1 starship preset gruvbox-rainbow -o ~/.config/starship.toml 同时，我希望对主题做一些自定义配置。打开 ~/.config/starship.toml 文件，找到 [directory] 配置，修改设置如下：\n1 2 3 truncation_length = 8 truncation_symbol = \u0026#34;…/\u0026#34; truncate_to_repo = false 目录超过 8 层再截断。\n默认情况下，如果是 Git 仓库，路径显示会自动截断使用 Git 仓库的根目录，我想看到完整的路径，所以设置 truncate_to_repo 为 false。\n激活主题 在 ~/.zshrc 文件最后添加以下内容：\n1 2 3 4 # starship if [[ \u0026#34;$TERM_PROGRAM\u0026#34; != \u0026#34;Apple_Terminal\u0026#34; ]]; then eval \u0026#34;$(starship init zsh)\u0026#34; fi 由于 StarShip 需要真彩支持，所以只有使用非 macOS 原生 terminal 才加载 StarShip。最终的效果如下：\n可以参考我的 Oh My Zsh 完整配置。\nVS Code 配置 由于命令行提示符使用了 Starship，为了避免 VS Code 的终端出现乱码，需要设置VS Code的终端字体。打开配置文件 Settings.json中，增加或修改为如下配置：\n1 \u0026#34;terminal.integrated.fontFamily\u0026#34;: \u0026#34;JetBrainsMono Nerd Font Mono\u0026#34;, 另外，推荐两个 VS Code 主题：\nCatppuccin，编码时选择 Catppuccin Mocha，显示清晰，同时也不刺眼。\nFlexoki，Obsidian 创始人 @kepano 创建的配色方案，宣纸的淡黄色，特别适合文字编辑。\nJetbrains IDE 主题 Catppuccin 主题支持几乎所有的终端和代码编辑器，当然少不了 Jetbrains 的 IDE，同样可以选择 Catppuccin 主题。\ntmux 配置 原先隐约知道 tmux 这个工具，是为了长时间运行命令，即使断开 SSH 连接，命令也不会退出，下次连上去还可以恢复。\n到了 agentic coding 时代，我突然明白了 tmux 的价值。想象这个场景：在实验室 ssh 连接到 Linux 服务器上使用 Claude Code 干活，离开时给它布置一个长任务，然后从 tmux session detach。吃饭完回到宿舍，使用自己的 MacBook 再次 ssh 到 Linux 服务器，tmux session attach，恢复 Claude Code 现场，看看它干的怎么样了，是否需要下一步指导。无法切换地点，不中断开发，太有用了！\n对于 tmux，我同样选择了 Flexoki 主题，并增加了如下配置：\n1 2 3 4 5 6 7 8 # 设置 tmux 会话内部默认使用的终端类型为 tmux-256color set -g default-terminal \u0026#34;tmux-256color\u0026#34; # 强制 tmux 启用「真彩色（True Color）」支持 set -ag terminal-overrides \u0026#34;,*:RGB\u0026#34; # 启用 tmux 的鼠标支持 set -g mouse 打开 tmux 的配置文件 ~/.tmux.conf，先复制 Flexoki 主题的内容，然后在末尾增加上面的配置。可以参考我的完整配置。\n全文完，希望对你有帮助，让我们在现代终端环境，愉快的 agentic coding！\n","date":"2026-01-13T21:47:28+08:00","permalink":"https://mahaoliang.tech/p/%E6%89%93%E9%80%A0%E7%BB%88%E7%AB%AF%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%832026-agentic-coding-%E7%89%88/","title":"打造终端开发环境（2026 agentic coding 版）"},{"content":"\n1. 核心概念：套娃结构 Tmux 的结构分为三层，理解这个就不会迷路：\nSession (会话)：最高层级。对应一个“项目”或“任务”。 场景：你可以有一个叫 work 的会话跑公司代码，一个叫 blog 的会话维护个人博客。断开 SSH 后，Session 依然在后台存活。 Window (窗口)：中间层级。相当于浏览器的“标签页”。 场景：在一个 Session 里，Window 1 跑 Claude Code，Window 2 跑 git 命令，Window 3 看日志。 Pane (窗格)：最底层。相当于一个屏幕被切分成几块。 场景：左边大屏跑 Claude Code，右边小屏实时看 top 监控性能。 2. Cheat Sheet 所有快捷键都需要先按 前缀键 (Prefix)，默认是 Ctrl + b。 (操作方式：按下 Ctrl 和 b，松开，然后按具体功能键)\nA. 会话管理 (最常用) 命令 作用 备注 tmux new -s \u0026lt;name\u0026gt; 新建带名字的会话 强烈推荐，不要直接敲 tmux tmux a -t \u0026lt;name\u0026gt; 重连 (Attach) 会话 回家继续工作的核心命令 tmux ls 列出所有后台会话 看看有哪些活着的会话 Prefix + d 剥离 (Detach) 暂时离开，程序后台继续跑 Prefix + s 交互式选择会话 弹出一个列表让你选，很直观 B. 窗格 (分屏) 操作 快捷键 (Prefix +) 作用 记忆法 % (Shift+5) 左右切分 看起来像把屏幕切两半 \u0026quot; (Shift+') 上下切分 引号像一上一下 方向键 在窗格间移动光标 z 最大化/还原当前窗格 神器。想专注写代码时按一下，写完再按一下还原 x 关闭当前窗格 会询问 y/n C. 窗口 (标签页) 操作 快捷键 (Prefix +) 作用 c 新建窗口 (Create) n / p 切换到 下一个/上一个 窗口 (Next/Prev) w 图形化选择窗口 (像 Exposé) , 重命名当前窗口 (方便记忆) D. 滚动与复制 因为你开启了 set -g mouse on：\n滚动： 直接用鼠标滚轮即可。 复制： Tmux 内部复制： 鼠标左键拖动选中（会自动复制到 tmux 剪贴板）。 系统级复制 (macOS)： 按住 Option (Alt) 键，然后用鼠标选中，直接 Cmd+C。 3. 最佳实践 (Best Practices) 1. 永远给 Session 起名字 ❌ 坏习惯：直接输入 tmux。你会得到 session 0, session 1，过两天你就不知道哪个里面跑着 Claude Code 了。 ✅ 好习惯： 1 2 tmux new -s backend # 开发后端 tmux new -s monitor # 监控服务器 2. 利用好“临时最大化” (Prefix + z) 不要手动去拖动调整分屏的大小，太慢了。\n场景：你在一个小窗格里看日志，突然报错了一大堆。 操作：按 Prefix + z 全屏看，看完再按 Prefix + z 缩回去。 3. 嵌套使用的原则 尽量避免“在 tmux 里再开一个 tmux”（Inception 模式），那会非常痛苦（按键冲突）。\n如果你 ssh 到跳板机，再从跳板机 ssh 到目标机器。建议只在最终的目标机器上运行 tmux。 4. 结合 Claude Code 的工作流 由于 Claude Code 是交互式的，建议这样布局：\nWindow 1 (\u0026ldquo;Code\u0026rdquo;):\n左侧大窗格 (70%)：运行 claude。\n右侧小窗格 (30%)：运行 git status 或简单的 shell 命令，方便随时看状态。\nWindow 2 (\u0026ldquo;Server\u0026rdquo;):\n运行项目主进程（如 npm run dev 或 python app.py），看实时日志。\n总结 SSH 上去。 tmux new -s work。 开始用 Claude Code 搬砖。 想分屏看日志？Ctrl+b 然后 %。 下班了？Ctrl+b 然后 d。 回家？tmux a -t work。 ","date":"2026-01-08T22:31:54+08:00","permalink":"https://mahaoliang.tech/p/tmux-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/","title":"Tmux 快速入门指南"},{"content":"今年暑假最大的收获就是拿到了驾照，然后就是参加了开源之夏 (Summer OSPP) 。经过两个月的开发，今天终于可以为 面向 openEuler distroless 镜像的 SDF 自动生成工具开发 项目提交了结项报告了。一个合并了 7 个 PR，完全超出了我的预期。\n下面是我的结项报告。\n结项报告 学生姓名： 马浩量 项目编号： 25b970448 项目信息 项目名称：面向 openEuler distroless 镜像的 SDF 自动生成工具开发\n方案描述： 本项目旨在为 openEuler 的 splitter 工具开发一个全新的 gen 命令，以实现 Slice Definition File (SDF) 的自动化生成。当前所有用于构建 Distroless 镜像的 SDF 文件均需手工编写，效率低而且容易出错。\n本方案通过设计一套自动化流水线，为任意 RPM 包生成高质量的 SDF 草稿。其核心流程包括：在隔离的 Docker 环境中动态构建一个依赖完备的分析环境，然后对目标 RPM 包进行智能文件分类和精确依赖分析，最终产出结构清晰、内容准确的 SDF YAML 文件。此工具旨在将社区开发者从繁琐的重复性工作中解放出来，加速 openEuler Distroless 生态的建设。\n时间规划：\n第一阶段 (7 月 1 日 - 7 月 15 日 ): 熟悉与框架搭建，研究 splitter 和 slice-releases 现有机制，分析手工 SDF 的设计模式，完成 gen 命令的基础框架。 第二阶段 (7 月 16 日 - 7 月 31 日): 核心功能实现。实现基于 file 命令的精确文件分类器，基于 readelf 的依赖分析器。完成路径压缩、内部依赖注入等 SDF 优化功能。 第三阶段 (8 月): 响应导师建议，引入 Docker 隔离环境，编写 Dockerfile 和 splitter-docker.sh 运行脚本。向 openEuler 官方镜像仓库贡献 splitter 镜像。 第四阶段 (8 月底): 完善 splitter 项目的 README.md，更新 openEuler Distroless 官方文档，撰写技术博客与结项报告。 项目总结 已完成工作 对照项目申请书的方案，我完成了预定任务，主要工作成果如下：\n新增 splitter gen 命令: 成功为 splitter 工具增加了 gen 子命令，提供了从无到有的 SDF 自动化生成能力。 对应 PR: !19 (已合并) 实现文件分类器: 基于对现有 SDF 的分析，建立了规则引擎，可识别 _bins, _libs, _config, _copyright 等核心 slice。 引入 file -L 命令进行文件类型校验，确保只有真正的 ELF 文件才会被归入 _bins 或 _libs，能正确处理脚本和符号链接。 实现依赖分析器: 采用“解析 -\u0026gt; 定位 -\u0026gt; 溯源”的三步策略，通过 readelf, ldconfig, rpm -qf 工具链，自动化分析 ELF 文件的外部依赖。 实现了内部依赖（如 _bins 对 _config）的自动注入，使生成的 SDF 更符合设计惯例。 SDF 写入器: 实现路径压缩算法，能将版本化的库文件路径（如 libfoo.so.1, libfoo.so.1.2.3）自动合并为 libfoo.so.1*，SDF 产出质量接近手工水平。 引入 Docker 沙箱化分析环境: 响应导师建议，实现了基于官方 openeuler/splitter 镜像的分析流程，解决了对宿主环境的依赖和污染问题。 贡献了官方 splitter 镜像: 编写了 Dockerfile ，将 splitter 工具镜像贡献至 openEuler 官方容器镜像仓。 对应 PR: !866 (已合并)， !1023 (已合并) 提供入口脚本 (splitter-docker.sh): 编写了一个用户友好的 splitter-docker.sh 脚本，封装了所有 Docker 操作，实现了cut和gen命令的“一键运行”。 对应 PR: !20 (已合并) 完善的文档: 更新了 splitter 项目的 README.md，介绍了 gen 命令的原理和使用方法。 对应 PR: !21 (已合并) ，!23(已合并) 更新了 openEuler Distroless 镜像官方文档，将 SDF 的获取流程，更新为使用本工具的自动化流程。 对应 PR: !1061 (已合并) 遇到的问题及解决方案 在本次项目中，遇到的主要挑战及解决方案如下：\n分析过程对环境的依赖和污染\n我的初始方案为了解决依赖问题，需要在运行工具的宿主系统上，通过dnf install安装待分析的软件包及其依赖。这个过程虽然能让分析跑通，但它会直接修改用户的系统环境，存在污染系统的风险。\n解决方案：在导师提出“使用 chroot 或容器技术隔离环境”的建议后，我最终选择了更加标准的 Docker 方案。基于官方的应用镜像，为每次分析启动一个干净的容器。在容器内部执行dnf install，安装待分析的包，在隔离环境中执行 SDF 的生成。分析结束后，直接销毁容器。\n如何安全、准确地分析二进制依赖？\n最初考虑使用ldd，但发现它存在执行代码的安全风险。\n解决方案：经过研究，我选择了readelf -d作为核心分析工具。它作为静态分析器，只关注我们需要的直接依赖。\n如何处理脚本、符号链接等“伪可执行文件”\nreadelf无法处理非 ELF 文件，导致分析bash等包含大量脚本或内建命令的包时出现大量错误。\n解决方案：引入了file -L命令作为“预检”步骤。在分类时，先用file -L 鉴定文件类型，只有确认为 ELF 文件后，才将其归入_bins或_libs，后面再并交由readelf分析。\n测试用例 单元测试：为writer.py中的路径压缩算法编写了单元测试，覆盖了空集合、单文件、多版本组、混合文件等多种边界情况，确保了 SDF 产出的格式质量。 功能测试：使用splitter-docker.sh脚本，对多种类型的软件包进行了端到端的 SDF 生成测试，并与手工编写的 SDF 进行比对。 后续工作安排 虽然核心功能已经完成，但仍然有需要优化和完善的地方：\n脚本依赖分析：探索对 Python import或 Shell 脚本中的命令调用进行静态分析，发现非二进制的依赖关系。\n功能性切分探索：研究为coreutils这类复杂工具集提供“功能分组元数据”的可能性，以支持更深层次的自动化切分。\n批量生成与验证：使用本工具对slice-releases仓库中尚未覆盖的软件包进行 SDF 的批量生成，进一步扩充 openEuler Distroless 的软件包生态。\n楼下的星巴克，经常去那儿敲代码，比在家里有氛围，这个项目大部份在那儿完成的。拍张照纪念一下。\n","date":"2025-09-04T00:00:00+08:00","permalink":"https://mahaoliang.tech/p/%E5%BC%80%E6%BA%90%E4%B9%8B%E5%A4%8F%E7%BB%93%E9%A1%B9%E6%8A%A5%E5%91%8A/","title":"开源之夏结项报告"},{"content":"\n当你刚刚安装好一个新的命令行程序，通常需要手动将它的 bin 目录添加到系统的 PATH 环境变量中。这样，你才能在任何路径下直接调用它的命令。\n此时，一个很实际的问题摆在了面前：应该把 export PATH=\u0026quot;\u0026lt;新增路径\u0026gt;:$PATH\u0026quot; 这行配置代码加到哪个文件里？\n你可能会想到 ~/.bashrc，或者 ~/.profile，也可能听说过 ~/.bash_login 和 ~/.bash_profile。这些文件似乎都可以在 Shell 启动时加载配置，那把 PATH 的设置放在哪里，是不是都一样呢？\n也许你觉得随便选一个文件把配置加在末尾，都能正常工作。但实际上，这些文件的加载时机和应用场景有着本质的区别。错误地放置配置，可能会导致 PATH 变量被反复添加重复的路径，变得越来越长；或者更严重地，可能导致一些自动化脚本（如 scp 文件传输）在远程执行时意外失败。\n本文的目的，就是梳理清楚这些 Shell 启动配置文件的差异，让你彻底搞懂它们的工作原理。读完之后，你将不再似懂非懂，而是能准确地判断出什么配置应该放在哪里。\n理解 Bash 的运行模式 要搞清楚 Shell 配置文件如何加载，首先必须理解 Bash 自身是如何启动和运行的。一个正在运行的 Bash 实例，其状态可以由两个独立的维度来描述：“交互式”还是“非交互式”，以及**“登录”还是“非登录”**。\n这四个基本概念的组合，决定了 Bash 会去加载哪个配置文件。\n交互模式 (Interactive) vs. 非交互模式 (Non-interactive) 这个维度的核心区别在于，Shell 是用来和人“对话”，还是用来自动执行任务。\n交互模式 (Interactive Mode)\n在这种模式下，Shell 会提供一个命令提示符（比如 $），等待用户从键盘输入命令。用户输入命令并按回车后，Shell 执行它，输出结果，然后再次显示提示符，等待下一个命令。顾名思义，这是一个与用户持续“交互”的过程。\n如何启动交互模式：\n在图形界面下打开一个终端程序（Terminal, iTerm, Konsole 等）。 在已有的终端中，直接输入 bash 并回车。 通过 ssh user@host 成功连接到远程主机后，获得的那个 Shell。 非交互模式 (Non-interactive Mode)\n在这种模式下，Shell 不会提供命令提示符，也不等待用户输入。它的任务是执行一个预先定义好的命令集，执行完毕后就自动退出。它的输入源通常是一个文件（脚本）或一个字符串，而不是键盘。\n如何启动非交互模式：\n执行一个 Shell 脚本，例如：bash my_script.sh。 使用 -c 选项来执行一个字符串命令，例如：bash -c \u0026quot;echo Hello, World\u0026quot;。 在 Cron 定时任务中执行的脚本。 登录 Shell (Login) vs. 非登录 Shell (Non-login) 这个维度的区别在于，这个 Shell 实例是不是用户登录会话（session）的第一个进程。\n登录 Shell (Login Shell)\n一个登录 Shell，代表你作为某个用户“登录”到了系统。这个过程通常需要身份验证（比如输入密码或使用 SSH 密钥）。这个 Shell 是你整个会话的起点，之后在该会话中启动的所有其他进程都是它的子进程或后代进程。\n如何启动登录 Shell：\n在物理控制台（没有图形界面的那种）输入用户名和密码登录系统。 通过 SSH 成功连接到远程服务器：ssh user@host。 使用 bash --login 或 bash -l 命令手动启动。 在某些操作系统（如默认配置的 macOS）中，打开终端应用启动的第一个 Shell 就是登录 Shell。 非登录 Shell (Non-login Shell)\n任何在已经存在的登录会话中启动的 Shell，都是非登录 Shell。\n如何启动非登录 Shell：\n在图形界面的终端里，再打开一个新的标签页或窗口（在大多数 Linux 发行版中是这样）。 在一个已有的 Shell 中，直接输入 bash 来启动一个新的子 Shell。 执行一个 Shell 脚本（如 bash my_script.sh），为这个脚本创建的 Shell 实例。 四种组合与常见场景 将以上两个维度组合起来，我们就得到了四种 Shell 的运行状态。不同的启动方式会对应不同的状态组合，而每种组合会加载不同的配置文件。\n下表总结了这四种组合的常见场景，以及它们默认会加载的用户配置文件（~ 目录下的文件）：\n模式组合 描述 常见示例 加载的用户配置文件 交互式登录 Shell 需要身份认证，启动后提供一个可反复输入命令的提示符，是整个用户会话的起点。 • ssh user@host• 在物理控制台输入密码登录 ~/.bash_profile(若无，则找 ~/.bash_login)(若再无，则找 ~/.profile) 交互式非登录 Shell 在一个已经登录的会话中，启动一个新的、提供命令提示符的 Shell 实例。 • 在 Ubuntu 桌面环境中打开一个新终端• 在已有 Shell 中执行 bash ~/.bashrc 非交互式登录 Shell 需要身份认证，但目的是为了执行一个特定的命令或脚本，执行完毕后立即退出，不提供交互提示符。 • ssh user@host 'ls -l'• bash --login my_script.sh 与“交互式登录 Shell”相同 非交互式非登录 Shell 纯粹的脚本执行器，在已登录的会话中启动，无需额外认证，也无须与用户交互。 • bash script.sh• Cron 定时任务• scp 命令在远程主机上的执行端 (默认无) 理解了这个表格，你就掌握了解读 Shell 配置文件的钥匙。接下来，我们将深入探讨每个配置文件具体的内容和它们之间的协作关系。\n各配置文件的加载时机 理解了 Shell 的四种运行模式后，我们就可以准确地“对号入座”，看看 Bash 在不同模式下会选择加载哪个配置文件了。\n登录 Shell 的加载顺序：三选一的规则 当 Bash 作为 登录 Shell 启动时，它会遵循一个非常明确的、一次性的查找规则来加载配置文件。它会按照以下顺序检查用户主目录（~）下的文件：\n~/.bash_profile ~/.bash_login ~/.profile 核心原则是：Bash 只会加载它找到的第一个文件，然后立即停止搜索。\n举个例子，如果你的主目录下同时存在 ~/.bash_profile 和 ~/.profile 这两个文件，那么在登录时，只有 ~/.bash_profile 会被执行，~/.profile 将被完全忽略。\n这三个文件有什么区别？为什么我的 Ubuntu 上只有 .profile？ 这三个文件在功能上都是为登录 Shell 服务的，它们的区别主要在于历史和兼容性。\n~/.bash_profile：这是 Bash 官方首选的、专用于 Bash 的登录配置文件。如果你的工作环境确定只使用 Bash，并且想在配置中使用一些 Bash 特有的高级语法，那么创建和使用这个文件是“最标准”的做法。\n~/.bash_login：这是一个历史遗留的备用选项，从 C Shell (csh) 的 .login 文件借鉴而来。如今已经非常少见，在新的配置中可以忽略它。\n~/.profile：这是兼容性最好的选项。它源自更古老的 Bourne Shell (sh)，因此，几乎所有主流的 Shell（包括 sh, dash, ksh, 以及 bash）都能识别并加载它。\n现在来回答那个关键问题：“为什么我的 Ubuntu 系统默认只有一个 ~/.profile 文件？”\n答案主要有两点：\n为了系统兼容性：Ubuntu 和其他 Debian 系的 Linux，其系统脚本（/bin/sh）默认是由 dash 这个轻量级 Shell 来解释执行的，而不是 bash。dash 为了追求速度和简洁，并不认识 ~/.bash_profile。为了保证系统在执行各类脚本时都能加载到一个基础的环境配置（比如系统默认的 PATH），使用所有兼容 Shell 都认识的 ~/.profile 是最稳妥、最可靠的选择。\n为了用户简洁性：对于绝大多数用户，提供一个 .profile 用于登录，一个 .bashrc 用于交互，分工明确，已经完全足够。这避免了让用户在三个功能相似的登录文件中纠结，简化了配置。\n.bashrc 的使命：为交互式非登录 Shell 服务 .bashrc 的加载规则非常简单和专一：每当一个交互式的、非登录的 Shell 启动时，它就会被加载。\n最常见的场景就是：在你登录系统后，在图形界面中打开一个新的终端窗口或标签页。每打开一次，.bashrc 就会被执行一次。\n.profile 是如何与 .bashrc 协作的 现在，一个逻辑上的问题出现了：登录时只加载 .profile，而打开新终端只加载 .bashrc。那我们定义在 .bashrc 里的别名（alias），为什么在登录 Shell 里也能用呢？\n答案就藏在 Ubuntu 默认的 ~/.profile 文件里。这个文件扮演了一个至关重要的“桥梁”角色。打开你的 ~/.profile，你会看到类似下面这样的代码片段：\n1 2 3 4 5 6 7 # if running bash if [ -n \u0026#34;$BASH_VERSION\u0026#34; ]; then # include .bashrc if it exists if [ -f \u0026#34;$HOME/.bashrc\u0026#34; ]; then . \u0026#34;$HOME/.bashrc\u0026#34; fi fi 这段代码的意思是：\n首先，检查当前运行的 Shell 是不是 Bash ([ -n \u0026quot;$BASH_VERSION\u0026quot; ])。 如果是 Bash，就再去检查 ~/.bashrc 文件是否存在。 如果存在，就通过 . 命令（source 的简写形式）来执行 .bashrc 文件的内容。 通过这段代码，一个优雅的协作流程就形成了：\n当你登录时 (Login Shell)：\nShell 首先执行 ~/.profile。 .profile 设置好 PATH 等环境变量。 然后，它内部的代码会主动调用并执行 ~/.bashrc。 .bashrc 里的别名、函数、提示符等交互式配置也随之生效。 最终，你的登录 Shell 拥有了完整的环境。 当你打开新终端时 (Non-login Shell)：\n这个 Shell 只会执行 ~/.bashrc。 别名、函数等交互式配置被设置好。 而 PATH 这类环境变量，则直接从创建它的父进程（你的桌面环境或登录 Shell）那里继承而来，无需重复设置。 通过这种“登录文件主动包含交互文件”的设计，系统实现了一套既高效又一致的 Shell 环境配置方案。\n什么配置应该放在哪里？ 现在我们清楚了不同文件的加载时机，下一个问题自然就是：具体哪种配置，应该放在哪个文件里？\n这里的核心判断原则是：这个配置是否需要被后续所有程序继承？以及，这个配置操作重复执行多次，会不会产生副作用？\n原则一：只需执行一次的配置 -\u0026gt; .profile 或 .bash_profile 这类配置的特点是，它们在登录时设置一次后，就会被当前会话中启动的所有子进程（包括之后打开的每一个新终端）所继承。\n应该放在这里的内容：\n环境变量的设置：这是最主要的应用。比如 PATH、JAVA_HOME、GOPATH、ANDROID_HOME 等。 启动会话级的后台服务：比如启动一个 ssh-agent。 为什么放在这里： 因为环境变量会被子进程继承，所以我们没有必要、也不应该在每次打开新终端时都去重复设置它们。在登录时设置一次，就“一劳永逸”了。\n“幂等” (Idempotent) 的概念： 在编程中，“幂等”指的是一个操作，无论执行一次还是执行 N 次，产生的结果都是完全相同的。 现在，让我们审视一下修改 PATH 变量的这行命令： export PATH=\u0026quot;$PATH:/new/path\u0026quot; 这个操作是幂等的吗？不是。每执行一次，它都会在现有的 $PATH 字符串后面追加一次 :/new/path。如果重复执行，PATH 变量会变得冗长、混乱且包含大量重复条目。\n结论：对于非幂等且需要被继承的配置，必须将它放在一个只执行一次的文件里，.profile (或 .bash_profile) 正是为此而生。\n原则二：每次打开新终端都需要的功能 -\u0026gt; .bashrc 这类配置的特点是，它们不会被子进程继承，只在当前 Shell 进程内有效。因此，如果希望每个新打开的终端都具备这些功能，就必须在每次启动时都重新加载它们。\n应该放在这里的内容：\n命令别名 (alias)：例如 alias ll='ls -alF'。 Shell 函数 (function)：你自己编写的各种便捷脚本函数。 自定义的命令提示符 (PS1)。 Shell 选项的设置：通过 set 或 shopt 命令开启或关闭的 Shell 行为。 为什么放在这里： 因为别名、函数等配置不会被继承。你在一个终端里设置的别名，在另一个新打开的终端里是无效的。所以，必须把它们放在每次打开新终端都会执行的 .bashrc 文件里。\n实验：一个反面教材 为了直观地感受错误配置带来的问题，我们来做一个简单的实验，故意将非幂等的 PATH 设置放进 .bashrc。\n场景布置 打开你的 ~/.bashrc 文件，在文件末尾添加下面这行代码并保存：\n1 export PATH=\u0026#34;$PATH:/my_test_path\u0026#34; 开始操作：\n首先，关闭所有终端，然后打开一个新的终端。这会加载一次 .bashrc。 在这个新终端里，输入 bash 并回车。这会启动一个子 Shell，它会再次加载 .bashrc。 在子 Shell 中，再输入一次 bash 并回车，启动孙 Shell，这将第三次加载 .bashrc。 现在，我们来检查一下 PATH 变量。输入以下命令： 1 echo $PATH 观察结果： 你会看到类似下面这样的输出，/my_test_path 在末尾重复出现了三次：\n1 /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/my_test_path:/my_test_path:/my_test_path 实验证明： 这个结果清晰地证明了，将非幂等操作放在 .bashrc 中是错误的做法。每启动一个交互式 Shell，它都会不加判断地执行一次，导致配置的累积和环境的污染。\n完成实验后，记得删除你添加到 .bashrc 的那一行测试代码。\nUbuntu 默认配置分析 本章我们来剖析 Ubuntu 默认 .bashrc 文件中的一个关键设计，并解释它为何能避免一些严重的潜在问题。\n.bashrc 的“保护判断” 如果你打开 Ubuntu 默认的 ~/.bashrc 文件，很可能会在文件开头看到下面这段代码：\n1 2 3 4 5 # If not running interactively, don\u0026#39;t do anything case $- in *i*) ;; *) return;; esac 这段代码是什么意思？它是一个“保护判断”，作用是确保 .bashrc 文件中后续的所有配置，只在交互模式下执行。\ncase $- in ... esac：这是一个条件判断语句，它检查 $- 这个特殊变量的值。 $-：这个变量包含当前 Shell 的一系列选项标志。如果它包含字母 i，就代表当前是一个交互式 (interactive) Shell。 *i*) ;;：这是一个模式匹配。如果 $- 的值包含 i，则匹配成功。后面的 ;; 表示“匹配成功后，什么也不做”，然后继续执行 .bashrc 文件的后续代码。 *) return;;：这是一个“捕获所有其他情况”的模式。如果 $- 的值不包含 i（即非交互模式），则执行 return 命令。在一个被 source 的脚本中，return 会立即终止该脚本的执行。 简单来说，这段代码的逻辑就是：“是交互模式吗？是就继续。不是？立刻退出，别往下读了。”\n当 scp 遭遇“热情”的 .bashrc 为什么要费这么大功夫做一个检查？非交互模式下执行一下别名、函数，似乎也无伤大雅？让我们来看一个真实且常见的失败案例，它能完美地解释这个保护判断的重要性。\n场景设定：\n远程服务器配置：一位系统管理员为了登录服务器时能看到一句欢迎语，就在服务器的 ~/.bashrc 文件里加了一行 echo \u0026quot;Welcome back to the server!\u0026quot;。 移除保护：为了模拟问题，我们假设他不小心删除了 .bashrc 文件开头的那段保护判断代码。 本地操作：现在，他在自己的本地电脑上，尝试用 scp 命令向这台配置错误的服务器上传一个文件： scp my_local_file.txt user@remote_server:/home/user/ 灾难是如何发生的：\nscp 的工作原理：scp 命令在后台通过 SSH 登录到远程服务器。它并不会启动一个我们平时用的那种交互式 Shell，而是请求服务器启动一个非交互式的 Shell 来专门处理文件传输。这是一个程序与程序之间的对话，它们之间通过一套严格的 scp 协议来通信。 “热情”的干扰：因为服务器上的 .bashrc 没有了保护判断，这个为 scp 启动的非交互式 Shell，也会去执行 .bashrc 里的所有内容。于是，echo \u0026quot;Welcome back...\u0026quot; 这条命令被执行了。 污染通信协议：这句“Welcome back\u0026hellip;”的问候语，作为一段普通的文本，被发送回了本地的 scp 客户端。但此时，scp 客户端正在等待的是符合协议规范的确认信号，而不是一段人类阅读的欢迎词。 命令失败：这段意料之外的文本“污染”了 scp 协议的通信流。scp 客户端无法解析它，认为通信出错，最终导致命令失败，并可能抛出一个令人费解的错误，如“protocol error”或“lost connection”。 这个例子清晰地表明，.bashrc 开头的保护判断是一个至关重要的安全措施。它确保了那些为人类交互而设计的配置，不会干扰到那些需要在纯净、可预测的环境下工作的自动化工具（如 scp、rsync、git 等）。\n非交互模式为何无法加载 .bashrc 我们可以通过一个简单的脚本实验，亲眼验证这个保护判断是如何工作的。\n创建脚本 test.sh: 在你的主目录下创建一个名为 test.sh 的文件，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/bin/bash UNIQUE_ID=$$ VAR_VALUE=\u0026#34;new_env_value_${UNIQUE_ID}\u0026#34; # 为了保证实验干净，先确保 .bashrc 中没有我们要测试的变量 sed -i \u0026#39;/export NEW_ENV/d\u0026#39; ${HOME}/.bashrc echo \u0026#34;--- 实验开始 (进程 ID: ${UNIQUE_ID}) ---\u0026#34; echo echo \u0026#34;1. 尝试向 .bashrc 文件末尾添加一个环境变量...\u0026#34; echo \u0026#34;export NEW_ENV=${VAR_VALUE}\u0026#34; \u0026gt;\u0026gt; \u0026#34;${HOME}/.bashrc\u0026#34; echo echo \u0026#34;2. 尝试在当前脚本 (非交互) 中 source .bashrc 来让它立即生效...\u0026#34; source ${HOME}/.bashrc echo echo \u0026#34;3. 读取 NEW_ENV 的值：[$NEW_ENV]\u0026#34; echo # 清理工作：再次移除我们添加的行 sed -i \u0026#39;/export NEW_ENV/d\u0026#39; ${HOME}/.bashrc echo \u0026#34;--- 实验结束 ---\u0026#34; 执行与结果： 给脚本执行权限 chmod +x test.sh，然后运行它 ./test.sh。你会看到如下输出：\n1 2 3 4 5 6 7 8 9 --- 实验开始 (进程 ID: 436292) --- 1. 尝试向 .bashrc 文件末尾添加一个环境变量... 2. 尝试在当前脚本 (非交互) 中 source .bashrc 来让它立即生效... 3. 读取 NEW_ENV 的值：[] --- 实验结束 --- 结果分析： 实验结果表明，$NEW_ENV 的值是空的！尽管我们确实把 export 语句添加到了 .bashrc 文件中，并且执行了 source 命令，但这个变量并没有被设置到当前脚本的环境中。\n原因正在于 .bashrc 开头的那段保护判断。当 test.sh 这个非交互式脚本执行到 source ${HOME}/.bashrc 时，.bashrc 内部的 case 语句检测到当前并非交互模式，于是立即执行 return，终止了自身的执行。因此，我们刚刚添加进去的 export NEW_ENV=... 那一行，以及文件中的其他所有配置，都根本没有机会被执行。\n总结回顾 让我们再次梳理一下核心知识点：\nShell 的四种模式：Bash 的运行状态由“交互式/非交互式”和“登录/非登录”这两个维度共同决定。不同的启动方式（如 ssh 登录、打开终端、执行脚本）会对应不同的模式组合。\n配置文件的分工：\n~/.bash_profile (或兼容性更强的 ~/.profile)：专为 登录 Shell 服务。它在用户会话开始时仅执行一次，是设置环境变量（如 PATH）和执行一次性初始化任务的最佳位置。 ~/.bashrc：专为 交互式非登录 Shell 服务。每次打开新的终端窗口时，它都会被执行，因此是定义别名、函数、自定义提示符等增强交互体验功能的理想场所。 协作的关键：在像 Ubuntu 这样的主流发行版中，~/.profile 文件会主动 source（加载）~/.bashrc 文件。这一“桥梁”设计，确保了登录 Shell 和后续打开的非登录 Shell 拥有一致的交互环境。\n环境的纯净性：.bashrc 文件开头的非交互模式保护判断至关重要。它能防止为人类交互设计的配置（如 echo 输出、别名等）干扰到需要在纯净环境下运行的自动化工具（如 scp, rsync 等），避免难以排查的协议错误。\n","date":"2025-08-31T21:04:45+08:00","permalink":"https://mahaoliang.tech/p/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90-.bash_profile-%E4%B8%8E-.bashrc/","title":"深入解析 .bash_profile 与 .bashrc"},{"content":"作为一名开发者，参与开源是提升技术、建立个人影响力的有效途径。但很多人，包括曾经的我，在刚接触时都会被 GitHub 的协作流程搞得一头雾水。\n比如，什么是 Fork？Upstream 和 Origin 又有什么区别？标准的贡献流程是怎样的？网上虽然有资料，但很少有一篇文章能为新手清晰地讲明白整个过程。\n今年我参加了“开源之夏”活动，在为 openEuler 社区的 splitter 项目提交 PR 的过程中，才算真正搞清楚完整的流程。因为自己经历过初期的困惑，所以决定把这个过程完整地记录下来，形成一篇能直接上手操作的指南。\n这篇文章的目的很明确：就是为那些想参与开源但对流程不熟悉的开发者，提供一份清晰、实用的操作手册。希望这篇总结能帮你扫清障碍，顺利迈出开源贡献的第一步。下面我们正式开始。\n核心概念解析 在动手操作之前，我们必须先理解几个关键的名词。搞清楚它们之间的关系，是顺利完成后续所有步骤的基础。\nGit 基础概念回顾 首先，你需要对 Git 本身的工作流程有一个基本了解。下面这张图很经典地展示了 Git 的核心区域和操作：\n简单来说，你的日常工作就是：\n在 Workspace (工作区) 修改代码。 使用 git add 将修改内容暂存到 Index (暂存区)。 使用 git commit 将暂存区的内容提交到本地 Repository (仓库)，形成一个版本记录。 使用 git push 将本地仓库的提交推送到 Remote (远程仓库)，比如 GitHub。 使用 git pull 或 git fetch 从远程仓库拉取更新。 对于这些基础命令的详细用法，阮一峰老师《常用 Git 命令清单》已经总结得非常全面，是很好的速查手册，本文不再赘述。\nGitHub 开源协作的概念 上面提到的模型只涉及一个本地仓库和一个远程仓库。但在开源协作中，通常会涉及三个仓库。下面这张图清晰地展示了它们的关系，这也是本节的重点：\n让我们结合这张图，来理解几个最重要的概念：\nUpstream (上游仓库)\n是什么：图中左上角的 \u0026ldquo;GitHub - Original\u0026rdquo;，也就是你想要贡献代码的那个原始开源项目仓库。 你的权限：你通常没有直接往这里 push 代码的权限。 Fork (你的个人复刻)\n是什么：图中右上角的 \u0026ldquo;GitHub - Fork\u0026rdquo;。这是你通过在原始项目页面上点击 \u0026ldquo;Fork\u0026rdquo; 按钮，在你自己的 GitHub 账号下创建的一个完整的项目副本。 你的权限：因为这个仓库在你的名下，所以你拥有全部的读写权限，可以随意 push 代码。这是你为开源项目贡献代码的“大本营”和“实验区”。 origin (你的远程仓库)\n是什么：简单来说，origin 是 Git 为你克隆的那个远程仓库地址起的一个默认别名。 如何创建：当你从 GitHub 克隆你的 Fork 仓库时，通过执行 git clone [URL of YOUR FORK] 这个命令，Git 会在你的本地仓库中自动创建这个名为 origin 的别名。你不需要任何手动设置。 指向哪里：在我们的协作流程中，origin 这个别名指向的就是你自己的 Fork 仓库 (也就是图右上角的 \u0026ldquo;GitHub - Fork\u0026rdquo;)。 作用：它是你 push 本地代码时的默认目标。当你执行 git push 时，你的代码变更就是通过 origin 这个别名，被推送到你自己的 Fork 仓库中。 upstream vs. origin 的关系\norigin 指向你自己的 Fork，是你推送 (push) 代码的地方。 upstream 指向原始项目仓库，是你 同步最新代码 (fetch) 的地方。 关键点：origin 是克隆时自动生成的，而 upstream 是需要我们手动添加的。这一步至关重要，它建立了你的本地仓库和原始项目之间的连接，让你能够随时获取项目的最新进展。 Pull Request (PR - 合并请求)\n是什么：当你觉得在自己 Fork 里的代码已经准备好，可以贡献给原始项目时，你就可以在 GitHub 上发起一个 Pull Request。 作用：这是一个正式的请求，请求 Upstream (原始项目) 的维护者，把你 Fork 里的代码变更拉取 (Pull) 到他们的仓库中。这也是代码审查 (Code Review)、讨论和最终合并的地方。 总结一下，整个流程的数据流是这样的：\n初始化：从 origin 到本地\n你首先 clone 的是你自己的 Fork 仓库 (origin)，把代码从你的 GitHub 仓库复制到本地电脑。这是你建立本地工作环境的第一步。\n开发与同步：upstream -\u0026gt; 本地 -\u0026gt; origin\n这个阶段是循环往复的。\n获取更新：你需要定期从 upstream (原始项目) fetch 或 rebase 最新的代码到本地，确保你的工作是基于最新版本。 推送你的贡献：在本地开发完成后，你把代码 push 到 origin (你自己的 Fork)。你的代码变更只会上传到你自己的远程仓库里。 提交贡献：从 origin 到 upstream\n最后，你通过在 GitHub 上创建一个 Pull Request，请求 upstream (原始项目) 的维护者，来审核并合并你 origin (你的 Fork) 里的代码。\n这个流程确保了你的所有修改都在自己的“地盘”上进行，既不会影响到原始项目，又能随时与原始项目保持同步，并最终通过 PR 的方式发起贡献。\n理解了这三个仓库和它们之间的关系，我们就已经扫清了最大的障碍。接下来，我们将进入实战环节。\n首次贡献实战演练 理论知识已经储备完毕，现在让我们卷起袖子，完整地走一遍实际的贡献流程。我们会模拟为一个名为 project-name 的开源项目贡献代码。\nStep 1: Fork - 拥有你自己的副本 首先，你需要进入 project-name 的 GitHub 主页。在页面的右上角，你会看到一个 \u0026ldquo;Fork\u0026rdquo; 按钮。点击它，GitHub 就会在你的个人账号下创建一个该项目的完整副本。\n完成后，你的 GitHub 主页上就会出现一个 your-username/project-name 的仓库。这就是你的个人复刻 (Fork)。\nStep 2: Clone - 将代码克隆到本地 现在，你需要把你 Fork 的仓库克隆到你的电脑上进行开发。\n进入你刚刚创建的 your-username/project-name 仓库页面，点击绿色的 \u0026ldquo;Code\u0026rdquo; 按钮，复制 HTTPS 或 SSH 链接。\n然后，在你的电脑终端中执行以下命令：\n1 2 # 确保将 YOUR-USERNAME 替换为你的 GitHub 用户名 git clone https://github.com/YOUR-USERNAME/project-name.git 注意：这里克隆的是你自己的 Fork 仓库地址，而不是原始项目的地址。这是非常关键的一步。\nStep 3: 配置 upstream，建立与上游的连接 为了能够随时获取原始项目的更新，我们需要在本地配置一个指向上游仓库 (upstream) 的远程地址。\n首先，进入项目目录：\n1 cd project-name 然后，添加 upstream：\n1 2 # 将 URL 替换为原始开源项目的 URL git remote add upstream https://github.com/original-owner/project-name.git 我们可以用 git remote -v 命令来检查是否配置成功。你应该能看到类似下面的输出：\n1 2 3 4 origin https://github.com/YOUR-USERNAME/project-name.git (fetch) origin https://github.com/YOUR-USERNAME/project-name.git (push) upstream https://github.com/original-owner/project-name.git (fetch) upstream https://github.com/original-owner/project-name.git (push) 看到 origin 和 upstream 同时存在，就说明配置成功了。\nStep 4: 创建特性分支 在开始写代码之前，一个最佳实践是为你的新功能或修复创建一个独立的分支。这能确保你的 master 分支保持干净，并与上游项目同步。\n首先，确保你的本地 master 分支是最新的：\n1 2 3 git fetch upstream git checkout master git rebase upstream/master 然后，从最新的 master 分支上创建一个新分支：\n1 2 # 把 my-feature-branch 换成一个有意义的名字，比如 fix-login-bug git checkout -b my-feature-branch 现在，你就可以在这个新分支上安全地进行开发了。\nStep 5: 开发与提交 在这个分支上，你可以自由地修改代码、添加新文件、修复 Bug。完成一个阶段性的工作后，就进行一次 commit。\n1 2 3 4 5 # 1. 添加你的修改到暂存区 git add . # 2. 提交你的修改，并写下清晰的提交信息 git commit -m \u0026#34;feat: Implement user profile page\u0026#34; Step 6: 保持同步，与上游代码对齐 (可选但重要) 如果你的开发周期比较长，在你开发期间，upstream 可能已经合并了其他人的代码。为了避免提交 PR 时产生冲突，建议在推送前，先同步一次上游的最新代码。\n1 2 git fetch upstream git rebase upstream/master rebase 可以让你的提交历史保持一条直线，看起来更整洁。如果遇到冲突，你需要先解决冲突，然后继续 rebase 过程。\nStep 7: 推送，将变更推送到你的 Fork 当你在本地完成开发和提交后，就可以把你的特性分支推送到你自己的 Fork 仓库 (origin) 了。\n1 git push --set-upstream origin my-feature-branch --set-upstream 参数只需要在第一次推送这个分支时使用，它会告诉 Git 你的本地分支 my-feature-branch 对应远程仓库 origin 的同名分支。\nStep 8: 创建 Pull Request 推送成功后，现在回到你在 GitHub 上的 Fork 仓库页面。通常，GitHub 会自动检测到你推送了新的分支，并显示一个黄色的提示条，让你方便地创建 Pull Request。\n点击 \u0026ldquo;Compare \u0026amp; Pull Request\u0026rdquo; 按钮，你会进入 PR 创建页面。在这里，你需要：\n填写一个清晰的标题：简明扼要地说明这个 PR 的作用。 撰写详细的描述：解释你为什么要进行这些修改，解决了什么问题，以及你是如何实现的。如果项目有 PR 模板，请务必按照模板填写。 确认无误后，点击 \u0026ldquo;Create pull request\u0026rdquo;。恭喜你，你的第一个 PR 已经成功提交了！\nStep 9: 响应代码审查 (Code Review) 提交 PR 只是开始，接下来你需要和项目维护者进行互动。\n维护者留下评论：项目维护者会审查你的代码，并在可能有问题的地方留下评论 (Comment)。你会在 GitHub PR 页面和邮件中收到通知。 回复与讨论：对于每一条评论，你都应该进行回复。如果同意修改，可以简单回复“好的，马上修改”；如果不理解或有不同意见，可以在评论区进行礼貌的讨论。\n修改代码并再次提交：\n回到你的本地电脑，确保你还在之前的 my-feature-branch 分支上。\n根据讨论结果，直接修改代码。\n修改完成后，创建一个新的 commit：\n1 2 git add . git commit -m \u0026#34;fix: Address review comments from maintainer\u0026#34; 重要：修改完成后，直接将新的提交推送到你的分支：\n1 git push origin my-feature-branch 你不需要重新创建一个 PR。这次 push 会自动更新你之前提交的那个 PR。\n解决对话 (Resolve Conversation)：\n当你认为你已经解决了某一条评论中的问题后，回到 PR 页面，找到对应的评论，点击 \u0026ldquo;Resolve conversation\u0026rdquo; 按钮。这是一种礼貌的表示，告诉维护者这个问题你已经处理完毕，方便他们再次审查。\n这个修改 -\u0026gt; 推送 -\u0026gt; 解决对话的循环可能会进行多次，直到所有问题都得到解决。\nStep 10: 合并 当你的代码通过了所有审查，维护者就会点击 \u0026ldquo;Merge pull request\u0026rdquo; 按钮，将你的代码合并到主项目中。\n至此，你的代码就正式成为了开源项目的一部分！你可以在项目的贡献者名单中看到自己的名字，这是对你辛勤付出的最好回报。\n保持更新，如何为下一次贡献做准备 恭喜你！在完成了一次 PR 合并后，你已经是一位正式的开源贡献者了。但工作还没结束。为了能方便地进行下一次贡献，你需要学会如何让你的 Fork 仓库与原始项目保持同步。\n为什么需要同步你的 Fork 在你完成一次贡献后，原始项目（upstream）的 master 分支因为合并了你的 PR 和其他人的代码，已经向前更新了。而你自己的 Fork 和本地仓库的 master 分支，还停留在你开始工作时的旧位置。\n如果你不进行同步，直接从一个过时的 master 分支上创建新分支进行开发，那么在提交下一次 PR 时，几乎必然会遇到大量的代码冲突，给自己和项目维护者都带来不必要的麻烦。\n因此，在开始下一次贡献前，保持你的 master 分支与 upstream 完全同步，是一个至关重要的好习惯。\n两步完成同步 整个同步过程非常简单，可以分为两步：先更新本地，再更新你的远程 Fork。\n第一步：更新你的本地仓库 切换到主分支\n首先，确保你回到了 master 分支。所有同步操作都应该在 master 分支上进行。\n1 git checkout master 从上游拉取最新变更\n这个命令会从原始项目（upstream）下载最新的代码历史，但不会自动修改你本地的任何文件。\n1 git fetch upstream 将本地 master 同步到上游 master\n这步是关键。它会以 upstream/master 的最新版本为基础，来更新你本地的 master 分支。执行后，你的本地 master 分支就和原始项目的 master 分支完全一致了。\n1 git rebase upstream/master 现在，你的本地 master 分支已经是最新版本了。\n第二步：更新你在 GitHub 上的 Fork 你的本地 master 是最新的了，但你 GitHub 上的 Fork (origin) 还停留在旧版本。我们需要把本地的更新推送到 origin。\n1 git push origin master 执行这个命令后，你 GitHub 上的 Fork 仓库的 master 分支也会被更新到最新状态。\n小结 现在，你的本地仓库和 GitHub 上的 Fork 都和上游项目完全同步了。\n你可以安心地从这个干净、最新的 master 分支上，通过 git checkout -b new-feature 创建新的特性分支，开始你的下一次贡献了。\n记住这个流程：每次开始新工作前，先同步 master 分支。这将让你的开源贡献之路更加顺畅。\n常用命令速查清单 (Cheat Sheet) 在熟悉了整个流程后，你不需要每次都回头阅读长篇的文字。这份速查清单总结了在不同阶段最核心的命令，你可以把它当作日常贡献时的“小抄”。\n首次设置 (为一个新项目贡献时，仅需一次) 1 2 3 4 5 6 7 8 9 10 11 # 1. 克隆你自己的 Fork 仓库到本地 git clone [URL of YOUR FORK] # 进入项目目录 cd [project-name] # 2. 添加原始项目仓库为 upstream git remote add upstream [URL of ORIGINAL REPO] # 3. 验证远程仓库设置是否成功 git remote -v 一个完整的开发周期 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # === 准备阶段 === # 1. 切换到主分支 git checkout master # 2. 与上游 master 分支保持同步 git fetch upstream git rebase upstream/master # 3. 从最新的 master 分支创建你的特性分支 git checkout -b [my-new-feature-branch] # === 开发阶段 === # ... 在这里进行代码的修改、添加、删除 ... # 1. 添加变更到暂存区 git add . # 2. 提交变更 git commit -m \u0026#34;feat: Add a new amazing feature\u0026#34; # === 提交阶段 === # 1. (可选但推荐) 在推送前，再次与上游同步，避免冲突 git fetch upstream git rebase upstream/master # 2. 将你的分支推送到你自己的 Fork 仓库 (origin) # 第一次推送时使用 --set-upstream git push --set-upstream origin [my-new-feature-branch] 推送完成后，去 GitHub 页面创建 Pull Request。\n响应代码审查 (Review 后修改代码) 1 2 3 4 5 6 7 8 9 # 确保你还在你的特性分支上 # ... 直接修改代码 ... # 1. 提交你的修改 git add . git commit -m \u0026#34;fix: Address review comments\u0026#34; # 2. 再次推送到你的分支，PR会自动更新 git push origin [my-new-feature-branch] 贡献完成后，保持 Fork 更新 (为下一次贡献做准备) 1 2 3 4 5 6 7 8 9 10 11 # 1. 切换到主分支 git checkout master # 2. 拉取上游的最新代码 git fetch upstream # 3. 将本地 master 更新到最新 git rebase upstream/master # 4. 将这个最新的 master 分支推送到你自己的 Fork，使其在 GitHub 上也保持最新 git push origin master 结语 到这里，我们已经完整地走完了在 GitHub 上参与一个开源项目的全过程：从理解 Fork、Upstream 这些核心概念，到一步步创建分支、提交代码、发起 Pull Request，再到如何与维护者互动以及如何为下一次贡献保持仓库同步。\n其实，整个流程可以被总结为一条清晰的路径：\nFork -\u0026gt; Clone -\u0026gt; Branch -\u0026gt; Develop -\u0026gt; Push -\u0026gt; PR\n开源贡献的流程并不复杂，最重要的，是勇敢地迈出第一步。\n希望这篇文章能成为你开启开源之旅的得力助手。现在，就去寻找一个你感兴趣的项目，提交你的第一个 PR 吧！\n","date":"2025-08-24T16:26:32+08:00","permalink":"https://mahaoliang.tech/p/%E4%B8%80%E4%BB%BD%E7%BB%99%E5%BC%80%E6%BA%90%E6%96%B0%E6%89%8B%E7%9A%84-github-%E8%B4%A1%E7%8C%AE%E6%B5%81%E7%A8%8B%E6%8C%87%E5%8D%97/","title":"一份给开源新手的 GitHub 贡献流程指南"},{"content":"今年我有幸参加了开源之夏 (Summer OSPP) 活动，并成功中选了 面向 openEuler distroless 镜像的 SDF 自动生成工具开发 项目。\n就在几天前，我收到了 Gitee 的合并通知。那一刻，我感觉自己几个月来的努力都有了最好的回报。经过一段时间的开发、学习和与华为老师的深入交流，我提交的 两 四个 PR 终于先后被合并了！这不仅仅是代码的合入，对我而言意义重大，毕竟这不再是个人玩具项目，而是为华为 openEuler 生态的基础软件贡献代码。我想借此机会，分享一下这段令人兴奋的旅程。\n开源之夏 是中国科学院软件研究所发起的“开源软件供应链点亮计划”系列活动，由中国科学院软件研究所与华为共同主办、中科南京软件技术研究院承办。开源之夏为学生提供了接触和贡献高质量开源项目的机会，通过真实的开源项目实践，培养和发掘优秀的开发者，促进优秀开源软件社区的蓬勃发展，助力开源软件供应链建设。开源之夏于 2020 年正式发起，今年已经是第六届，该活动已成为国内开源社区中极具影响力的人才培养平台。 2025 年的开源之夏联合了全球 182 个开源社区，共发布了 566 个项目任务，覆盖了操作系统、人工智能、数据库、云原生、RISC-V 等多个前沿技术领域，吸引了全球 450 所高校，两千多名学生的报名，提交了项目申请书 1176 份，最终 518 名学生中选。\n我的任务：为 splitter 打造 SDF 自动生成器 首先简单介绍一下我参与的项目 splitter。\n在容器化的世界里，我们追求更小、更安全的镜像。openEuler 的 Distroless 镜像就是为此而生。它的核心思想是，不再完整地打包一个 RPM 软件包，而是将其精细地“切分”成多个功能独立的“Slice”，软件包之间的依赖关系也就更精细地表现为 slice 之间的依赖。然后我们以 slice 为最小构建单元生成最终的 distroless 镜像，可以有效减少冗余文件，进而降低安全风险。\n如上图所示，软件包 B 依赖于软件包 A 等价于 B_slice1 和 B_slice2 依赖于 A_slice1、A_slice2，在生成 B 的应用镜像时，可以不再打包 A_slice3 所包含的文件。\nsplitter 通过解析 RPM 软件包，并根据预定义的规则文件 SDF (Slice Definition File)，将软件包切分成多个 slice。\nSDF (Slice Definition File) 精准定义每个软件包的拆分规则。目前，所有的 SDF 文件都由社区专家手工编写，存放在 slice-releases 仓库中。当软件包数量和版本不断增多时，手工编写 SDF 就成了一个巨大的瓶颈。\n我的任务，就是为splitter开发一个gen命令，实现 SDF 文件的自动化生成。\nSDF 生成器的核心实现 我的任务是自动化生成 SDF 文件，但在开始之前，我们首先要理解：SDF 文件究竟是什么？\nSDF 文件的构成 一个 SDF（Slice Definition File）文件，本质上是一个 YAML 格式的“软件包拆分说明书”。它精确地定义了一个 RPM 包如何被拆解成多个功能独立的、可按需组合的“Slice”。一个典型的 SDF（以brotli.yaml为例）包含两个核心部分：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package: brotli deps: - brotli_copyright slices: libs: deps: - glibc_libs contents: common: - /usr/lib64/libbrotlicommon.so.1* - /usr/lib64/libbrotlidec.so.1* - /usr/lib64/libbrotlienc.so.1* copyright: contents: common: - /usr/share/licenses/brotli/LICENSE slices (切片)：这是主体部分，定义了包内每个文件的归属。 contents: 列出了这个 Slice 包含的所有文件路径。比如，brotli_libs slice 包含了所有的.so库文件。 deps (依赖)：定义了 Slice 之间的依赖关系。 deps: 列出了要让当前 Slice 正常工作，需要依赖哪些其他的 Slice。比如，brotli_libs slice 依赖于glibc_libs，因为brotli的库函数调用了glibc的底层功能。 SDF 的自动化生成流程 理解了 SDF 的构成后，我设计了一套自动化的流水线来生成它。整个流程就像一个工厂的生产线，每一步都有明确的输入和输出：\n下载 (Download): 首先，从 openEuler 的仓库中下载指定的目标 RPM 包。 解压 (Extract): 将 RPM 包解压到一个临时目录，暴露出其内部的所有文件。 文件分类 (Classify Files): 遍历所有文件，根据一系列规则，将它们分配到不同的 Slice 中。这是填充 SDF 中slices和contents部分的核心步骤。 依赖分析 (Analyze Dependencies): 对分类好的 Slice（特别是包含二进制文件和库的）进行分析，找出它们之间的依赖关系。这是填充 SDF 中deps部分的核心步骤。 生成 SDF (Generate): 最后，将分类和依赖分析的结果，按照 SDF 的 YAML 格式，写入到最终的文件中。 在这条流水线中，最核心、最具技术挑战性的，无疑是“文件分类”和“依赖分析”这两个环节。\n文件分类原理 文件分类就是确定 RPM 包里的每一个文件，应该属于哪个 Slice。\n一个软件包通常包含可执行文件、库文件、配置文件、版权声明等。我实现了一个基于规则的智能分类器，它的工作思路是：\n建立规则：我分析了大量手工编写的 SDF，总结出了一套通用的分类“约定”。例如：\n以/etc/开头的文件 -\u0026gt; 归入_config slice。 以/usr/bin/或/usr/sbin/开头的文件 -\u0026gt; 归入_bins slice。 以/usr/lib*/开头且包含.so的文件 -\u0026gt; 归入_libs slice。 包含LICENSE, COPYING等字样的文件 -\u0026gt; 归入_copyright slice。 精确识别：仅靠路径还不够。比如，在/usr/bin目录下，既有真正的二进制可执行文件，也可能混杂着 Shell 脚本或 Bash 内建命令的占位符。为了精确区分，我的分类器会调用 Linux 的file命令对每个文件进行“身份鉴定”。只有file命令确认是“ELF executable”的文件，才会被归入_bins slice，从而保证了分类的准确性。\n通过这个分类器，我们就能自动地将一个 RPM 包内的上百个文件，有条不紊地分配到不同的slices中。\nSlice 依赖分析原理 依赖分析就是找出每个 Slice（尤其是_bins和_libs）依赖了哪些外部的 Slice。\n这是技术上最关键的一步。一个二进制文件运行时，需要操作系统动态链接器加载它所依赖的共享库（.so文件），我需要自动追溯这条“依赖链”。\n我的依赖分析器采用了一个三步走的策略，来模拟动态链接器的行为：\n静态解析“需求”：对于_bins和_libs中的每个 ELF 文件，我使用readelf -d命令。这是一个静态分析工具，它能安全地读取文件头，并列出这个文件在运行时“需要 (NEEDED)”哪些共享库。例如，readelf会告诉我们brotli的库需要libc.so.6。\n全系统“寻址”：知道了需要libc.so.6，下一步就是要在系统中找到它。我通过查询ldconfig -p维护的系统库缓存，可以快速地将一个库名（libc.so.6）映射到它在文件系统上的绝对路径（例如/usr/lib64/libc.so.6）。\n反向“溯源”：拿到了库文件的绝对路径，最后一步就是确定它的“主人”。我使用rpm -qf \u0026lt;文件路径\u0026gt;命令，它可以精确地反向查询出这个文件是由哪个 RPM 包提供的。例如，rpm -qf /usr/lib64/libc.so.6会返回glibc。\n至此，完整的依赖链就建立起来了：brotli_libs -\u0026gt; (需要libc.so.6) -\u0026gt; (位于/usr/lib64/libc.so.6) -\u0026gt; (属于glibc包) -\u0026gt; (因此依赖glibc_libs)。\n通过自动化这个流程，我的工具就能为每个 Slice 精确地填充出它的deps列表。\n无法逾越的环境依赖 当我完成了上述完整的自动化流水线，并满怀信心地在一个最小化的 openEuler 环境上进行测试时，我遇到了第一个真正的挑战。\n我的依赖分析器在第 2 步“全系统寻址”时失败了！原因很简单：我测试用的这个最小化系统里，根本就没有预装待分析包所需要的所有依赖库。\n这个问题是致命的。它意味着我的工具能否成功运行，完全取决于它所在的宿主环境是否“恰好”是完备的。这对于一个追求自动化和可靠性的工具来说，是不可接受的。\n引入 Docker“沙箱化”分析 在 PR review 的过程中，华为鲁卫军老师建议我使用 chroot 或容器技术来隔离分析环境，避免安装待分析的 RPM 包污染宿主机环境。基于这个建议，我采用 Docker 实现了环境隔离，新的流程是这样的：\n构建基础镜像：编写了一个 Dockerfile，它会预先构建一个包含了 splitter 工具本身，以及所有依赖（如 python-dnf, binutils 等）的“SDF 生成器基础镜像”。\n隔离的分析流程: 当运行 gen-sdf-docker.sh 时：\n它会自动使用上述的基础镜像启动一个干净、一次性的 Docker 容器。 在容器内部，它只执行必要的操作：dnf install 来安装待分析包及其运行时依赖。 然后调用 splitter gen 命令执行核心的分析逻辑。 分析结束后，容器会被自动销毁，对用户的宿主系统环境无影响。\n意外的惊喜：第一个成功合并的 PR 竟是“副产品”！ 为了实现上述的 Docker 化流程，我需要一个包含了splitter工具本身的基础镜像。这时，鲁老师又给了我一个的建议：\n构建 splitter 的 Dockerfile 可以提交到 openEuler 的官方镜像仓库去 https://gitee.com/openeuler/openeuler-docker-images\n这让我意识到，这个为了我自己的工具而制作的镜像，本身就可以成为一个交付给社区的“官方应用镜像”！于是，我仔细阅读了官方镜像的贡献指南，编写了Dockerfile和相关的元数据，并提交了PR 到 openeuler-docker-images 仓库。\n没想到，这个作为我主线任务“副产品”的 PR，竟然先一步通过了审核，正式合并！那一刻的喜悦难以言表。我的第一个被大型开源项目合并的 PR，就这样诞生了！\n主线达成：为华为基础软件贡献代码！ 有了官方镜像的加持，我为 splitter 增加 gen 命令的PR也很快被顺利合并了，这标志着我的开源之夏项目取得了阶段性的成功。\n这次的感觉又有所不同。虽然我自己在 GitHub 上也发布过一些个人项目，但为 openEuler/splitter 这样重量级的基础软件贡献核心代码，意义完全不一样。它服务于整个 openEuler 的云原生生态，背后是华为和众多社区开发者的努力。能够成为其中一员，哪怕只是贡献了一小部分，也让我感到无比自豪。\n后续工作 开源贡献不是一次性的，而是一个持续迭代的过程。在核心功能合并后，我立刻投入到了后续的优化工作中：\n提供便携的工具入口：我提交了新的 PR #20，为项目增加了一个splitter-docker.sh脚本。它封装了所有 Docker 操作，让任何用户都可以通过一条简单的命令，使用官方镜像来运行splitter的cut和gen命令，极大地降低了使用门槛。 在写文章的过程中，这个 PR 也审核通过，成功合并了！\n更新官方镜像：随着splitter的版本迭代（比如gen命令的加入），官方镜像也需要更新。我提交了新的 PR #1023来将镜像中的splitter版本升级到最新。 文章还没发布，这个 PR 也审核通过，成功合并了！\n这让我深刻体会到，一个功能的完成，往往是另一个优化的开始。\n结语 这次开源之夏的经历，让我从一个开源的旁观者，变成了一个真正的参与者和贡献者。我不仅学到了如何设计和实现一个健壮的工具，更学会了如何在社区中沟通、协作，以及如何遵循大型项目的规范和流程。\n感谢开源之夏提供了这么好的平台，感谢 openEuler 社区的开放和包容，更要感谢我的导师鲁卫军老师的一路悉心指导。这段旅程才刚刚开始，未来，我希望能为开源世界贡献更多力量。\n","date":"2025-08-21T10:48:37+08:00","permalink":"https://mahaoliang.tech/p/%E6%88%91%E5%9C%A8%E5%BC%80%E6%BA%90%E4%B9%8B%E5%A4%8F%E4%B8%BA-openeuler-%E6%8F%90%E4%BA%A4%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA-pr-%E8%A2%AB%E5%90%88%E5%B9%B6%E4%BA%86/","title":"我在开源之夏为 openEuler 提交的第一个 PR 被合并了！"},{"content":"作为开发者，我们经常需要通过 SSH 连接到远程 Linux 服务器进行开发。工具如 VS Code 的 Remote-SSH 插件，让我们几乎感觉不到自己是在一台远程机器上工作。但一个常见的痛点随之而来：SSH 密钥管理。\n我们希望：\n使用 SSH 密钥登录服务器，并向 Git 仓库（如 GitHub）推送代码。 对每一次 Git 提交进行签名，以验证提交的来源。 最重要的一点：不希望将包含私钥的任何文件拷贝到远程服务器上，以防服务器被入侵导致私钥泄露。 幸运的是，通过 1Password 内置的 SSH Agent 和 SSH Agent Forwarding 技术，我们可以完美地解决这个问题。本文将带你一步步配置，实现安全、无缝的远程开发流程。\n核心概念简介 在开始之前，我们先简单了解几个关键概念：\nSSH Agent (SSH 代理)：它就像一个临时的密钥管理员。你可以在会话开始时，将解密的私钥（decrypted private key）加载到 Agent 中。之后，任何需要使用该密钥的 SSH 操作都会向 Agent 请求，而无需你反复输入密码。当你关闭终端会话时，Agent 也会随之关闭，密钥被安全地清除。\n1Password SSH Agent：1Password 8 及以上版本内置了一个功能强大的 SSH Agent。它将你的 SSH 私钥安全地存储在 1Password 保管库中，并通过一个安全的套接字文件（socket file）与你的系统交互。这意味着你的私钥永远不会以明文形式存在于磁盘上，所有使用请求都需要经过 1Password 的授权（例如 Touch ID 或主密码）。\nSSH Agent Forwarding (SSH 代理转发)：这是一个非常强大的 SSH 功能。当你从本地电脑 SSH 到远程服务器时，它可以建立一个安全通道，将远程服务器上需要密钥认证的操作请求，“转发”回你的本地电脑，交由你本地的 SSH Agent 来处理。这样一来，远程服务器本身完全不需要存储任何私钥。\n我们的目标流程 在本地电脑上，1Password 管理着我们的 SSH 私钥。 通过 VS Code Remote-SSH 或终端连接到远程服务器，并启用 Agent Forwarding。 在远程服务器上，执行 git push 时，认证请求被转发回本地，由 1Password 处理。 在远程服务器上，执行 git commit 时，签名请求也被转发回本地，由 1Password 处理和授权。 配置流程 前提条件 你已经安装了 1Password 8 或更高版本的桌面客户端。 你的 SSH 密钥已经创建并保存在 1Password 的 SSH 密钥 分类中。 第一步：配置本地电脑，让 SSH 使用 1Password 首先，我们需要告诉本地的 SSH 客户端，让它把所有密钥相关的请求都交给 1Password 处理。\n在 1Password 中启用 SSH Agent\n打开 1Password 桌面应用。 进入 设置 -\u0026gt; 开发者。 勾选 使用 SSH 代理。 配置本地 SSH 配置文件 (~/.ssh/config)\n参考 1password 的官方文档，我们有两种方式告诉 SSH 客户端 Agent 在哪里：IdentityAgent 指令和 SSH_AUTH_SOCK 环境变量。推荐使用 IdentityAgent。\n编辑你本地电脑上的 ~/.ssh/config 文件（如果不存在，请创建它）。在文件顶部添加以下内容：\n1 2 3 # 告诉所有 SSH 连接 (*) 都使用 1Password 的 Agent Host * IdentityAgent \u0026#34;~/Library/Group Containers/2BUA8C4S2C.com.1password/t/agent.sock\u0026#34; 验证配置\n在本地电脑的终端里运行以下命令：\n1 ssh-add -l 如果配置成功，它会列出你在 1Password 中存储的所有 SSH 密钥的公钥指纹。这证明你的本地 SSH 客户端已经成功与 1Password 对接。\n第二步：配置连接，启用 Agent Forwarding 现在，我们需要在连接到特定远程服务器时，启用 Agent Forwarding 功能。最佳实践依然是修改 ~/.ssh/config 文件。\n继续编辑你本地电脑上的 ~/.ssh/config 文件，为你的服务器添加一个专有配置块：\n1 2 3 4 5 # 给你的远程服务器起一个别名，方便连接 Host my-dev-server HostName \u0026lt;your_server_ip_or_domain\u0026gt; User mahaoliang ForwardAgent yes # \u0026lt;-- 关键！启用 Agent Forwarding Host my-dev-server: 这是你连接时使用的快捷别名。 HostName: 服务器的实际 IP 或域名。 User: 你在服务器上的用户名。 ForwardAgent yes: 这就是开启 Agent Forwarding 的开关。 现在，你可以通过 ssh my-dev-server 或在 VS Code Remote-SSH 中直接连接到 my-dev-server，转发功能会自动启用。\n第三步：配置远程服务器上的 Git 这是最后一步，也是最关键的一步。我们需要告诉远程服务器上的 Git，如何使用我们转发过来的 SSH Agent 进行提交签名。\n连接并验证转发\n首先，连接到你的远程服务器：\n1 ssh my-dev-server 连接成功后，在远程服务器的终端上，再次运行验证命令：\n1 ssh-add -l 如果 Agent Forwarding 正常工作，这里显示的输出应该和你本地电脑的输出完全一样！\n如果提示“Could not open a connection to your authentication agent.”，请检查 SSH 服务配置，确保 AllowAgentForwarding yes 已启用。\n使用 vim 打开 /etc/ssh/sshd_config 文件，找到 AllowAgentForwarding 配置项，设置为 yes：\n1 AllowAgentForwarding yes 保存后重启 SSH 服务使配置生效：\n1 systemctl restart sshd 再次连接你的远程服务器，执行 ssh-add -l，确认 Agent Forwarding 正常工作。\n获取用于签名的公钥\nGit 需要知道用哪个具体的密钥来签名。我们需要提供完整的公钥字符串作为标识。在远程服务器上运行：\n1 ssh-add -L 这个命令会列出 Agent 中所有密钥的完整公钥。复制你想要用来签名的那一行，它看起来像这样： ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICxxxxxxxxxxxxxxxxxxxx your-key-comment\n配置远程服务器的 .gitconfig\n现在，编辑你远程服务器上的 ~/.gitconfig 文件。将你原来的配置更新如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 [user] email = mahaoliang@gmail.com name = mahaoliang # 将 signingkey 的值设置为你上一步复制的完整公钥字符串 signingkey = ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICxxxxxxxxxxxxxxxxxxxx your-key-comment [gpg] # 告诉 Git 使用 ssh 程序进行签名 format = ssh [commit] # 让所有提交都默认进行签名 gpgsign = true 最重要的改动在于 signingkey。我们不再使用一个文件路径，而是直接提供了公钥本身。这让 Git 可以直接向转发过来的 Agent 请求使用这个特定的密钥进行签名。\n大功告成！来测试一下吧 一切准备就绪！在远程服务器上，进入你的任意一个 git 项目，尝试创建一个新提交：\n1 git commit --allow-empty -m \u0026#34;Test: Signed commit with 1Password Agent Forwarding\u0026#34; 此时，奇妙的事情发生了：你的本地电脑上会弹出 1Password 的授权请求，提示你应用正在请求使用你的 SSH 密钥。通过 Touch ID 或输入主密码授权后，远程服务器上的 git commit 命令瞬间完成。\n最后，检查一下你的提交日志：\n1 git log --show-signature -1 你将会看到类似下面的输出，Good signature 明确告诉你，这次提交已经由你的密钥成功签名！\n1 2 3 4 5 6 commit \u0026lt;commit_hash\u0026gt; (HEAD -\u0026gt; main) Good \u0026#34;git\u0026#34; signature for mahaoliang@gmail.com with ED25519 key SHA256:GKaU0ZCgehQ73X... Author: mahaoliang \u0026lt;mahaoliang@gmail.com\u0026gt; Date: ... Test: Signed commit with 1Password Agent Forwarding 总结 下图展示了使用 1Password 和 SSH Agent Forwarding 进行远程 Git 提交签名的核心流程。\n图中组件\n本地电脑 (Local)\n开发者：操作的发起者。 1Password SSH Agent：安全存储私钥，并处理所有签名请求。 SSH Client：本地的 SSH 程序，配置为使用 1Password Agent 并启用转发。 远程服务器 (Remote)\n远程终端：开发者在服务器上的工作界面。 Git：版本控制工具，配置为使用 SSH 进行签名。 SSH Daemon：服务器上的 SSH 服务，负责建立安全连接和转发请求。 交互流程详解\n发起提交：开发者在远程终端中执行 git commit 命令。 请求签名：Git 根据配置，向 SSH 服务请求使用密钥进行签名。 转发请求：远程服务器的 SSH Daemon 将签名请求通过加密的 SSH 隧道转发回本地电脑的 SSH Client。这是 Agent Forwarding 的核心。 请求 1Password：本地的 SSH Client 将请求交给 1Password SSH Agent。 请求用户授权：1Password 在本地电脑上弹出提示，向开发者请求授权。 用户授权：开发者在本地进行身份验证（如 Touch ID 或主密码）。 返回签名：授权成功后，1Password 生成签名并返回给本地 SSH Client。 返回签名至远程：签名通过 SSH 隧道被安全地传回远程服务器的 SSH Daemon。 完成提交：远程的 Git 进程收到有效的签名，成功创建提交，并在远程终端中显示成功信息。 通过以上配置，我们构建了一个既安全又便捷的远程开发工作流。你的私钥始终安全地躺在本地的 1Password 保管库中，而远程服务器上的所有 Git 操作（认证和签名）都能够无缝、安全地使用它。这不仅提升了安全性，也大大简化了多服务器环境下的密钥管理，让你能更专注于编码本身。\n","date":"2025-07-29T21:05:21+08:00","permalink":"https://mahaoliang.tech/p/%E4%BD%BF%E7%94%A8-1password-%E5%92%8C-ssh-agent-forwarding-%E6%8F%90%E5%8D%87%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E4%BD%93%E9%AA%8C/","title":"使用 1Password 和 SSH Agent Forwarding 提升远程开发体验"},{"content":"过去一个月，密集学习了各种 Python 相关的工具。\n在我看来，Python 的知识分为两个方面，一个是 Python 语言本身，要学习它的语法，掌握基本数据结构，了解常用的模块，总之就是会写 Python 代码，解决实际问题。\n另一个方面，是 Python 工程方面的知识。所谓工程，就是在开发大型项目时，如何让开发过程更高效：项目复杂了，如何组织代码结构；如何进行依赖管理；如何多人协作；如何做项目间隔离，避免依赖冲突；如何管理多个版本的 Python；如何构建、发布自己写的模块等等。总之就是，除了写代码之外，如何做，让开发 Python 的过程更高效。\n学习的过程中，写了一系列的技术文章。从最开始用 pyenv 管理多版本，venv 和 Conda 管理虚拟环境，到阶段性的总结了虚拟环境工具选择建议，之后深入技术实现原理，探讨了虚拟环境底层实现原理 和 Python 与 pip 的关系，之后总结了 Python 项目管理的发展历程。\n到今天，是这个系统的最后一篇：因为我会用 uv 代替前面介绍的所有工具，用 uv 管理 Python 项目的全流程。我决定了，以后的 Python 项目，都使用 uv 管理。\nuv 简介 uv 是一个用 Rust 编写的 Python 打包和项目管理器，它将多个 Python 工具（如 pip、venv、pyenv 等）的功能整合到一个工具中，提供了一个统一、高效的 Python 开发流程。\n实际上，uv 的实现并不是完全从头开始，而是对 venv 和 pip 的高级封装，对使用者提供更简单、更统一的接口。\n本文将从零开始，使用 uv 完整地走完一个项目的开发、构建和安装流程。\n安装 uv 使用官方脚本安装 推荐参考uv 项目文档，使用官方脚本安装：\n1 2 3 4 5 # macOS and Linux curl -LsSf https://astral.sh/uv/install.sh | sh # Windows powershell -ExecutionPolicy ByPass -c \u0026#34;irm https://astral.sh/uv/install.ps1 | iex\u0026#34; 脚本执行成功后，新开一个 shell，执行 uv --version 命令，查看是否安装成功。\n1 2 ❯ uv --version uv 0.8.3 (7e78f54e7 2025-07-24) PATH 环境变量 uv 被安装到用户目录的 .local/bin 目录下。\n安装脚本会自动在 .zshrc 或 .bashrc 增加配置，将 $HOME/.local/bin 添加到 $PATH 环境变量中。\n1 2 3 $ cat $HOME/.zshrc ... . \u0026#34;$HOME/.local/bin/env\u0026#34; 查看.local/bin/env ，文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 #!/bin/sh # add binaries to PATH if they aren\u0026#39;t added yet # affix colons on either side of $PATH to simplify matching case \u0026#34;:${PATH}:\u0026#34; in *:\u0026#34;$HOME/.local/bin\u0026#34;:*) ;; *) # Prepending path in case a system-installed binary needs to be overridden export PATH=\u0026#34;$HOME/.local/bin:$PATH\u0026#34; ;; esac 可以看出，.zshrc 中这行命令 . \u0026quot;$HOME/.local/bin/env\u0026quot; 的作用，是将 $HOME/.local/bin 添加到 $PATH 中。\nShell 自动补全 建议为 uv 命令启用 shell 自动补全功能：\n1 2 3 4 5 # zsh echo \u0026#39;eval \u0026#34;$(uvx --generate-shell-completion zsh)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc # bash echo \u0026#39;eval \u0026#34;$(uvx --generate-shell-completion bash)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc 升级 uv 通过官方脚本安装 uv 后，可按需自行更新：\n1 uv self update Python 版本管理 可以使用 uv python list 命令查看 uv 支持的所有 Python 版本：\n1 2 3 4 5 6 7 8 9 ❯ uv python list cpython-3.14.0rc1-macos-aarch64-none \u0026lt;download available\u0026gt; cpython-3.13.5-macos-aarch64-none \u0026lt;download available\u0026gt; ... cpython-3.9.6-macos-aarch64-none /usr/bin/python3 ... pypy-3.8.16-macos-aarch64-none \u0026lt;download available\u0026gt; ... graalpy-3.8.5-macos-aarch64-none \u0026lt;download available\u0026gt; uv 会列出所用可用版本，包括系统自带的 Python。\n我们可以指定安装某个版本 Python 版本：\n1 ❯ uv python install cpython-3.13.5 命令执行成功后，Python 将被安装到 ~/.local/bin 目录下：\n1 2 3 4 5 6 7 ❯ ll ~/.local/bin total 73784 -rw-r--r-- 1 haoliangma staff 328B 7 25 21:01 env -rw-r--r-- 1 haoliangma staff 165B 7 25 21:01 env.fish lrwxr-xr-x 1 haoliangma staff 89B 7 25 21:07 python3.13 -\u0026gt; /Users/haoliangma/.local/share/uv/python/cpython-3.13.5-macos-aarch64-none/bin/python3.13 -rwxr-xr-x 1 haoliangma staff 36M 7 25 05:09 uv -rwxr-xr-x 1 haoliangma staff 328K 7 25 05:09 uvx 可以看到，在 ~/.local/bin 目录下建立了符号链接（Symbolic Link），指向 Python 的实际安装位置 ~/.local/share/uv/python。\n值得注意的是，符号链接名称带了具体的版本号 python3.13，也就是说，当前的 python3 命令，仍然是系统自带的 Python 环境。\n1 2 ❯ which python3 /usr/bin/python3 因为我们在项目中都会使用虚拟环境，所以并不在乎全局环境的 Python。这里我们使用 uv 安装的 Python，并不设置为全局环境，只供以后项目的虚拟环境使用。\n后面还会看到，我们可以在 ~/.local/bin 目录下安装可以独立运行的 Python 工具，而这些工具也自带了完整环境，并不会用到全局环境的 Python，工具之间的环境也是完全隔离的。\n项目开发 创建项目 使用 uv init 命令可以快速初始化一个项目。我们可以指定项目名称，init 命令会自动创建项目目录，并生成项目结构。\n1 2 ❯ uv init -p 3.13 skycmd Initialized project `skycmd` at `/Users/haoliangma/Documents/works/skycmd` 实际上，我们不必在上一步安装 Python，因为在项目初始化的时候，如果 -p 参数指定的 Python 版本不存在，uv 会自动下载并安装。\n我们也可以在一个空的项目目录中执行 uv init 命令，这时会使用目录名作为项目名。\n1 2 3 4 5 ❯ mkdir skycmd skycmd ❯ cd skycmd ❯ uv init -p 3.11 Initialized project `skycmd` 在初始化的项目目录下，.python-version 文件会记录项目的 Python 版本号。\npyproject.toml 是最核心的项目配置文件，项目元数据、依赖项、构建配置等等都定义在这里。\n运行初始化项目 uv init 为我们生成一个示例程序 main.py，使用 uv run 命令运行：\n1 2 3 4 ❯ uv run main.py Using CPython 3.13.5 Creating virtual environment at: .venv Hello from skycmd! 第一次运行项目，会自动创建虚拟环境 .venv。\n添加依赖 我准备做一个查看天气的命令行工具，会用到 requests 处理 HTTP 请求，并使用 click 解析命令行参数。\n使用 uv add 命令添加依赖：\n1 2 uv add requests uv add click 注意，不用执行 source .venv/bin/activate 显示的激活虚拟环境，uv 会自动将依赖安装到虚拟环境中。\n安装的依赖，会自动记录在 pyproject.toml 文件中。\n1 2 3 4 dependencies = [ \u0026#34;click\u0026gt;=8.2.1\u0026#34;, \u0026#34;requests\u0026gt;=2.32.4\u0026#34;, ] 使用 uv + pyproject.toml 的方式管理依赖，和使用 pip + requirements.txt 的方式有本质区别。\npyproject.toml 只会记录直接依赖，而 requirements.txt 会记录所有依赖，包括直接依赖和间接依赖。\n这两种方式的区别，在移除依赖项时就体现出来了：\n使用 uv remove requests，会移除 requests 和所有 requests 的依赖项。 而 pip uninstall requests 只会移除 requests，留下了 requests 的依赖项。 配置构建工具 uv 支持多种构建打包工具，这里我们使用常见的 setuptools。\n参考 setuptools 的官方文档，在 pyproject.toml 中添加 build-system 配置：\n1 2 3 [build-system] requires = [\u0026#34;setuptools\u0026#34;] build-backend = \u0026#34;setuptools.build_meta\u0026#34; Python 不是脚本吗，为什么还要构建打包呢？\n首先，因为我们的项目不可能总是一个简单的脚本，随着项目的复杂，可能会分为多个模块，每个模块提供独立、完整的功能。同时，项目还可以作为库发布出去，就像 requests 一样，让别人使用。这时，我们不可能将一个个的 Python 文件发给别人，而是需要使用一个构建打包工具，将我们的项目打包，方便别人安装使用。\n另外，就算不发给别人使用，如果配置了 [build-system]，在开发阶段，我们不必使用 pip install -e . 在虚拟环境安装项目自身模块，而是直接使用 uv run \u0026lt;project_name\u0026gt; 命令就可以运行整个项目。\n源码目录结构 setuptools 在构建时，会按照约定的源码目录结构，自动发现 Python 文件进构打包构建。\nsetuptools 默认支持两种源码目录布局：src-layout 和 flat-layout。也就是说，按照这两种方式放置 Python 文件，不必做额外的配置，setuptools 都能自动找到。\n我们使用简单的 flat-layout，调整项目目录结构如下：\n1 2 3 4 5 6 7 8 skycmd ├── pyproject.toml ├── ... └── skycmd/ ├── __init__.py ├── main.py └── weather/ └── __init__.py 这里有几点说明请注意：\n首先，在项目的根目录下，创建与项目同名的目录 skycmd，用来存放 Python 源码。注意，项目的根目录名，根目录下存放源码的目录名，都要跟 pyproject.toml 定义的项目名称保持一致。\n将 uv 生成的 main.py，从项目根目录，移到源码目录 skycmd 下，作为项目执行入口。\n在 skycmd 目录下，可以创建子目录，作为子模块。例如可以创建 weather 目录，用于放置获取天气信息的模块代码。\n源码目录及其子目录，每层目录下都要建一个空的 __init__.py 文件。因为目录下如果有 __init__.py 文件，Python 会认为它是一个模块。有了 __init__.py 文件，每层目录都会被自动识别为模块。\n配置项目执行入口 在 pyproject.toml 文件中，添加 [project.scripts] 配置项，指定项目执行入口。\n1 2 [project.scripts] skycmd = \u0026#34;skycmd.main:main\u0026#34; skycmd.main:main 表示 skycmd 模块下的 main.py 文件中的 main 函数。\n现在可以使用 uv run skycmd 命令来运行项目了：\n1 2 3 4 ❯ uv run skycmd Built skycmd @ file:///Users/haoliangma/Documents/works/skycmd Installed 1 package in 7ms Hello from skycmd! 因为前面我们前面配置了构建工具，现在又配置了执行入口，在运行 uv run skycmd 时，会自动进入虚拟环境，构建，将构建的包安装到虚拟环境，然后运行项目。\n注意，整个过程，我们都只使用了 uv 命令，没有显示的执行 source .venv/bin/activate 命令来激活虚拟环境，uv 命令会自动激活虚拟环境。\n同时，我们指定的是执行入口 skycmd，而不再是 main.py。\n现在看好像差别不大，但如果项目包含了多个模块，uv run skycmd 命令会自动将项目的所有模块安装到虚拟环境，然后运行入口函数：skycmd 模块的 main.py 文件中的 main 函数。如何运行的是 main.py，将不会自动安装项目自身编写的模块。\n开发服务模块 搭建好了整个项目结构，我们可以开发提供获取天气信息的模块。\n实现很简单，构建 HTTP 请求访问 wttr.in ，提供城市名作为参数，HTTP 响应内容就是城市天气信息。\n在 skycmd/weather 目录下，创建 service.py 文件，并添加如下代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import requests def get_weather_from_wttr(city_name): try: url = f\u0026#34;https://wttr.in/{city_name}?m\u0026amp;format=3\u0026#34; response = requests.get(url, timeout=10) response.raise_for_status() return response.text.strip() except requests.exceptions.RequestException as e: return f\u0026#34;Error getting weather information: {e}\u0026#34; def get_detailed_weather_from_wttr(city_name): try: url = f\u0026#34;https://wttr.in/{city_name}?m\u0026#34; response = requests.get(url, timeout=10) response.raise_for_status() return response.text except requests.exceptions.RequestException as e: return f\u0026#34;Error getting detailed weather information: {e}\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: weather_info = get_weather_from_wttr(\u0026#34;Shenzhen\u0026#34;) print(weather_info) 记得要在 skycmd/weather 目录下要创建一个空的 __init__.py 文件。当前的目录结构如下：\n1 2 3 4 5 6 7 8 9 skycmd ├── pyproject.toml ├── ... └── skycmd/ ├── __init__.py ├── main.py └── weather/ ├── __init__.py └── service.py 我们可以在 skycmd/weather/server.py 中添加测试方法，使用 uv run 运行该模块，验证编写的服务。\n1 2 ❯ uv run skycmd/weather/service.py Shenzhen: ⛅️ +34°C 开发入口程序 获取天气信息的模块已经完成，现在我们编写入口程序。\n修改 skeycmd/main.py文件，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 import requests import os import sys import click from skycmd.weather.service import get_weather_from_wttr, get_detailed_weather_from_wttr @click.command() @click.argument(\u0026#39;city\u0026#39;, type=str, required=False) @click.option(\u0026#39;-v\u0026#39;, \u0026#39;--verbose\u0026#39;, is_flag=True, help=\u0026#39;显示详细天气信息\u0026#39;) def main(city, verbose): \u0026#34;\u0026#34;\u0026#34; 命令行天气工具 - 获取指定城市的天气信息 \\b 使用示例： skycmd # 获取帮助信息 skycmd Shenzhen # 获取深圳的天气 skycmd -v Shenzhen # 获取深圳的详细天气信息 \u0026#34;\u0026#34;\u0026#34; if not city: # 如果没有提供城市名，显示帮助信息 ctx = click.get_current_context() click.echo(ctx.get_help()) ctx.exit() if verbose: # 显示详细天气信息 weather_info = get_detailed_weather_from_wttr(city) print(weather_info) else: # 显示简单天气信息 weather_info = get_weather_from_wttr(city) print(weather_info) if __name__ == \u0026#34;__main__\u0026#34;: main() 现在可以使用 uv run skycmd 运行项目：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ❯ uv run skycmd Usage: skycmd [OPTIONS] [CITY] 命令行天气工具 - 获取指定城市的天气信息 使用示例： skycmd # 获取帮助信息 skycmd Shenzhen # 获取深圳的天气 skycmd -v Shenzhen # 获取深圳的详细天气信息 Options: -v, --verbose 显示详细天气信息 --help Show this message and exit. ❯ uv run skycmd Shenzhen Shenzhen: ⛅️ +34°C 因为前面已经在 pyproject.toml 中配置了执行入口 skycmd = \u0026quot;skycmd.main:main\u0026quot;，所以使用 uv run 的时候，指定的是 skycmd，而不是 Python 文件名 main.py。\n现在我们简单的修改一下程序，增加显示简单天气时的输出内容：\n1 2 3 4 # 显示简单天气信息 weather_info = get_weather_from_wttr(city) print(f\u0026#34;🌍 天气信息：\u0026#34;) print(weather_info) 修改完成后，再次使用 uv run skycmd Shenzhen 运行，可以查看到改动后的效果：\n1 2 3 ❯ uv run skycmd Shenzhen 🌍 天气信息： Shenzhen: ⛅️ +34°C 完美！开发过程顺畅丝滑。\n构建和安装工具 如果想使用我们开发的命令行工具 skycmd，必须每次都要到项目目录下运行 uv run skycmd，这样有点麻烦。\n我们可以先使用 uv build，将整个项目打包成标准的 Python 包：\n1 2 3 4 5 ❯ uv build Building source distribution... ... Successfully built dist/skycmd-0.1.0.tar.gz Successfully built dist/skycmd-0.1.0-py3-none-any.whl 可以看到，构建成功后，在 dist 目录下生成了两个文件：\nskycmd-0.1.0.tar.gz：源代码分发包 skycmd-0.1.0-py3-none-any.whl：二进制分发包 接着可以使用 uv tool install 命令，将 skycmd 安装到用户目录：\n1 2 3 4 5 6 7 8 9 10 11 12 ❯ uv tool install dist/skycmd-0.1.0-py3-none-any.whl Resolved 7 packages in 563ms Prepared 1 package in 4ms Installed 7 packages in 7ms + certifi==2025.7.14 + charset-normalizer==3.4.2 + click==8.2.1 + idna==3.10 + requests==2.32.4 + skycmd==0.1.0 (from file:///Users/haoliangma/works/skycmd/dist/skycmd-0.1.0-py3-none-any.whl) + urllib3==2.5.0 Installed 1 executable: skycmd tool install uv tool install 安装的是二进制分发包 dist/skycmd-0.1.0-py3-none-any.whl。\n命令执行成功，将在 ~/.local/bin 目录下生成 skycmd 命令。\n1 2 3 4 5 ❯ ll ~/.local/bin ... lrwxr-xr-x 1 haoliangma staff 57B 7 25 22:34 skycmd -\u0026gt; /Users/haoliangma/.local/share/uv/tools/skycmd/bin/skycmd -rwxr-xr-x 1 haoliangma staff 36M 7 25 05:09 uv -rwxr-xr-x 1 haoliangma staff 328K 7 25 05:09 uvx 这个 skycmd 是一个符号链接，指向 ~/.local/share/uv/tools/skycmd/bin/skycmd。\n所以，skycmd 工具的实际文件存放在 ~/.local/share/uv/tools/skycmd/目录中。查看这个目录的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 ❯ tree -L 2 ~/.local/share/uv/tools/skycmd/ /Users/haoliangma/.local/share/uv/tools/skycmd/ ├── bin │ ├── activate │ ├── ... │ ├── python -\u0026gt; /Users/haoliangma/.local/share/uv/python/cpython-3.13.5-macos-aarch64-none/bin/python3.13 │ └── skycmd ├── CACHEDIR.TAG ├── lib │ └── python3.13 ├── pyvenv.cfg └── uv-receipt.toml 可以看出，这个目录包含了一个完整的 Python 虚拟环境。所以虽然 skycmd 安装到了用户目录，但它使用自己的虚拟环境，指定了 Pythonn 版本，依赖也安装在自己的环境中。这样，工具之间的环境是完全隔离的，不会相互影响。\n现在，可以在命令行中使用 skycmd 命令了：\n1 2 3 ❯ skycmd Shenzhen 🌍 天气信息： Shenzhen: ⛅️ +29°C 不想用了，记得删除：\n1 2 ❯ uv tool uninstall skycmd Uninstalled 1 executable: skycmd 完美！\n总结 本文详细记录了使用 uv 管理 Python 项目的全流程（虽然没有讲到模块发布）。\nuv 会自动创建虚拟环境，执行 uv 命令会自动在虚拟环境中运行，不需要显示的激活。\n所以使用 uv 时要记得：一切操作都要使用 uv，最好不要 uv 和 pip 混用。\n整理本文提到的主要命令 功能 命令 描述 更新 uv self update 将 uv 更新到最新版本。 Python 版本管理 uv python list 列出所有可用的 Python 版本，包括已安装和可下载的。 uv python install \u0026lt;version\u0026gt; 下载并安装指定版本的 Python。 项目初始化 uv init -p \u0026lt;python_version\u0026gt; \u0026lt;project_name\u0026gt; 初始化一个新的 Python 项目，可指定 Python 版本并自动创建项目目录和基础文件，如 pyproject.toml。 运行代码 uv run \u0026lt;file_or_script\u0026gt; 在项目的虚拟环境中运行 Python 脚本或已配置的入口。首次运行时会自动创建虚拟环境。 依赖管理 uv add \u0026lt;package\u0026gt; 向项目中添加依赖，并自动更新 pyproject.toml 文件。 uv remove \u0026lt;package\u0026gt; 从项目中移除依赖及其相关子依赖。 项目构建 uv build 将项目打包成标准的源代码分发包 (.tar.gz) 和二进制分发包 (.whl)。 工具安装 uv tool install \u0026lt;package_or_wheel\u0026gt; 将一个 Python 包（如本地构建的 .whl 文件）作为一个独立的命令行工具安装。uv 会为其创建一个隔离的虚拟环境。 uv tool uninstall \u0026lt;tool_name\u0026gt; 卸载已安装的命令行工具。 项目开发流程 本文通过创建一个名为 skycmd 的命令行天气查询工具，演示了使用 uv 的完整开发流程：\n初始化项目: 使用 uv init 创建项目结构和 pyproject.toml 配置文件。 添加依赖: 使用 uv add requests click 添加项目所需的第三方库。 配置构建系统: 在 pyproject.toml 中配置 setuptools 作为构建工具，以便后续的构建和打包。 组织源码: 采用 flat-layout 目录结构，将源代码放置在与项目同名的子目录中。 配置执行入口: 在 pyproject.toml 的 [project.scripts] 部分指定项目的命令行入口，使得可以使用 uv run skycmd 来运行。 开发与调试: 在开发过程中，直接使用 uv run 来测试和运行代码，uv 会自动处理环境和依赖。 构建与安装: 开发完成后，使用 uv build 将项目打包，然后使用 uv tool install 将其安装为系统级的命令行工具，方便在任何地方调用。 本文的示例代码在 GitHub，希望本文对你有帮助。\n","date":"2025-07-25T19:54:51+08:00","permalink":"https://mahaoliang.tech/p/python-%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5uv-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/","title":"Python 项目管理最佳实践：uv 使用指南"},{"content":"在很长一段时间里，我管理 Python 项目的方式堪称“原始”。每当项目需要新的依赖，我便会在 README.md 文件里手动记上一笔，提醒自己需要 pip install 哪些库。这种“刀耕火种”的模式，对于个人写的一些小脚本尚可应付，但当项目变得复杂，或是需要与他人协作时，其脆弱和低效便暴露无遗。\n真正的转折点发生在今年夏天。我有幸参与了开源之夏，并成功中选了 openEuler 社区的 面向 openEuler distroless 镜像的 SDF 自动生成工具开发 项目，为 splitter 这个工具贡献代码。当我满怀激情地克隆代码库，准备大展拳脚时，一个文件赫然出现在我的眼前：pyproject.toml。这个我以往只是模糊听闻过的文件，在这里却是项目配置的核心。\n这次经历像一扇窗，让我窥见了现代 Python 项目管理的全新世界。于是，我将这段学习和探索的经历整理成文。\n传统方式：venv + requirements.txt 以 venv 和 requirements.txt 为核心的工作流，是 Python 社区在告别全局安装、走向规范化管理过程中迈出的重要一步。\n环境隔离与依赖列表 虚拟环境 (venv)\n如果你经历过在系统全局 Python 环境中安装各种包的混乱时期，你一定对 “依赖地狱” (dependency hell) 这个词深有体会。项目 A 需要 requests==2.20.0，而项目 B 依赖的另一个库却需要 requests==2.28.0，它们在全局环境中相互冲突，让开发者头痛不已。\nvenv 的出现正是为了解决这个问题。它允许我们为每个项目创建一个独立的、与全局环境隔离的 Python 工作空间。\n1 2 # 在项目根目录下，创建一个名为 .venv 的虚拟环境 python3 -m venv .venv 这个命令会创建一个 .venv 文件夹，里面包含了项目所需的一个迷你的 Python 运行环境。要使用它，我们需要先“激活”：\n1 source .venv/bin/activate 激活后，你会发现命令行提示符前面多了 (.venv) 的标识。此时，所有 pip 的安装、卸载操作都将被限制在这个独立的虚拟环境中，再也不会污染全局环境了。\n依赖文件 (requirements.txt)\n环境隔离了，但如何与他人协作呢？我们总不能把整个 .venv 文件夹都发给别人吧。这时，requirements.txt 就登上了历史舞台。它的作用，就是一份项目的“依赖清单”。\n最常见的生成方式是使用 pip freeze 命令：\n1 2 # 将当前虚拟环境中所有已安装的包及其精确版本号导出 pip freeze \u0026gt; requirements.txt 这样，你的项目协作者或者部署服务器，只需要拿到你的代码和这个 requirements.txt 文件，然后执行一条简单的命令，就可以复现出一个一模一样的运行环境：\n1 2 # 从 requirements.txt 文件中安装所有指定的依赖 pip install -r requirements.txt 这套组合拳极大地提升了 Python 项目的规范性和可复现性，至今仍有大量项目在使用。\n传统方式的隐患 尽管 venv + requirements.txt 解决了大问题，但随着项目复杂度的提升，其内在的缺陷也逐渐暴露出来。\n依赖混淆\n最大的问题在于 pip freeze 的工作方式。它像一个不加分辨的记录员，会把你环境中所有的包都记录下来。这其中既包含了你为了实现功能而主动安装的直接依赖（比如 Web 框架 Flask），也包含了 Flask 运行所必需的间接依赖（比如 Werkzeug, Jinja2, click 等）。\n最终生成的 requirements.txt 文件看起来会是这样：\n1 2 3 4 5 6 7 8 # requirements.txt blinker==1.7.0 click==8.1.7 Flask==3.0.0 itsdangerous==2.1.2 Jinja2==3.1.3 MarkupSafe==2.1.3 Werkzeug==3.0.1 在这个文件里，你已经分不清谁是谁的依赖了。项目的核心依赖关系被模糊掉了，给后续的维护，比如升级某个特定的核心库，带来了不小的麻烦。\n孤儿依赖\n另一个令人头疼的问题是“孤儿依赖”。假设你的项目后续不再需要 Flask 了，于是你执行了 pip uninstall flask。pip 很听话地卸载了 Flask 本身，但它当初为了 Flask 而自动安装的 blinker、click 等间接依赖，却被遗留在了环境中，变成了无人认领的“孤儿”。\n日积月累，你的虚拟环境会因为这些残留的孤儿依赖而变得越来越臃肿，还可能在未来引发难以预料的依赖冲突。\n正是因为这些的缺陷，Python 社区开始探索一种更清晰、更智能的管理方案。这便引出了我们下一章的主角——pyproject.toml。\n现代篇章：pyproject.toml 面对 requirements.txt 带来的依赖混淆问题，Python 社区需要一个更强大、更规范的解决方案，答案就是 pyproject.toml。\n标准的诞生 (PEP 518 \u0026amp; 621) 在 pyproject.toml 成为标准之前，一个 Python 项目的配置信息可谓“四分五裂”。项目元数据可能在 setup.py 或 setup.cfg 里，运行依赖在 requirements.txt 中，测试配置在 tox.ini 或 .coveragerc 里，代码格式化工具 black 和静态检查工具 mypy 又有它们各自的配置文件。这种碎片化的状态让项目维护变得异常繁琐。\nPEP 518 的提出正是为了终结这种乱象。它定义了一个名为 pyproject.toml 的文件格式，旨在成为所有构建工具的统一配置入口。随后，PEP 621 进一步规范了如何在这个文件中声明项目的核心元数据（如名称、版本、作者和依赖项），使其彻底摆脱了对 setup.py 的依赖。\n简单来说，pyproject.toml 的使命就是将所有与项目相关的配置，集中到一个官方认可的、格式统一的文件中。\n声明式依赖管理 pyproject.toml 最重要的改进，在于它引入了声明式依赖管理。与 pip freeze 那种不加区分的全量记录不同，我们现在只需要在 pyproject.toml 文件中清晰地声明项目的直接依赖。\n让我们回到上一章的 Flask 项目，它的 pyproject.toml 文件现在会是这样：\n1 2 3 4 5 6 7 # pyproject.toml [project] name = \u0026#34;my-flask-app\u0026#34; version = \u0026#34;0.1.0\u0026#34; dependencies = [ \u0026#34;Flask==3.0.0\u0026#34; ] 看，多么清爽！dependencies 列表里只有 Flask。我们在这里表达的是意图：“我的项目需要 Flask 3.0.0 版本”，而不是 Flask 运行所需要的所有包的冗长列表。至于 Flask 自身依赖的 Werkzeug、Jinja2 等，将由工具在安装时自动去解析和处理。\n这种方式将项目的直接依赖与间接依赖彻底分离，让依赖关系一目了然，极大地提升了项目的可读性和可维护性。\npip install . 的背后原理 现在我们有了 pyproject.toml，那么 pip 是如何利用它来安装项目的呢？\n当你进入项目根目录，在激活的虚拟环境中执行 pip install . 背后其实发生了两个关键步骤：构建和安装。这个过程不仅仅是复制文件，而是将你的项目变成一个标准的、可分发的 Python 软件包。\n第一步：构建 (Build)\npip 首先会扮演一个“构建前端”的角色。它会读取 pyproject.toml 文件中的 [build-system] 表，找到指定的“构建后端”（通常是 setuptools）。然后，pip 会指示构建后端，依据当前项目的源代码和 pyproject.toml 中的元数据，构建出一个标准的 Python 软件包，通常是一个 .whl (wheel) 文件。这个 wheel 文件是一个包含了所有代码和元数据的 zip 压缩包，是现代 Python 的标准分发格式。\n第二步：安装 (Install)\n构建完成后，pip 会接手这个新鲜出炉的 wheel 文件，并将其内容“解压”并安装到你的虚拟环境中。这个过程会产生以下三类核心产物：\n项目代码与元数据 (Project Code and Metadata) 你的项目代码（即所有的 .py 文件和包）会被复制到虚拟环境的 site-packages 目录下（例如 .venv/lib/python3.11/site-packages/my_flask_app）。\n同时，一个名为 my_flask_app-0.1.0.dist-info 的元数据目录也会被创建。你可以把它看作是这个软件包的“身份证”，里面包含了从 pyproject.toml 中提取的所有信息，比如项目名、版本、作者以及最重要的——依赖列表。\n项目依赖 (Project Dependencies) pip 会读取上述元数据文件，找到其中声明的 dependencies 列表（例如 \u0026quot;Flask==3.0.0\u0026quot;）。\n然后，它会自动下载并安装所有这些直接依赖，以及这些依赖所需要的间接依赖（如 Werkzeug, Jinja2 等），并将它们全部安装到 site-packages 目录中。\n可执行的命令行脚本 (Executable Command-line Scripts) 如果你在文件中定义了 [project.scripts] 部分，像这样：\n1 2 [project.scripts] my-app = \u0026#34;my_flask_app.cli:main\u0026#34; 那么 pip 在安装时，会在虚拟环境的 bin 目录（Windows 上是 Scripts）下创建一个名为 my-app 的可执行文件。\n这个文件是一个小小的“启动器”脚本，它的作用是调用当前虚拟环境中的 Python 解释器，并执行你指定的函数 (my_flask_app.cli:main)。\n正因为如此，一旦安装完成并激活了虚拟环境，你就可以在任何路径下直接通过命令行运行 my-app 来启动你的应用程序了。\n通过这套标准化的流程，pip install . 不仅安装了代码和依赖，还完成了命令行工具的创建，将一个项目从一堆源代码变成了一个功能完整、随时可用的工具。\n可编辑模式 (pip install -e .) pip install . 非常适合用于最终的安装和部署，但在日常开发中，每次修改代码后都重新构建和安装一遍，显然效率太低。为此，pip 提供了一种强大的可编辑模式 (editable mode)。\n1 pip install -e . -e 参数是这个模式的关键。执行这条命令后，pip 不会再把你的项目文件复制到 site-packages 目录中，而它会在 site-packages 里创建一个特殊的链接文件（.pth 文件），这个链接直接指向你当前项目的源代码目录。\n这样做的好处是显而易见的：你的项目源代码和虚拟环境中的“已安装版本”实现了实时同步。你在编辑器里对任何 .py 文件做的修改，保存后会立即生效，无需任何重新安装的步骤。这极大地简化了“修改 - 运行 - 调试”的开发循环，是现代 Python 开发的必备技巧。\n通过 pyproject.toml 和可编辑模式，我们不仅拥有了清晰的依赖管理，还获得了高效的开发体验。接下来，让我们通过一个真实的项目，来看看 pyproject.toml 在实战中是如何发挥作用的。\n解析splitter 项目的 pyproject.toml 以我参与的 splitter 项目为例，解析它的 pyproject.toml 文件，看看它是如何将项目的所有配置信息尽收囊中的。\n以下是 splitter 项目中 pyproject.toml 文件的核心内容，为便于说明，已做适当简化：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # pyproject.toml of splitter project [build-system] requires = [\u0026#34;setuptools\u0026#34;, \u0026#34;wheel\u0026#34;] build-backend = \u0026#34;setuptools.build_meta\u0026#34; [project] name = \u0026#34;splitter\u0026#34; version = \u0026#34;1.0.0\u0026#34; # Assuming a version for clarity description = \u0026#34;A tool for splitting software packages into smaller components.\u0026#34; authors = [{name = \u0026#34;openEuler Cloudnative SIG\u0026#34;}] readme = \u0026#34;README.md\u0026#34; requires-python = \u0026#34;\u0026gt;=3.7\u0026#34; license = {text = \u0026#34;MulanPSL-2.0\u0026#34;} dependencies = [ \u0026#34;PyYAML\u0026#34;, \u0026#34;click\u0026#34;, \u0026#34;packaging\u0026#34;, \u0026#34;jinja2\u0026#34; ] [project.urls] Homepage = \u0026#34;https://gitee.com/openeuler/splitter\u0026#34; [project.scripts] splitter = \u0026#34;tools.main:main\u0026#34; [tool.setuptools.packages.find] where = [\u0026#34;.\u0026#34;] exclude = [\u0026#34;tests\u0026#34;, \u0026#34;docs\u0026#34;] 现在，让我们逐段来剖析这个文件。\n[build-system] 1 2 3 [build-system] requires = [\u0026#34;setuptools\u0026#34;, \u0026#34;wheel\u0026#34;] build-backend = \u0026#34;setuptools.build_meta\u0026#34; 这是 pyproject.toml 的“入口”，也是遵循 PEP 518 规范的体现。它告诉 pip 这样的构建工具：在构建本项目之前，请确保你的环境中安装了 setuptools 和 wheel 这两个包，因为它们是我需要的构建工具。然后，请使用 setuptools.build_meta 这个入口点来执行实际的构建操作。\n这一段配置，实现了项目构建依赖与项目本身运行依赖的解耦。\n[project] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [project] name = \u0026#34;splitter\u0026#34; version = \u0026#34;1.0.0\u0026#34; description = \u0026#34;...\u0026#34; authors = [...] readme = \u0026#34;README.md\u0026#34; requires-python = \u0026#34;\u0026gt;=3.7\u0026#34; license = {text = \u0026#34;MulanPSL-2.0\u0026#34;} dependencies = [ \u0026#34;PyYAML\u0026#34;, \u0026#34;click\u0026#34;, \u0026#34;packaging\u0026#34;, \u0026#34;jinja2\u0026#34; ] 这部分是项目的核心元数据，遵循 PEP 621 规范。\nname, version, description, authors：这些都是项目的基本信息，会被打包到软件中，并在 PyPI 等包索引网站上展示。\nrequires-python：这是一个非常重要的字段，它声明了项目运行所需的最低 Python 版本。如果用户尝试在一个不兼容的 Python 版本（如 Python 3.6）上安装，pip 会直接报错并终止安装，避免了后续可能出现的各种运行时错误。\ndependencies：这里是项目的依赖。它清晰地列出了 splitter 运行所必需的直接依赖项。当执行 pip install . 时，pip 会负责安装 PyYAML, click, packaging, jinja2 以及它们各自的所有子依赖，而我们无需关心这些复杂的依赖链。\n[project.scripts] 1 2 [project.scripts] splitter = \u0026#34;tools.main:main\u0026#34; 这一行配置，定义了一个名为 splitter 的命令行入口。\n当项目被安装后，pip 会在虚拟环境的 bin/ 目录下创建一个名为 splitter 的可执行文件。当我们运行这个命令时，系统会自动调用 tools/main.py 文件中的 main() 函数。这使得一个复杂的 Python 项目可以像一个普通的系统命令一样被调用，极大地提升了用户体验。\n[tool.setuptools.packages.find] 1 2 3 [tool.setuptools.packages.find] where = [\u0026#34;.\u0026#34;] exclude = [\u0026#34;tests\u0026#34;, \u0026#34;docs\u0026#34;] 前缀为 [tool.*] 的表是为各种第三方工具预留的配置空间。这里，我们为构建后端 setuptools 提供了配置。\npackages.find 指示 setuptools 自动发现项目中的所有 Python 包。where = [\u0026quot;.\u0026quot;] 告诉它从当前根目录开始查找，而 exclude = [\u0026quot;tests\u0026quot;, \u0026quot;docs\u0026quot;] 则明确排除了测试代码和文档目录，确保它们不会被打包到最终发布的应用中。\n通过这个真实的例子，我们可以看到 pyproject.toml 如何将项目的构建信息、元数据、运行依赖、命令行入口和工具配置，全部地组织在了一起，让 Python 项目的结构变得清晰和标准化。\nPoetry, UV, PDM 等高级管理工具 venv + pyproject.toml + pip 的组合，已经构建起了一个相当稳固和规范的项目管理框架，它解决了依赖隔离和声明的核心问题。然而，这个工作流依然存在一些需要开发者手动操作的环节：\n手动管理虚拟环境：你需要记得先创建，再激活。\n手动编辑依赖文件：添加或移除依赖时，你需要手动去编辑 pyproject.toml 文件。\n有没有一种工具，能将这些步骤完全自动化呢？当然有。这便是 Poetry, PDM 以及新秀 uv 这类高级项目管理工具的使命所在。\n新一代高级管理工具 这些工具并非要推翻 venv 和 pyproject.toml，恰恰相反，它们是建立在这些官方标准之上的高级封装和工作流引擎。它们的核心理念可以概括为：\n自动化环境管理：你不再需要关心 venv 的创建和激活，工具会自动为你处理好一切。\n命令式依赖操作：通过简单的命令（如 add, remove）来管理依赖，工具会自动更新 pyproject.toml 文件。\n确定性构建：通过生成一个精确的 lock 文件（如 poetry.lock, pdm.lock, uv.lock），锁定项目中所有依赖（包括直接和间接依赖）的精确版本。这确保了任何人在任何时间、任何机器上都能构建出完全一致的运行环境。\n集成化体验：将依赖管理、环境管理、打包、发布等功能集成到一套统一的命令行接口中，提供“一站式”的解决方案。\n以 uv 为例的现代化工作流 uv 是由 ruff 的作者开发的最新一代 Python 打包工具，它用 Rust 编写。让我们以 uv 为例，体验一下现代化的工作流是多么流畅。\n假设你已经通过 pip install uv 安装了它。\n开发者 A (项目创建者)\n初始化项目 在一个空目录中，我们不再需要手动创建任何文件。\n1 2 # uv 会引导你创建 pyproject.toml 文件 uv init 添加依赖 现在，想给项目添加 flask 依赖？告别手动编辑，一条命令即可：\n1 uv add flask 这条命令，uv 在背后为你完成了一系列操作：\n检查并创建虚拟环境：它会自动检测当前目录下是否存在 .venv，如果没有，就为你创建一个。\n修改 pyproject.toml：自动将 flask 添加到 [project.dependencies] 列表中。\n解析依赖并安装：解析 flask 的所有依赖树，并将它们全部安装到虚拟环境中。\n生成锁文件：创建一个 uv.lock 文件，里面精确记录了本次安装的所有包（包括间接依赖）的版本号和哈希值，锁定了当前环境的状态。\n开发者 B (协作者)\n同步环境 当开发者 B 从 GitHub 克隆了项目后，他不需要再去研究 pyproject.toml 或执行复杂的安装命令。他只需要：\n1 uv sync uv 会读取 uv.lock 文件，然后下载并安装所有被锁定的包，为他创建一个与开发者 A 完全一致的虚拟环境。\n日常开发与执行\n在开发过程中，你甚至不需要手动 source .venv/bin/activate 来激活环境。\n1 2 # uv 会自动在项目的虚拟环境中执行 python main.py uv run python main.py uv run 命令会自动寻找并使用当前项目的虚拟环境来执行后续的命令，让你的操作更加简洁。\n通过 uv 的演示，我们可以看到，现代化的项目管理工具将开发者从繁琐的、易出错的手动操作中彻底解放出来。它们通过自动化和确定性的机制，让依赖和环境管理变得简单、可靠且高效。\n好的，这是最后一章的总结部分。它将对全文进行回顾，并提炼出具体、可操作的最佳实践，为读者画上一个圆满的句号。\n构建你的现代化 Python 工作流 无论你正在开始一个新项目，还是打算重构一个旧项目，请遵循以下三个原则。\n1. 隔离是基础：始终为你的项目创建虚拟环境\n这是现代化项目管理的基石，也是最不应该被忽略的一步。为每个项目创建一个独立的虚拟环境，可以从根源上杜绝依赖冲突和全局环境污染。像 uv, Poetry, PDM 这类工具已经将这一步完全自动化，你甚至无需再手动操作。如果你仍在使用原生工具，请务必将 python -m venv .venv 作为你开启任何新项目的第一条命令。\n2. 声明是核心：拥抱 pyproject.toml，告别 requirements.txt\npyproject.toml 是 Python 项目的未来。请将它作为你项目配置的唯一真实来源。\n只声明直接依赖：在 [project.dependencies] 中，只列出你的项目代码直接 import 的那些库。这能让你的项目依赖关系保持最大程度的清晰和可维护性。\n统一所有配置：将代码检查工具 (linter)、格式化工具 (formatter)、测试框架等所有工具的配置，都迁移到 pyproject.toml 的 [tool.*] 表中，让项目根目录保持整洁。\n3. 工具是利器：选择一个现代化的管理工具\n虽然你可以手动维护 pyproject.toml 并结合 pip 使用，但一个现代化的项目管理工具能极大地提升你的生产力。\n对于新项目：强烈推荐直接使用 uv 或 Poetry。它们提供的 add, remove, sync, run 等命令，将依赖管理和环境操作的体验提升到了一个全新的高度。它们带来的确定性构建（通过 lock 文件）对于团队协作和持续集成（CI/CD）至关重要。\n对于现有项目：迁移到这些工具也比你想象的要简单。大多数工具都提供了从 requirements.txt 导入依赖的功能，可以帮助你平滑过渡。\n写在最后 希望这篇指南能够帮助你理清 Python 项目管理的脉络，并为你提供一套清晰、可行的实践方案。现在就动手，为你的下一个 Python 项目开启一个现代化的新起点吧！\n","date":"2025-07-23T18:39:02+08:00","permalink":"https://mahaoliang.tech/p/%E7%8E%B0%E4%BB%A3-python-%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E6%8C%87%E5%8D%97/","title":"现代 Python 项目管理指南"},{"content":"Python 与 pip 的关系 刚开始学 python 的时候，知道 python 是一个解释器，而 pip 是一个包管理器，以为它们的“地位”是平等的。然而随着了解的深入才发现，pip 本身就是一个 Python 包，它的运行，完全依赖于 python 解释器。\n简单来说，它们的关系可以这样定义：\nPython 是解释器：它是一种高级编程语言，我们编写的 .py 脚本需要通过 Python 解释器来执行，将代码转换成机器可以理解的指令。没有 Python 解释器，我们写的 Python 代码就是一堆没有生命的文本文件。\npip 是包管理器：可以把它想象成 Python 世界的“应用商店 (App Store)”。它是一个命令行工具，专门用来查找、下载、安装、卸载和管理 Python 的第三方软件包（也称为库或模块）。\n最关键的一点是：pip 本身也是一个用 Python 编写的包。这意味着 pip 的运行离不开 python 解释器的支持，它们是一个密不可分的组合，Python 提供了运行环境，而 pip 则极大地丰富 Python 的能力。\n为什么需要包管理？ 在没有包管理的早期，开发者如果想使用别人写好的代码，可能需要手动去网站下载源码压缩包，解压后自己想办法放到项目特定的目录下，如果这个代码还依赖其他代码，整个过程将成为一场噩梦。pip 的出现彻底改变了这一切：\n代码重用与效率提升：当需要实现一个复杂的功能时，比如发送网络请求，很大概率已经有非常成熟的第三方库存在了。通过一条简单的 pip install requests 命令，你就能获得强大的网络请求能力，无需从零开始编写复杂的底层代码。\n自动处理依赖关系：一个库通常会依赖其他多个库才能正常工作。例如，你安装 A 库，A 库可能需要 B 库的 1.5 以上版本和 C 库的任意版本。如果手动管理，这将变得极其繁琐且容易出错。pip 则能自动分析这些依赖关系，将所有需要的“配料”一次性、按正确的版本要求准备妥当。\n确保版本一致性：pip 允许我们将项目所有依赖包记录在一个 requirements.txt 文件中。这样，任何人在任何地方，都可以通过这个文件创建出完全相同的运行环境。\n捆绑安装 为了强调 pip 的重要性，自 Python 3.4 版本以来，官方的 Python 安装程序已经默认将 pip 捆绑在内。这意味着，当你从官方渠道下载并安装 Python 时，pip 工具也一并被安装到了你的系统中。\n精准定位：我的 Python 和 pip 在哪里？ 在日常开发中，我们常常会在电脑上安装多个版本的 Python，例如系统自带的 Python、通过 Homebrew 安装的 Python、以及使用 pyenv 等版本管理工具安装的多个 Python。这就带来了一个核心问题：当我们在终端里敲下 python3 或 pip3 命令时，我们到底在调用哪一个？\n快速定位：which 和 where 命令 最直接的定位方法是使用操作系统提供的路径查找命令。\n在 macOS 或 Linux 系统中，我们可以使用 which 命令： 1 2 3 4 5 # 查看 python3 可执行文件的路径 which python3 # 查看 pip3 可执行文件的路径 which pip3 在 Windows 系统中，对应的命令是 where： 1 2 3 4 5 # 查看 python3 可执行文件的路径 where python3 # 查看 pip3 可执行文件的路径 where pip3 这些命令会搜索系统的环境变量 PATH，并返回找到的第一个匹配的可执行文件的完整路径。\n真正的 Python 解释器在哪里？ 当你使用像 pyenv 这样的 Python 版本管理工具时，which 命令的结果可能会让你感到困惑：\n1 2 ❯ which python3 /Users/haoliangma/.pyenv/shims/python3 这个路径指向的并不是一个真实的 Python 解释器，而是一个叫做“shim”的转发脚本。当你执行 python3 命令时，实际上是这个“shim”接到了指令。它会检查你当前目录或全局设置，判断你希望使用哪个版本的 Python（例如 3.10.18），然后再将你的命令无缝地转接给那个版本对应的真实解释器。\n那么，如何才能找到真正的 Python 解释器？\n执行下面的命令，返回真实解释器路径：\n1 ❯ python3 -c \u0026#34;import sys; print(sys.executable)\u0026#34; 通过这个命令，我们可以清晰地看到真相：\n1 2 3 4 5 6 7 # 由 pyenv 管理的 Python ❯ python3 -c \u0026#34;import sys; print(sys.executable)\u0026#34; /Users/haoliangma/.pyenv/versions/3.10.18/bin/python3 # macOS 系统自带的 Python ❯ python3 -c \u0026#34;import sys; print(sys.executable)\u0026#34; /Applications/Xcode.app/Contents/Developer/usr/bin/python3 这个路径才是我们正在使用的 Python 解释器所在的位置。\n确认 pip 的真实位置 同样地，我们也需要确认 pip 的真实位置。可以使用 pip3 --version 命令：\n1 2 3 4 5 6 7 # 由 pyenv 管理的 Python ❯ pip3 --version pip 23.0.1 from /Users/haoliangma/.pyenv/versions/3.10.18/lib/python3.10/site-packages/pip (python 3.10) # macOS 系统自带的 Python ❯ pip3 --version pip 21.2.4 from /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages/pip (python 3.9) 我们还可以用 pip 自己来“调查”自己，因为 pip 本身也是一个包：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 由 pyenv 管理的 Python ❯ pip3 show pip Name: pip Version: 23.0.1 Summary: The PyPA recommended tool for installing Python packages. Home-page: https://pip.pypa.io/ Author: The pip developers Author-email: distutils-sig@python.org License: MIT Location: /Users/haoliangma/.pyenv/versions/3.10.18/lib/python3.10/site-packages Requires: Required-by: # macOS 系统自带的 Python ❯ pip3 show pip Name: pip Version: 21.2.4 Summary: The PyPA recommended tool for installing Python packages. Home-page: https://pip.pypa.io/ Author: The pip developers Author-email: distutils-sig@python.org License: MIT Location: /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages Requires: Required-by: pip install 的三个目的地 当我们执行 pip install \u0026lt;package_name\u0026gt; 时，这个包究竟被安装到哪里了？\npip install 有三个不同的目的地：全局 Site-Packages、用户 Site-Packages 和虚拟环境 Site-Packages。\n全局安装：便捷但危险的默认选项 这是最直接的安装方式，但通常也是最不推荐的方式。\n当你的全局 Python 环境拥有写入权限时（例如，使用 pyenv、Homebrew 或在 Windows 上默认安装的 Python），执行 pip install package_name，会将 package 安装到全局 site-packages 目录下。\n如何知道全局 site-packages 具体在哪里？\n可以通过 pip show 查看已安装包的位置，得知 site-packages 的位置：\n1 2 3 4 5 6 7 8 9 ❯ pip3 install pyfiglet Successfully installed pyfiglet-1.0.3 ❯ pip3 show pyfiglet Name: pyfiglet Version: 1.0.3 ... Location: /Users/haoliangma/.pyenv/versions/3.10.18/lib/python3.10/site-packages ... 输出中的 Location 字段地告诉我们，pyfiglet 被安装的具体位置。由于是在全局环境执行的安装，这个位置就是全局 site-packages 目录。\n在安装之前，你可以通过编程方式，预先知道全局 site-packages 的路径：\n1 2 ❯ python3 -c \u0026#34;import site; print(site.getsitepackages())\u0026#34; [\u0026#39;/Users/haoliangma/.pyenv/versions/3.10.18/lib/python3.10/site-packages\u0026#39;] 对于 macOS 自带的 Python，site.getsitepackages() 可能会返回一个包含多个路径的列表：\n1 2 python3 -c \u0026#34;import site; print(site.getsitepackages())\u0026#34; [\u0026#39;/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages\u0026#39;, \u0026#39;/Applications/Xcode.app/Contents/Developer/AppleInternal/Library/Python/3.9/site-packages\u0026#39;, \u0026#39;/Library/Python/3.9/site-packages\u0026#39;, \u0026#39;/AppleInternal/Library/Python/3.9/site-packages\u0026#39;, \u0026#39;/AppleInternal/Tests/Python/3.9/site-packages\u0026#39;] 这意味着 Python 在查找全局包时，会按照列表中的顺序依次搜索这些目录。这是一种分层系统，允许系统本身、Xcode 工具链以及管理员安装的包共存。\n我们应该极力避免全局安装。这种方式会“污染”你的主 Python 环境，一旦项目增多，极易导致不同项目间的依赖版本冲突。\nmacOS 系统自带的 Python，普通用户没有权限进行全局安装，除非你明确使用 sudo 执行安装。\n用户目录安装 在两种情况下，pip install 会将包安装到用户级别的 site-packages 目录下。\n第一种情况是被动触发，当执行全局安装没有权限时，pip 会自动将包安装到用户级别的 site-packages，例如使用 macOS 系统自带的 Python 时：\n1 2 3 4 ❯ pip3 install pyfiglet Defaulting to user installation because normal site-packages is not writeable ... Successfully installed pyfiglet-1.0.3 全局目录不可写入，但安装还是成功了，实际上是将包安装到了用户目录。查看刚刚的安装包，可以知道用户目录的具体位置。\n1 2 3 4 5 6 7 8 9 10 11 ❯ pip3 show pyfiglet Name: pyfiglet Version: 1.0.3 Summary: Pure-python FIGlet implementation Home-page: https://github.com/pwaller/pyfiglet Author: Peter Waller (Thanks to Christopher Jones and Stefano Rivera) Author-email: p@pwaller.net License: MIT Location: /Users/haoliangma/Library/Python/3.9/lib/python/site-packages Requires: Required-by: 第二种情况是，通过 --user 参数，明确要求将包安装在自己的用户目录下。\n1 2 3 4 5 6 7 8 9 10 11 12 ❯ pip3 install --user pyfiglet ❯ pip3 show pyfiglet Name: pyfiglet Version: 1.0.3 Summary: Pure-python FIGlet implementation Home-page: https://github.com/pwaller/pyfiglet Author: Peter Waller (Thanks to Christopher Jones and Stefano Rivera) Author-email: p@pwaller.net License: MIT Location: /Users/haoliangma/.local/lib/python3.10/site-packages Requires: Required-by: 这次使用的是 pyenv 管理的 python，包被安装到用户的 .local/lib/python3.10/site-packages 目录中。\n我们还发现，不同方式安装的 Python，可能有不同的用户目录。不用安装包，可以执行python3 -m site --user-site 命令，获得当前 Python 的用户目录：\n1 2 3 4 5 6 7 # pyenv 管理的 python ❯ python3 -m site --user-site /Users/haoliangma/.local/lib/python3.10/site-packages # macOS 系统自带的 python ❯ python3 -m site --user-site /Users/haoliangma/Library/Python/3.9/lib/python/site-packages 虚拟环境安装 虚拟环境是为每个项目创建一个独立的，与其他项目隔离的“沙盒”。\n在已激活 (activated) 的虚拟环境中执行 pip install，会将包安装到虚拟环境的 site-packages 目录下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 在项目目录下创建一个名为 .venv 的虚拟环境 ❯ python3 -m venv .venv # 激活它 (macOS/Linux) ❯ source .venv/bin/activate # 安装包 (.venv) ❯ pip3 install pyfiglet # 查看包的安装位置 (.venv) ❯ pip3 show pyfiglet Name: pyfiglet Version: 1.0.3 ... Location: /Users/haoliangma/works/demo/.venv/lib/python3.10/site-packages ... Location 清晰地指向了我们刚刚创建的 .venv 目录内部，与全局或用户目录的路径毫无关系。\n在激活虚拟环境的状态下，我们再次运行之前的查询命令，查看当前的 site-packages 路径：\n1 2 (.venv) ❯ python3 -c \u0026#34;import site; print(site.getsitepackages())\u0026#34; [\u0026#39;/Users/haoliangma/works/demo/.venv/lib/python3.10/site-packages\u0026#39;] 查看与管理已安装的包 pip list pip list 会列出当前 Python 环境中所有已安装的包及其版本号。\n使用 pyenv 管理的 python，在全局环境中运行pip list，会列出全局和用户目录下的包。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ❯ pip3 list Package Version ---------- ------- pip 23.0.1 pyfiglet 1.0.3 setuptools 65.5.0 ❯ pip3 show pyfiglet Name: pyfiglet Version: 1.0.3 Summary: Pure-python FIGlet implementation Home-page: https://github.com/pwaller/pyfiglet Author: Peter Waller (Thanks to Christopher Jones and Stefano Rivera) Author-email: p@pwaller.net License: MIT Location: /Users/haoliangma/.local/lib/python3.10/site-packages Requires: Required-by: 从这里可以，pip list 中列出的pyfiglet，是安装在用户目录的 site-packages 中。\n查看 macOS 系统自带的 Python 安装了哪些包：\n1 2 3 4 5 6 7 8 9 10 11 ❯ pip3 list Package Version ------------ -------- altgraph 0.17.2 future 0.18.2 macholib 1.15.2 pip 21.2.4 setuptools 58.0.4 six 1.15.0 vboxapi 1.0 wheel 0.37.0 这里的列表通常会长很多，因为它包含了很多操作系统或开发工具（如 Xcode）正常运行所依赖的包。\n在一个全新的、刚刚激活的虚拟环境中执行 pip list：\n1 2 3 4 5 ❯ pip3 list Package Version ---------- ------- pip 23.0.1 setuptools 65.5.0 pip freeze pip list 非常适合日常查看，但当我们需要将项目依赖分享给他人或进行部署时，pip freeze 命令是更好的选择。\npip freeze 的输出格式与 pip list 类似，但它遵循一种特定的格式，可以直接用于依赖文件。\n1 ❯ pip freeze \u0026gt; requirements.txt pip install 使用依赖文件重建环境：\n1 2 3 $ python3 -m venv .venv $ source .venv/bin/activate (.venv) $ pip install -r requirements.txt 总结 本文从一个常见的认知误区出发，首先明确了 pip 本身是依赖于 python 解释器运行的一个包。\n随后，我们深入探讨了在复杂的开发环境中，如何使用命令，定位当前正在使用的 python 与 pip 的真实位置，详细解析了 pip install 的三大目的地：全局、用户目录和虚拟环境。\n为了方便回顾和查阅，以下是本文提到的所有核心命令及其功能摘要：\n功能分类 命令 描述 定位与识别 which \u0026lt;command\u0026gt; (macOS/Linux) 在 PATH 环境变量中查找并显示命令的可执行文件路径。 where \u0026lt;command\u0026gt; (Windows) 功能与 which 类似，在 PATH 中查找命令的路径。 python3 -c \u0026quot;import sys; print(sys.executable)\u0026quot; 精准查找并打印当前正在运行的 Python 解释器的绝对路径，不受 shims 等影响。 pip3 --version 显示 pip 的版本号及其所属的 Python 环境和具体的 site-packages 路径。 pip3 show \u0026lt;package\u0026gt; 显示指定包的详细信息，尤其是其 Location 字段，是确认包安装位置的关键命令。 python3 -c \u0026quot;import site; print(site.getsitepackages())\u0026quot; 显示当前 Python 环境的 site-packages 目录列表。 python3 -m site --user-site 显示当前 Python 环境的用户专属 site-packages 目录路径。 包管理 pip3 install \u0026lt;package\u0026gt; 安装一个 Python 包。默认尝试全局安装，无权限时可能自动转为用户目录安装。 pip3 install --user \u0026lt;package\u0026gt; 明确指定将包安装到用户 site-packages 目录。 pip3 install -r requirements.txt 从指定的 requirements.txt 文件中读取并安装所有依赖包，用于环境复现。 pip3 list 列出当前环境中所有已安装的包及其版本，适合日常查看。 pip3 freeze \u0026gt; requirements.txt 将 pip freeze 的输出重定向，生成或覆盖 requirements.txt 依赖清单文件。 ","date":"2025-07-20T08:05:26+08:00","permalink":"https://mahaoliang.tech/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-python-%E4%B8%8E-pip/","title":"深入理解 Python 与 pip"},{"content":"对于许多 Python 开发者而言，venv、pyenv 与 conda 如同三位熟悉的“魔术师”。我们熟练地使用它们的命令来隔离项目、切换版本，却常常对其背后的运作一知半解。\n本文的目标，正是要揭开这些工具的魔法外衣。我们不罗列命令，而是直击核心：深入 PATH 环境变量、Shim 机制和文件系统布局，揭示它们各自的实现原理。理解了底层，你才能真正驾驭它们，告别环境管理的混乱。\n争夺 PATH 环境变量 Python 环境管理工具的核心机制，都围绕着对 PATH 环境变量的控制。\nPATH 是一个由目录路径组成的有序列表。当你执行一个命令时，操作系统会按照这份列表的顺序，从左到右依次在这些目录中查找对应的可执行文件。一旦找到，便立即执行并停止搜索。\nvenv、conda 和 pyenv 都通过修改 PATH 来确保其管理的 Python 解释器被优先调用，但它们实现这一目标的具体策略存在不同。\n直接修改 PATH：venv 与 conda 的策略 venv 和 conda 采用的是一种直接修改 PATH 变量的策略。当一个环境被激活时，该环境的 bin 目录（在 Windows 上是 Scripts 目录）会被插入到 PATH 列表的最前端。\nvenv 的实现 执行 source .venv/bin/activate 命令后，该脚本会获取当前 PATH 变量的值，并将 /path/to/project/.venv/bin 这个路径字符串添加到其最左侧。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 查看当前的 PATH 变量（为清晰起见，已简化） $ echo $PATH /usr/local/bin:/usr/bin:/bin # 查找 python 命令的位置 $ which python3 /usr/bin/python3 # 激活虚拟环境 $ source .venv/bin/activate # 再次查看 PATH 变量 $ echo $PATH /path/to/project/.venv/bin:/usr/local/bin:/usr/bin:/bin # 再次查找 python 命令的位置 $ which python3 /path/to/project/.venv/bin/python3 conda 的实现： conda activate my-env 命令执行的逻辑与 venv 相同。它会将 /path/to/miniconda3/envs/my-env/bin 目录路径插入到 PATH 变量的最前端。\n这种策略的共同点是：激活操作直接将包含目标 Python 解释器的目录置于最高查找优先级。\n通过 Shim 间接控制：pyenv 的策略 pyenv 采用了一种更为间接的控制策略。它不直接将任何特定版本的 Python bin 目录添加到 PATH，而是在 pyenv 初始化时，要求用户将一个名为 shims 的特殊目录添加到 PATH 的最前端。\npyenv 初始化后的 PATH\n1 2 $ echo $PATH /Users/haoliangma/.pyenv/shims:/opt/homebrew/bin:... shims 目录是 pyenv 实现版本动态切换的关键。该目录中包含了一系列与常用命令（如 python, pip）同名的可执行文件，这些文件被称为 Shim。\n当用户执行 python 命令时，实际运行的是 ~/.pyenv/shims/python 这个 Shim 文件。该文件的核心任务是：\n执行 pyenv 的内部逻辑。 pyenv 根据当前配置（如 .python-version 文件、全局设置或环境变量）确定需要使用的真实 Python 版本（例如 3.10.9）。 pyenv 随后将命令的执行权转发给该版本的真实解释器，其路径为 ~/.pyenv/versions/3.10.9/bin/python。 这种策略的特点是：PATH 的最高优先级被一个固定的 Shim 目录占据，由该目录中的程序根据上下文动态地决定并调用真正的目标可执行文件。\n本章小结 venv 和 conda 通过直接修改 PATH 将特定环境的 bin 目录置于首位，是一种静态的状态切换。而 pyenv 则是通过一个固定的 shims 目录来拦截命令，并进行动态的命令转发。\n为什么 pyenv + venv 可以合作 既然 pyenv 和 venv 都会争夺 PATH 环境变量的最高优先级，为什么它们的组合不会产生冲突呢？\n答案在于，它们对 PATH 的控制服务于一个有序的、分阶段的执行流程，并通过 符号链接（Symbolic Link） 这一关键技术，确保了执行权的无缝交接。\n阶段一：环境构建 (pyenv Shim 机制主导) 在创建虚拟环境的阶段，pyenv 的 Shim 机制起着决定性作用，它确保了 venv 模块由正确版本的 Python 解释器执行。\n初始状态 pyenv 已初始化，~/.pyenv/shims 目录位于 PATH 的最前端。用户已通过 pyenv shell 3.10.9 等命令指定了 Python 版本。\n执行创建虚拟环境的命令 1 $ python -m venv .venv 命令的执行解析 Shell 依据 PATH 顺序，首先执行 ~/.pyenv/shims/python 这个代理脚本。\npyenv 的 Shim 逻辑被触发，它检测到版本配置 (.python-version)，确定目标为 3.10.9 版本。\npyenv 随即将执行权转发给真实的解释器：~/.pyenv/versions/3.10.9/bin/python。\n最终，由 3.10.9 版本的解释器来执行 -m venv .venv 任务。\nvenv 的核心产出 在 .venv/bin/ 目录下，venv 模块创建了一个名为 python 的符号链接。此链接的目标地址，被精确地设置为用于创建它的那个解释器的绝对路径。\n1 2 $ ls -l .venv/bin/python3 lrwxr-xr-x 1 haoliangma staff 52B 7 20 13:21 .venv/bin/python3 -\u0026gt; /Users/haoliangma/.pyenv/versions/3.10.9/bin/python3 这个符号链接的存在至关重要，它以文件系统级别的指针形式，永久性地记录了该虚拟环境所绑定的 Python 解释器版本。至此，pyenv 在构建阶段的任务已经完成。\n阶段二：环境激活 (venv 接管 PATH 优先级) 在环境被激活后，venv 虽然在 PATH 层面取得了最高优先级，但符号链接机制确保了最终的执行流依然正确。\n激活虚拟环境 1 $ source .venv/bin/activate PATH 的变更 activate 脚本将 .venv/bin 目录插入到 PATH 的最前端，此时 PATH 变为：\n1 2 $ echo $PATH /path/to/project/.venv/bin: /Users/haoliangma/.pyenv/shims:... 从 PATH 的顺序上看，.venv/bin 的优先级已经高于 pyenv 的 shims 目录。\n最终命令的执行解析 当用户再次输入 python3 命令：\nShell 首先在 PATH 的第一站 /path/to/project/.venv/bin 中找到了 python3 文件。\n操作系统识别出这是一个符号链接。\n操作系统自动解引用（dereference）该链接，即跟随指针找到了它的真实目标：/Users/haoliangma/.pyenv/versions/3.10.9/bin/python3。\n最终，由 pyenv 管理的 3.10.9 解释器被执行。\n本章小结 pyenv 和 venv 的 PATH 争夺之所以没有导致冲突，是因为它们的交互是一个非竞争性的时序过程：\n在环境创建时，pyenv 的 Shim 机制处于活动状态，用于选择正确的 Python 解释器。\nvenv 将这个选择结果通过符号链接的形式固化下来。\n在环境激活后，venv 接管 PATH 的最高优先级。此时 pyenv 的 Shim 机制虽然被绕过，但这无关紧要，因为符号链接已经确保了任何对 python 的调用都会被直接路由到 pyenv 事先选定的那个解释器。\n为什么 pyenv 与 conda 会冲突 两套并行的版本管理体系 pyenv 与 conda 之间的冲突，首要根源在于，两者都试图控制 Python 解释器的版本管理。\npyenv 的功能：其核心功能是安装和管理多个不同版本的 Python 解释器（例如 3.9.13, 3.10.9），并将它们存储在 ~/.pyenv/versions/ 目录下。\nconda 的功能：conda 将 Python 解释器本身也视为一个普通的软件包。执行 conda create -n myenv python=3.9 时，conda 会从其官方渠道下载一个预编译的 Python 3.9，并将其安装在 ~/miniconda3/envs/myenv/ 目录内。\n这就造成了一个直接的矛盾：一个系统内存在两套独立的、用于获取和管理 Python 版本的机制。它们各自维护着不同的 Python 安装路径。\nPATH 控制权的互斥冲突 这是导致两者无法共存的最直接的技术原因。\npyenv 有效的前提是，~/.pyenv/shims 目录必须位于 PATH 环境变量的最前端。只有这样，pyenv 的代理脚本才能生效。\n而 conda 的激活操作，恰恰会破坏了这个前提。\n初始状态 假设 pyenv 和 conda 均已在 Shell 配置文件中初始化。PATH 的起始部分可能如下（取决于初始化顺序）：\n1 2 $ echo $PATH /home/user/.pyenv/shims:/path/to/miniconda3/condabin:... 激活虚拟环境 1 $ conda activate my-env conda 的 PATH 修改 conda activate 命令会强制将 my-env 环境的 bin 目录插入到 PATH 的最前端。\n冲突后的 PATH 状态 PATH 变量变为：\n1 2 $ echo $PATH /path/to/miniconda3/envs/my-env/bin:/home/user/.pyenv/shims:... 现在，conda 环境的 bin 目录取代了 pyenv 的 shims 目录，成为了 PATH 的最高优先级。\n最终的执行解析 当用户再次输入 python 命令时，Shell 首先在 /path/to/miniconda3/envs/my-env/bin 目录中找到了 python 可执行文件。这个文件是 conda 自己安装的，与 pyenv 毫无关系。pyenv 的 shims 目录因为排在后面，其中的代理脚本根本没有机会被执行。\n与 pyenv+venv 的组合不同，这里不存在任何“交接”机制。conda 环境中的 python 不是一个指向 pyenv 所管理版本的符号链接；它是一个由 conda 独立安装的、完全自洽的二进制文件。\n本章小结 在 PATH 控制上，pyenv 要求其 shims 目录占据最高优先级以实现动态代理，而 conda activate 则要求其环境 bin 目录在激活期间占据最高优先级。\n这两个要求是互斥的，因此，两者无法稳定共存。\n隔离空间，本地分散 vs. 全局集中 在解决了“如何激活”的问题后，我们下一个要探究的是：这些隔离的环境和它们所依赖的包，究竟被存放在了哪里？\n尽管 venv 和 conda 都为项目提供了独立的包安装空间，但它们在物理存储上采用了截然不同的策略。\nvenv：环境与项目同在 venv 遵循的是一种本地化、分散式的管理哲学。当你站在一个项目目录下，执行 python -m venv .venv 时，它会在当前目录下创建一个名为 .venv 的文件夹。这个文件夹就是你的整个虚拟环境。\n.venv 是一个自包含的目录，里面有独立的 bin（或 Scripts）和 lib 文件夹。当你在这个环境中 pip install requests 时，requests 库的所有文件都会被原封不动地放进 .venv/lib/pythonX.X/site-packages/ 目录下。\n这种范式的优缺点十分鲜明。\n优点\n概念清晰：环境与项目代码紧密绑定，一目了然。\n管理简单：当项目结束时，只需将整个项目文件夹删除，与之关联的虚拟环境也被一并彻底清理，不留任何痕迹。\n缺点\n空间冗余：这是它最大的弊端。如果你有十个 Web 项目都依赖于 Django 和 requests，那么你的硬盘上就会躺着十份几乎完全相同的库文件拷贝，造成了不小的磁盘空间浪费。 venv 的设计就像是为每个项目都配备了一个独立的“随身工具箱”，方便携带，但如果每个工具箱里的工具都大同小异，那无疑是种累赘。\nconda：统一管理与高效复用 与 venv 不同，conda 采用的是一种高度集中、统一管理的模式。它的存储体系主要由两个核心目录构成，通常位于你的用户主目录下（如 ~/miniconda3）：\nenvs 目录：所有通过 conda create 创建的环境，都以子目录的形式集中存放在这里。\npkgs 目录：这是 Conda 的“中央包仓库缓存”。所有下载过的包（包括不同版本的 Python 解释器本身）都会在这里存放一份。\n当你在 myenv 环境中安装 numpy 时，Conda 并不会将 numpy 的文件从 pkgs 目录完整地复制到 envs/myenv 目录下。相反，它会使用一种名为“硬链接”的文件系统特性。\n这种“中央集权”范式的优缺点也同样突出。\n优点\n空间优化：得益于硬链接，即使一百个环境都使用 numpy，它在物理磁盘上也只占用一份空间，极大地节省了资源。\n全局管理便利：只需一个 conda env list 命令，就能清晰地列出并管理本机上所有的 Conda 环境。\n缺点\n物理分离：环境与项目代码在文件系统上是分离的。这要求开发者需要自行维护“哪个项目对应哪个环境”的映射关系，有时可能会造成混淆。 conda 的设计更像一个大型的“中央仓库”，所有项目都按需从中领取工具的使用权，而不是复制一份。这种模式在处理拥有大量共同依赖的多个项目时，效率和优势尽显。\n总结 走过这趟深入 Python 环境管理底层的旅程，我们拨开了 venv、pyenv 与 conda 这三位“魔术师”的神秘面纱。理解了这些原理，可以让我们更好地管理 Python 环境，并更高效地使用 Python。\n我们花费精力去理解工具的内在，是为了在日常工作中能彻底忘掉它们的存在，将所有心力都投入到代码和创造本身。希望本文能帮助你找到那把最称手的钥匙，去打开更广阔的开发世界。\n","date":"2025-07-19T12:15:07+08:00","permalink":"https://mahaoliang.tech/p/%E6%8F%AD%E7%A7%98-python-%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%E7%9A%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0/","title":"揭秘 Python 环境管理的底层实现"},{"content":"前面我写文章分别介绍了 Python 的两个虚拟环境管理工具 venv 和 Conda ，并进行过适用场景对比。但具体应该如何选择呢，我想分享我个人的选择策略，希望能给你提供一个更具体的参考。\nmacOS 在 MacBook Pro 上，我一般进行纯 Python 开发，倾向于使用 pyenv + venv 的组合。pyenv 负责管理和切换全局的 Python 版本，venv 则为每个项目创建极致轻量的虚拟环境。\n这套组合非常优雅，工具链清晰解耦，完全符合这类项目的需求。\nWindows Windows 上的情况要复杂一些。\n在我的 Windows 笔记本 上，由于配备了 NVIDIA 显卡，可以用于 AI 和机器学习开发，所以会安装 miniconda，但一定要注意：\n不要“Add Miniconda3 to my PATH environment variable”，\n也不要“Register Miniconda3 as my default Python”，\n避免干扰系统。如果想使用 Conda，通过“开始菜单”找到并打开 Anaconda Prompt (Miniconda3) 来使用。\n至于使用哪种虚拟环境管理工具，需要根据场景选择。\n纯 Python 开发 纯 Python 开发，仍然使用 venv 创建虚拟环境。\nAI 开发 AI 开发，需要依赖复杂的 CUDA Toolkit 和 cuDNN。按道理应该使用 Conda，可以一键在虚拟环境中安装 CUDA Toolkit 和 cuDNN 本地依赖。\n1 conda install pytorch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 pytorch-cuda=12.4 -c pytorch -c nvidia 但最新的 pytorch 安装向导 Start Locally，已经取消了 Conda 选项，只能在 Previous PyTorch Versions 中找到 Conda 的安装命令。\nPyTorch 官方在 2024 年 10 月 22 日通过 GitHub Issue 正式宣布了这一重要决定：\n\u0026ldquo;2.5 will be the last release of PyTorch that will be published to the pytorch channel on Anaconda.\u0026rdquo;\n至于原因，官方声明中提到，将维护资源集中在用户最常用的平台上，可以提供更好的支持和更优质的用户体验。\n那么如果想使用最新版的 pytorch，就不能使用 Conda 了。还是老老实实参考文档，在全局环境中安装并设置 CUDA Toolkit 和 cuDNN，然后使用 venv 创建虚拟环境，拷贝执行 Start Locally 提供命令，在虚拟环境中安装 PyTorch。\n多版本需求 如果你的项目依赖特定的 Python 版本，那么建议使用 Conda，为项目创建指定 Python 版本的虚拟环境。\n复杂的本地依赖 Conda 能够轻松处理那些依赖复杂底层库，它不仅仅管理 Python 包，它管理的是一个完整的、包含所有底层二进制依赖的软件栈。对于依赖复杂非 Python 组件的项目，Conda 很适合。\n","date":"2025-07-17T10:37:55+08:00","permalink":"https://mahaoliang.tech/p/python-%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%E9%80%89%E6%8B%A9%E5%BB%BA%E8%AE%AE/","title":"Python 虚拟环境管理工具选择建议"},{"content":"为什么需要环境管理？ “在我电脑上明明能跑啊！”\n这句经典的抱怨背后，是无数开发者都曾遭遇的“依赖地狱”：项目 A 需要 TensorFlow 1.x，新项目 B 却要 TensorFlow 2.x；团队成员间的环境难以统一；系统自带的 Python 环境被各种库弄得一团糟。\n解决这一切的关键在于环境隔离：为每个项目创建一个独立的、干净的“沙盒”。而 Conda，正是实现这一目标的利器。\n本文将是一份全面的 Conda 指南，带你从核心用法、工作原理到最佳实践，让你彻底告别环境管理的混乱，拥抱一个更专业、高效的开发工作流。\nConda 是什么？ 很多初学者会将 Conda 简单地等同于一个虚拟环境工具，就像 Python 自带的 venv。这其实只说对了一半。要真正理解 Conda，你需要认识它的三重身份：\n环境管理器 (Environment Manager) 这是 Conda 最广为人知的功能。它允许你创建相互隔离的独立环境。每个环境都可以拥有自己专属的 Python 版本和一套独立的软件包。\n你可以为项目 A 创建一个搭载 Python 3.7 和旧版库的环境。 同时为项目 B 创建另一个搭载 Python 3.10 和最新库的环境。 这两个环境井水不犯河水，你可以通过一条简单的命令在它们之间自由切换。\n包管理器 (Package Manager) 这是 Conda 与 venv + pip 组合的一个区别：pip 是 Python 官方的包管理器，它主要从 PyPI (Python Package Index) 下载软件包；而 Conda 拥有自己独立的包管理系统和软件源（Channels）。\n它的关键优势在于：\n跨语言支持： Conda 不仅仅能安装 Python 包！它可以安装和管理任何语言的软件包，比如 C/C++ 库、R 语言包、CUDA 工具链、MKL 数学库等。这对于数据科学和机器学习领域至关重要，因为许多高性能计算库（如 NumPy, TensorFlow）的底层都依赖于这些非 Python 组件。Conda 会一并帮你处理好这些复杂的依赖关系。\n二进制分发： pip 有时会下载源码包，需要在你的本地机器上进行编译，这个过程可能因为缺少编译器或依赖库而失败。而 Conda 官方渠道中的包绝大多数都是预先编译好的二进制文件，针对你的操作系统（Windows, macOS, Linux）量身打造。这意味着 conda install 通常比 pip install 更快、更稳定，极大地避免了烦人的编译错误。\n发行版管理器 (Distribution Manager) 这一点常常被忽视。Conda 能够将 Python 解释器本身也视作一个普通的软件包来管理。当你执行 conda create -n myenv python=3.9 时，Conda 不会去寻找你系统上已有的 Python，而是会从自己的软件源中下载一个纯净、独立的 Python 3.9 解释器，并安装到 myenv 这个环境中。\n这意味着，你无需借助 pyenv 这类工具，只用 Conda 就能在同一台机器上轻松拥有和管理 Python 3.7, 3.8, 3.9\u0026hellip; 等任意多个版本，并将它们分配给不同的项目环境。\n总结一下： venv 只负责创建隔离的环境“空壳”，包的安装和管理仍由 pip 负责，且它无法管理 Python 版本。而 Conda 则是一个集环境隔离、包安装、依赖处理、Python 版本管理于一身的“全能瑞士军刀”。\n下载和安装 Anaconda vs. Miniconda 在安装 Conda 时，你通常会遇到两个选项：Anaconda 和 Miniconda。这常常让新手感到困惑。\n可以这样理解：\nAnaconda： 这是一个“精装修豪华套餐”。它不仅包含了核心的 Conda 工具，还预装了一个特定版本的 Python 和超过 150 个常用的科学计算、数据分析包，如 NumPy, Pandas, Scipy, Jupyter Notebook 等。它的安装包体积较大（通常几百 MB 到数 GB），旨在提供“开箱即用”的体验。\nMiniconda： 这是一个“毛坯房”。它只包含了最核心的 Conda 工具和一个基础的 Python 解释器。整个安装包非常小巧（几十 MB）。安装完成后，你的环境是几乎纯净的，里面没有任何多余的包。你需要什么，就通过 conda install 自己动手安装什么。\n我的建议是：\n对于所有开发者，尤其是追求环境纯净和良好习惯的开发者，我们强烈推荐从 Miniconda 开始。\n安装 Miniconda 后，你就拥有了完整的 Conda 功能。需要用到 Anaconda 里的那些包？没问题，只需创建一个新环境，然后用 conda install numpy pandas jupyter 一行命令就能搞定，效果完全一样。\n下载 Miniconda 准备好开始了吗？让我们一起动手安装 Miniconda。\n访问 anaconda 的官方下载页面https://www.anaconda.com/download，点击跳过注册，进入正式下载页面。\n在页面右侧“Miniconda Installers”的下方，根据你的操作系统和芯片架构，选择最新的安装包。\n安装 Miniconda Windows 用户 双击下载的 .exe 文件。 在 \u0026ldquo;Installation Type\u0026rdquo; 步骤，推荐选择 “Just Me”，这可以避免很多权限问题。 关键步骤： 安装程序会提供两个高级选项，请按照推荐设置： 不勾选 \u0026ldquo;Add Miniconda3 to my PATH environment variable\u0026rdquo; (将 Miniconda 添加到系统 PATH)。官方不推荐这样做，因为它可能干扰系统上其他的软件。我们应当使用 Conda 自己的方式来激活环境。 不勾选 \u0026ldquo;Register Miniconda3 as my default Python\u0026rdquo; (将 Miniconda 注册为默认 Python)。如果你希望系统默认的 Python 就是 Conda 的，可以勾选，但对于初学者，不勾选也无妨，保持系统纯净。 完成安装后，通过“开始菜单”找到并打开 Anaconda Prompt (Miniconda3) 来使用 Conda。 macOS / Linux 用户 打开你的终端 (Terminal)。 进入到你下载 .sh 文件的目录（通常是 Downloads 目录）。 运行安装脚本，命令如下（请将文件名替换为你下载的实际文件名）： 1 bash Miniconda3-latest-MacOSX-arm64.sh 安装过程中，按 Enter 查看许可协议，然后输入 yes 同意。 当询问安装位置时，直接按 Enter 接受默认路径即可（通常是 ~/miniconda3）。 关键步骤： 安装的最后，它会询问 Do you wish to update your shell profile to automatically initialize conda?。务必输入 yes。这一步会自动修改你的 shell 配置文件（如 .bashrc 或 .zshrc），让 conda 命令在你的命令行终端中可用。 完成安装后，请务必关闭并重新打开你的终端窗口（或 Anaconda Prompt）。这是为了让刚才的 conda init 配置生效。\n然后，输入以下命令：\n1 conda --version 如果安装成功，它会显示出 Conda 的版本号，例如 conda 25.5.1。你可能还会注意到，命令行提示符的前面多了一个 (base) 的字样。这表示你当前正处于 Conda 的 base 环境中。\n恭喜你，Conda 已经成功安装并准备就绪！接下来，我们将学习如何驾驭它。\nConda 核心命令实战 Conda 的强大之处在于其简洁而强大的命令行接口。掌握下面这些核心命令，你就足以应对 95% 以上的日常开发需求。\n打开你的终端（macOS/Linux）或 Anaconda Prompt (Windows)，让我们开始施展魔法。\n环境管理 (Environment Management) 环境管理是 Conda 的基石。请记住：不要在 base 环境中工作，为每个项目创建新环境。\n创建新环境 假设我们要启动一个名为 data_analysis 的新项目，并且希望使用 Python 3.10.18。\n1 2 # 语法: conda create --name \u0026lt;环境名\u0026gt; python=\u0026lt;python版本\u0026gt; [其他包...] conda create --name data_analysis python=3.10.18 Conda 会解析依赖，下载一个独立的 Python 3.10.18 和一些基础包，然后为你创建一个名为 data_analysis 的环境。你还可以在创建时就指定要安装的包：\n1 2 # 创建环境的同时安装 pandas 和 matplotlib conda create -n data_analysis python=3.10.18 pandas matplotlib 注意，参数 --name 可以缩写为 -n。\n查看可安装的 Python 版本 在创建环境时，你可能会问：“我怎么知道哪些 Python 版本是可用的呢？”Conda 把 Python 也当作一个包来管理，所以我们可以用 search 命令来查找：\n1 conda search python 这个命令会列出 Conda 软件源中所有可供安装的 Python 版本。这样，在执行 conda create 之前，你就可以清楚地知道有哪些版本可以选择。\n激活（进入）环境 环境创建好后，它就像一个独立的房间，使用 activate 命令进入它：\n1 conda activate data_analysis 执行后，你会发现命令行提示符的前缀从 (base) 变成了 (data_analysis)。这明确地告诉你：你现在就在 data_analysis 这个沙盒里了！ 在此之后，你所有关于 python、pip、conda install 的操作，都将只影响这个环境。\n停用（退出）环境 项目工作完成后，或者需要切换到其他项目时，使用 deactivate 命令退出当前环境，返回到 base 环境。\n1 conda deactivate 执行后，提示符前面的 (data_analysis) 就会消失，变回 (base)。\n列出所有环境 想看看自己都创建了哪些独立环境？\n1 2 conda env list # 或者 conda info --envs 这个命令会列出所有已创建的环境，并在当前激活的环境旁边用星号 * 标记。\n删除环境 当一个项目彻底结束，不再需要对应的环境时，可以将其彻底删除以释放磁盘空间。\n1 2 3 # 确保你已退出该环境 (conda deactivate) # 语法：conda remove --name \u0026lt;环境名\u0026gt; --all conda remove -n data_analysis --all --all 参数至关重要，它会删除环境下的所有包以及环境本身。\n包管理 (Package Management) 进入了指定的环境后，接下来就是为项目“添砖加瓦”——安装所需要的各种库。\n安装包 在已激活的环境中，使用 conda install 命令。\n1 2 3 4 5 6 7 8 9 10 11 # 激活环境 conda activate data_analysis # 安装单个包 conda install scikit-learn # 同时安装多个包 conda install beautifulsoup4 requests # 安装指定版本的包 conda install tensorflow=2.10.0 查看已安装的包 1 2 # 必须在激活的环境中执行 conda list 上面的命令会列出当前环境中所有的包及其版本号。\n搜索可用的包 不确定某个包是否存在，或者想看看有哪些可用的版本？\n1 conda search numpy 这个命令会搜索所有名为 numpy 的包及其可用版本。\n更新包 1 2 3 4 5 # 更新单个包到最新兼容版本 conda update pandas # 更新环境中的所有包 conda update --all 删除包 1 2 # 从当前环境中删除一个包 conda remove beautifulsoup4 环境复现 这是 Conda 最强大的功能之一，也是保证团队协作一致性的关键。我们可以将一个环境的所有配置导出到一个文件中，其他人拿到这个文件就能一键复制出完全相同的环境。\n导出环境配置 首先，激活你想要导出的环境，然后执行：\n1 2 conda activate data_analysis conda env export \u0026gt; environment.yml 这会生成一个名为 environment.yml 的文件。打开它看看，里面精确记录了环境名称、所有包的版本号。\n从文件创建环境 当你的同事拿到这个 environment.yml 文件后，只需一行命令，就能在自己的机器上克隆出一个一模一样的环境：\n1 2 # 无需先创建环境，Conda 会根据文件中的名字自动创建 conda env create -f environment.yml 核心命令总结 功能分类 常用命令 说明 环境管理 conda create -n myenv python=3.10.18 创建一个名为 myenv 的新环境，并指定 Python 版本。 conda activate myenv 激活（进入）指定的环境。 conda deactivate 停用（退出）当前环境，返回 base 环境。 conda env list 列出所有已创建的环境。 conda remove -n myenv --all 彻底删除一个环境及其所有内容。 包管理 conda install \u0026lt;package_name\u0026gt; 在当前激活的环境中安装一个或多个包。 conda list 查看当前激活环境中已安装的所有包。 conda update \u0026lt;package_name\u0026gt; 更新指定的包。 conda remove \u0026lt;package_name\u0026gt; 从当前激活的环境中移除一个包。 环境复现 conda env export \u0026gt; environment.yml 将当前激活环境的配置导出到 environment.yml 文件。 conda env create -f environment.yml 根据 environment.yml 文件创建或复现一个完整的环境。 信息查询 conda --version 查看 Conda 的版本号。 conda search python 列出 Conda 源中所有可供安装的 Python 版本。 conda search \u0026lt;package_name\u0026gt; 在软件源中搜索一个包的所有可用版本。 深入原理：Conda 是如何工作的？ 我们已经学会了 Conda 的核心命令，能够熟练地创建、切换和管理环境。但是，你是否好奇过：\n当执行 conda install numpy 时，这个包到底被安装到哪儿去了？ conda activate myenv 这条命令，是如何让 python 指向一个全新的解释器？ Conda 会不会搞乱系统自带的 Python 环境呢？ 让我们一起深入 Conda 的“后台”，一探究竟。\n依赖安装到哪里了？ 当安装 Miniconda 后，它会在你的用户主目录下创建一个文件夹：macOS 上默认是~/miniconda3，Windows 上默认是 C:\\Users\\\u0026lt;用户名\u0026gt;\\miniconda3。这个文件夹就是 Conda 的大本营，其中有两个子目录至关重要：envs 和 pkgs。\nenvs 目录：环境的独立“公寓” envs 目录是所有虚拟环境的家。当你执行 conda create -n data_analysis 时，Conda 会在 envs 目录下创建一个名为 data_analysis 的子文件夹。\n这个 data_analysis 文件夹不是一个空壳，而是一个几近完整的、独立的 Python 环境。它里面包含了：\n一个独立的 Python 解释器 (envs/data_analysis/bin/python) 独立的包安装目录 (envs/data_analysis/lib/pythonX.X/site-packages) 所有安装到这个环境的包的二进制文件和脚本 (envs/data_analysis/bin/) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ❯ tree -L 3 ~/miniconda3/envs /Users/haoliangma/miniconda3/envs └── data_analysis ├── bin │ ├── ... │ ├── python -\u0026gt; python3.10 │ ├── python3 -\u0026gt; python3.10 │ ├── python3.1 -\u0026gt; python3.10 │ ├── python3.10 │ └── ... ├── ... ├── lib │ ├── ... │ ├── python3.10 │ │ ├── site-packages │ │ │── ... │ │ └── ... │ └── ... └── ... 每个环境都是一个独立的目录，这就是 Conda 实现文件级别隔离的基础。删除环境时，只需删掉 envs 下对应的文件夹即可，干净利落。\npkgs 目录：包的中央仓库 这个目录存储所有下载的包文件缓存。Conda 会将下载的包保存在这里，以便在创建或更新环境时可以快速访问，而无需重新下载。\n如何做到隔离的？ 文件隔离解决了存储问题，但运行时隔离又是如何实现的呢？为什么在激活 data_analysis 后，我在终端里输入 python，系统就知道要运行 ~/miniconda3/envs/data_analysis/bin/python，而不是系统自带的 /usr/bin/python？\n答案在于一个至关重要的环境变量：PATH。\nPATH 变量是一系列由冒号（在 Windows 上是分号）隔开的目录路径。当你在终端输入一个命令（如 python、pip）时，操作系统会按照 PATH 变量中列出的顺序，从左到右依次在这些目录里查找是否存在同名的可执行文件。一旦找到，就立即执行，并停止向后搜索。\nconda activate data_analysis 命令的核心魔法就在于：\n它会暂时性地修改当前终端会话的 PATH 变量，将当前激活环境的 bin 目录路径（例如 ~/miniconda3/envs/data_analysis/bin）添加到 PATH 变量的最前面。\n让我们看一个例子：\n激活前，查看 PATH 环境变量： 1 2 $ echo $PATH /Users/haoliangma/miniconda3/bin:... 执行 conda activate data_analysis 之后，查看 PATH 环境变量： 1 2 3 $ conda activate data_analysis $ echo $PATH /Users/haoliangma/miniconda3/envs/data_analysis/bin:... 现在，当你输入 python，操作系统会首先在 ~/miniconda3/envs/data_analysis/bin 目录里查找。找到了，它立刻执行这个文件，搜索结束。\n系统自带的 /usr/bin/python 因为排在后面，根本没有机会被找到。这就实现了运行时的完美隔离。\n而 conda deactivate 命令则执行相反的操作：它会从 PATH 变量中移除之前添加的路径，将其恢复到激活前的状态。\nConda 会影响系统环境吗？ 设计上，Conda 不会“污染”你的系统环境。\nConda 所有的环境和包都严格限制在 Miniconda 的安装目录内。conda activate 对 PATH 的修改也仅限于当前的终端会话，一旦关闭窗口，一切都会复原。它不会去修改或覆盖你系统目录（如 /usr/bin）下的任何文件。\n唯一的“例外”是 conda init 命令。\n在你安装 Miniconda 的最后一步，或者手动执行 conda init zsh 时，它会在你的 Shell 配置文件（~/.zshrc）的末尾添加一小段脚本。\n这一步是必要且安全的。 这段脚本的作用是让 conda activate 等命令能够正确地修改你当前 Shell 的环境变量。如果没有它，conda 命令本身可能可用，但 activate 这种需要与 Shell 交互的功能将无法工作。\n你可以随时打开你的 .bashrc 或 .zshrc 文件查看 Conda 添加的内容，它有清晰的注释 # \u0026gt;\u0026gt;\u0026gt; conda initialize \u0026gt;\u0026gt;\u0026gt;。如果你想彻底移除 Conda，除了删除 Miniconda 的安装目录，只需将这段脚本从配置文件中删除即可。\n横向对比，选择适合的工具 Conda 功能强大，但它并非唯一的环境管理工具。在 Python 的世界里，还有 venv 和 pyenv 等广受欢迎的工具，它们有各自不同的适用场景。\n理解它们之间的区别，能帮助你根据自己的项目需求，选择最合适的工具。\nConda vs. venv 这是最常见的比较。venv 是 Python 3.3+ 版本中内置的虚拟环境创建工具，它通常与 pip（Python 官方包安装器）配合使用，形成了一套轻量级的环境管理方案。\n让我们用一个表格来清晰地对比它们：\n特性 / 对比项 Conda venv + Pip 核心定位 一体化解决方案：环境、包、Python 版本全包 轻量级组合拳：venv 管环境，pip 管包 管理范围 语言无关：能管理 Python, R, C++, CUDA 等任何软件包。 专注 Python：只能管理 Python 包。 Python 版本管理 内置功能：可自行下载和管理任意版本的 Python 解释器。 不具备：使用创建环境时系统当前激活的 Python 版本。 包来源 Conda Channels (如 anaconda, conda-forge)，主要是预编译二进制包。 PyPI (Python Package Index)，包含源码包和 Wheels (二进制包)。 依赖解析能力 非常强大。Conda 在安装前会进行严格的依赖关系检查，能解决复杂的非 Python 依赖（如 MKL, cuDNN）。 相对较弱。Pip 的依赖解析器近年来有改进，但处理复杂或冲突的依赖时仍可能遇到困难。 跨平台一致性 极高。通过 environment.yml 文件，可以保证在 Windows, macOS, Linux 上复现几乎完全一致的环境。 较好，但有风险。requirements.txt 文件在不同操作系统间可能因系统级依赖不同而表现不一。 适用场景 数据科学、机器学习、生物信息学等需要复杂非 Python 依赖的项目；需要管理多 Python 版本的场景。 Web 开发 (Django, Flask)、纯 Python 库开发、简单脚本等依赖相对纯净的场景。 简单总结：\n选择 venv + pip，如果你：\n在做纯粹的 Python 项目，如 Web 开发。 项目的依赖项简单，不涉及复杂的 C/C++ 库。 选择 Conda，如果你：\n数据科学或科学计算领域，需要处理 NumPy, SciPy, TensorFlow 等依赖复杂底层库的包。 需要在一个项目中混合使用 Python 和其他语言的工具（如 R）。 希望一个工具就能搞定环境和 Python 版本的所有问题。 Conda vs. pyenv 另一个让初学者困惑的组合是 Conda 和 pyenv。两者似乎都能管理 Python 版本，它们有什么不同？\npyenv 是一个纯粹的 Python 版本管理器。它的目标只有一个：让你在系统上轻松安装、切换多个 Python 版本（例如，全局用 3.10，某个项目用 3.8）。它本身不管理虚拟环境。pyenv 需要和 venv 配合使用：用 pyenv 切换到项目的目标 Python 版本，然后用该版本下的 venv 模块创建项目专属的虚拟环境。\npyenv 通过一种名为 \u0026ldquo;shims\u0026rdquo; 的机制工作。它会在你的 PATH 路径最前面插入一个 ~/.pyenv/shims 目录。当你执行 python 命令时，实际运行的是 shims 里的一个脚本，这个脚本会根据你当前的配置（全局、项目局部等）决定启动哪个版本的真实 Python 解释器。\n如前所述，Conda 将 Python 解释器本身也视为一个普通的“包”。conda create -n data_analysis python=3.8 会为你下载并安装一个独立的 Python 3.8，它与系统中的其他 Python 绝缘。\n关键区别： pyenv 管理的是“裸露”的 Python 解释器，而 Conda 管理的是包含 Python 解释器的“环境”。\n强烈建议不要同时使用 Conda 和 pyenv 混用 Conda 和 pyenv 就像让两个不同的交通指挥员，在同一个十字路口同时指挥交通，结果必然是混乱和冲突。\n冲突的根源在于 PATH 的控制权之争：\npyenv 通过 shims 机制，劫持了 python 命令的调用。 Conda 通过 conda activate，修改 PATH 变量，也想控制 python 命令的指向。 当你同时安装并初始化了两者，执行 python 命令时，到底听谁的？这取决于你的 Shell 配置中，谁的初始化脚本排在后面，谁就可能覆盖前者的设置。这会导致一些极其诡异且难以排查的问题。\n如果你的工作流以数据科学为中心，或者你喜欢 Conda 的一体化便利性，那就只用 Conda。让它来管理你所有的环境和 Python 版本。\n如果你是一名 Python Web 开发者或库开发者，偏爱 UNIX-like 的小工具组合哲学，那么 pyenv + venv 是一个非常优雅和强大的组合。\n避免将两者混合，可以为你节省大量调试环境问题的时间，让你可以专注于代码本身。\n总结 回顾一下我们的收获，Conda 的核心价值：\n不仅仅是虚拟环境： 我们了解到，Conda 是一个集环境管理、包管理、Python 版本管理于一身的“三合一”强大工具。它通过在 envs 目录中创建隔离的文件系统，并在 pkgs 目录中共享缓存，实现了高效、节省空间的隔离。\n强大的依赖处理： 凭借其跨语言的包管理能力和预编译的二进制包，Conda 能够轻松处理那些依赖复杂底层库（如 C++, FORTRAN, CUDA）的科学计算包，这是它在数据科学领域封神的关键。\n我们对比了 Conda 与 venv/pip 和 pyenv 的区别，结论是：没有最好的工具，只有最适合你当前场景的工具。\n","date":"2025-07-15T17:24:32+08:00","permalink":"https://mahaoliang.tech/p/%E4%B8%80%E6%96%87%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82-python-%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%E7%A5%9E%E5%99%A8-conda/","title":"一文彻底搞懂 Python 环境管理神器 Conda"},{"content":"对于许多在 macOS 上工作的开发者来说，VMware Fusion 就像一把瑞士军刀，让我们能随时拥有一个纯净、独立的 Linux 环境。无论是进行 Linux 系统编程、用 Python 做 Linux 系统工具开发，还是部署和测试应用程序，虚拟机都为我们提供了一个完美的沙盒。\n然而，当你将所有的代码、项目文件和配置文件都存放在 Linux 虚拟机中，万一哪天虚拟机意外崩溃、无法启动，所有的心血都可能付诸东流。\n好消息是，VMware Fusion 提供了一个极其强大的功能，“共享文件夹”（Shared Folders），通过简单的配置，我们可以将 macOS 上的任意一个文件夹，直接“挂载”到 Linux 虚拟机的系统中，让它看起来就像是 Linux 自己的一个目录。\n这意味着，你可以在 macOS 或 Linux 上修改代码，因为它们都是同一份，同时你也可以使用 Time Machine 或其他云盘来实现代码的备份，即方便又安全。\n本文将作为你的向导，手把手带你完成从 VMware Fusion 的配置，到 Linux 系统配置的全部过程。让我们开始吧！\n准备工作，安装 VMware Tools 在开始配置共享之前，我们需要确保环境准备就绪，已经在 VMware Fusion 中安装好一个 Linux 发行版。\n接下来需要在 Linux 虚拟中安装 VMware Tools。\n你可以把 VMware Tools 理解为是连接 macOS 宿主机和 Linux 虚拟机的“桥梁”或“驱动程序”。像虚拟机屏幕分辨率自适应、鼠标无缝切换、剪贴板共享，以及我们本次的目标文件夹共享，都完全依赖于它。\n对于大多数的 Linux 发行版，官方和社区共同维护了一个开源实现，叫做 open-vm-tools，它已经预置在大多数 Linux 发行版的软件源中，安装起来非常方便。\nopen-vm-tools 主要由以下几个软件包组成：\nopen-vm-tools: 这是核心包，提供了最基础的功能，如虚拟机时钟同步、与宿主机的电源操作（正常关机）、心跳检测，以及最重要的，它包含了实现文件夹共享所必需的组件。\nopen-vm-tools-desktop: 它在核心包的基础上，增加了改善图形化交互体验的功能，例如剪贴板复制粘贴、窗口大小自适应等。\nopen-vm-tools-devel 和 open-vm-tools-debuginfo: 这两个包分别用于二次开发和调试，普通用户完全不需要关心。\n了解了这些，我们的目标就非常明确了。在虚拟机的命令行终端执行以下命令：\n1 2 3 4 5 6 # Ubuntu sudo apt update sudo apt install open-vm-tools # RHEL sudo yum install open-vm-tools 安装完成后，重启虚拟机，确保所有服务都能正常加载。下一步，我们就去 VMware Fusion 中开启文件共享功能。\n在 VMware Fusion 中配置共享 先将 Linux 虚拟机关机，然后进入虚拟机的设置面板，点击“共享”图标，勾选 “启用共享文件夹” 这个复选框，接着，点击下方的 + 号按钮，准备添加一个具体的共享目录。\n确保“启用”是勾选状态，并且权限设置为“读与写”，这样你才能在 Linux 中创建和修改文件。\n关闭配置，启动虚拟机，接下来需要在 Linux 中完成共享目录的挂载，就可以访问共享文件夹了。\n在 Linux 虚拟机中访问和挂载 在最新版的 open-vm-tools 的支持下，VMware 的共享文件夹通常会被自动挂载到一个系统级的公共目录：/mnt/hgfs (Host-Guest File System)。\n我们可以先验证一下。打开终端，运行：\n1 ls /mnt/hgfs 如果能看到你在上一步设置的共享名（如 linux），那么恭喜你，已经成功了！\n手工挂载 有时 /mnt/hgfs 目录是空的，这可能是因为权限或 FUSE 服务问题。解决方法是使用 vmhgfs-fuse 命令手动挂载。\n1 2 3 4 5 6 # .host:/ 是一个特殊地址，代表所有已启用的共享 # /mnt/hgfs 是挂载点 # -o allow_other 允许其他用户(包括你自己)访问，非常重要！ # -o uid=$(id -u) 将文件所有者设置为当前登录的用户 # -o gid=$(id -g) 将文件所属组设置为当前登录的用户 sudo vmhgfs-fuse .host:/ /mnt/hgfs -o allow_other -o uid=$(id -u) -o gid=$(id -g) 现在 /mnt/hgfs 目录下就可以看到共享的文件了。\n不过，/mnt/hgfs/linux 这个路径太深，不方便日常使用，我们想将共享目录挂载在用户主目录下的 ~/works 目录中。\n首先在用户主目录下创建 works 目录作为挂载的目标。\n1 mkdir ~/works 然后同样使用 vmhgfs-fuse 工具来执行挂载。\n1 sudo vmhgfs-fuse .host:/ ~/works -o allow_other -o uid=$(id -u) -o gid=$(id -g) 现在，来验证一下命令效果：\n1 ls ~/works 你应该能看到 linux 目录。再进一步查看：\n1 ls ~/works/linux 此刻，你看到的就是 macOS 宿主机上那个共享文件夹里的所有内容了！\n然而手动挂载是临时的，一旦你重启虚拟机，挂载就会失效。要实现一劳永逸，请看下一步。\n实现开机自动挂载 为了避免每次重启都要手动敲一遍命令，我们需要将挂载信息写入 /etc/fstab 文件中。\n在 fstab 中，我们不能使用 $(id -u) 这样的命令。需要把用户 ID 和组 ID 的具体数字写进去。运行以下命令查看：\n1 2 id # 你会看到类似 uid=1000(ubuntu) gid=1000(ubuntu) ... 的输出 通常，第一个创建的用户的 UID 和 GID 都是 1000。请记下你自己的这两个数字。\n使用你熟悉的编辑器打开 /etc/fstab。\n1 sudo vi /etc/fstab 在文件的末尾，添加下面这一行。请注意，你需要将 /home/ubuntu/works 替换成你的实际路径，并将 uid=1000,gid=1000 替换成你自己的 ID。\n1 .host:/ /home/ubuntu/works fuse.vmhgfs-fuse defaults,allow_other,uid=1000,gid=1000 0 0 以后每次启动 Linux 虚拟机，VMware 的共享文件夹都会自动出现在 ~/works 目录下，方便随时访问。\n大功告成！\n","date":"2025-07-09T11:41:57+08:00","permalink":"https://mahaoliang.tech/p/%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E5%9C%A8-vmware-fusion-%E4%B8%AD%E5%AE%9E%E7%8E%B0%E4%B8%8E-linux-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E6%96%87%E4%BB%B6%E5%85%B1%E4%BA%AB/","title":"一步步教你在 VMware Fusion 中实现与 Linux 虚拟机的文件共享"},{"content":"今年我非常幸运地入选了“开源之夏”活动，参与的项目是面向 openEuler distroless 镜像的 SDF 自动生成工具开发。由于该项目是在 openEuler 上进行开发的，因此，搭建一个稳定可靠的 openEuler 开发环境便成了我的首要任务。\n我的主力设备是一台 M1 芯片的 MacBook Pro，由于去年 VMware Fusion 已经对个人用户免费，所以自然就选择在 VMware Fusion 上安装 openEuler。\n从 openEuler 官方镜像站下载 aarch64 架构最新的 openEuler 24.03 LTS SP1 镜像，使用 VMware Fusion 创建虚拟机，挂载镜像，启动后在 GRUB 界面选择“Install openEuler”，没想到就遇到了问题，安装过程卡住不动了：\n习惯性的截图问 Google AI studio，它列举了多个可能的原因，逐一排除确认，发现其中一个分析很有启发性：\n这个问题可能是由于内核在初始化虚拟显卡时卡住了\n于是尝试在 openEuler 的安装引导菜单，按 e 键进入编辑模式：\n继续截图让 Google AI studio 分析：\n您看到的这个界面是 GRUB 引导加载器的编辑模式。它允许您在启动 Linux 内核之前，临时修改传递给内核的参数。\nvideo=efifb:off video=VGA-1:640x480...: 这是一个重要线索！这行参数试图禁用 EFI 帧缓冲（efifb:off）并强制使用一个非常基础的 640x480 分辨率的 VGA 模式。这说明 openEuler 镜像为了在各种（尤其是老旧或奇怪的）硬件上都能启动而加入的“安全模式”参数。然而，这个配置与您的 VMware Fusion 环境冲突，导致了卡死。 请按照以下步骤操作：\n在 linux 那一行，手动删除掉现有的两个 video=... 参数，即删除 video=efifb:off 和 video=VGA-1:640x480-32@60me。 按 Ctrl+X 或 F10 启动。 按照 AI 的指示操作，奇迹发生了，系统顺利越过了卡死点，成功进入了图形化的安装界面！\nGemini 2.5 Pro 太厉害了，遇事不会问 Gemini，准没错！\n","date":"2025-07-07T20:26:30+08:00","permalink":"https://mahaoliang.tech/p/macos-%E4%B8%8A-vmware-fusion-%E6%97%A0%E6%B3%95%E5%AE%89%E8%A3%85-openeuler-%E7%9A%84%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/","title":"macOS 上 VMWare Fusion 无法安装 OpenEuler 的问题解决"},{"content":"Python 虚拟环境概述 为什么需要虚拟环境 在 Python 开发中，不同的项目往往需要不同版本的 Python 解释器和第三方库。如果所有项目都共享同一套全局 Python 环境，很容易导致版本冲突和依赖混乱。虚拟环境通过创建隔离的 Python 运行环境，解决了这一问题，确保每个项目都能独立管理自己的依赖关系。\n虚拟环境的核心优势包括：\n环境隔离：不同项目的依赖相互隔离，避免版本冲突 系统保护：保持系统全局 Python 环境干净整洁，防止不必要的包污染 项目可移植性：通过requirements.txt文件记录依赖，方便在不同环境中重建相同的运行环境 版本控制：可以为每个项目指定特定的 Python 版本和库版本 venv 目录结构 venv 是 Python 3.3 及以上版本内置的虚拟环境创建工具，它的工作原理是通过创建一个独立的目录结构，其中包含 Python 解释器的符号链接和独立的site-packages目录。当你使用python -m venv myenv命令创建虚拟环境时，会生成以下关键文件和目录：\n1 2 3 4 5 6 7 8 myenv/ ├── bin/ │ ├── python -\u0026gt; # 指向系统Python解释器的符号链接 │ └── pip ├── lib/ │ └── python3.13/ │ └── site-packages/ # 第三方包安装目录 └── pyvenv.cfg # 环境配置文件 venv 基础操作指南 创建虚拟环境 要创建一个新的虚拟环境，使用以下命令：\n1 2 3 4 5 # macOS/Linux python -m venv myenv # Windows python -m venv myenv 其中，myenv是虚拟环境的名称，你可以根据需要修改。执行上述命令后，会在当前目录下创建一个名为myenv的文件夹，其中包含虚拟环境的所有文件和目录。\n激活虚拟环境 创建虚拟环境后，需要激活它才能使用：\n1 2 3 4 5 6 7 8 # macOS/Linux source myenv/bin/activate # Windows (命令提示符) myenv\\Scripts\\activate.bat # Windows (PowerShell) myenv\\Scripts\\Activate.ps1 激活成功后，你会注意到命令提示符前出现了虚拟环境的名称，例如：\n1 (myenv) $ 这表示你现在正在myenv虚拟环境中工作。\n激活虚拟环境的本质，实际上是执行了一个脚本，该脚本会：\n设置VIRTUAL_ENV环境变量指向虚拟环境的根目录 修改系统 PATH 环境变量，将虚拟环境的bin或Scripts目录添加到最前面 确保后续执行的python和pip命令都指向虚拟环境中的版本 停用虚拟环境 当你完成工作后，可以通过以下命令退出虚拟环境：\n1 $ deactivate 退出后，你将返回到系统的全局 Python 环境，虚拟环境的相关设置将不再生效。\n验证虚拟环境 在激活虚拟环境后，你可以通过以下方式验证环境是否正确设置：\n检查 Python 版本： 1 $ python --version 这将显示虚拟环境中使用的 Python 版本。\n检查 Python 解释器路径： 1 2 3 4 5 # macOS/Linux which python # Windows where python 输出应该是虚拟环境中 Python 解释器的路径，例如/path/to/your/project/myenv/bin/python（macOS/Linux）或C:\\path\\to\\your\\project\\myenv\\Scripts\\python.exe（Windows）。\n检查 pip 版本： 1 pip --version 这将显示虚拟环境中使用的 pip 版本。\n检查环境变量： 1 2 3 4 5 # macOS/Linux echo $VIRTUAL_ENV # Windows echo %VIRTUAL_ENV% 输出应该是虚拟环境的根目录路径。\n依赖管理与包操作 安装包 在激活的虚拟环境中，你可以使用pip命令安装项目所需的 Python 包。安装包的基本语法如下：\n1 pip install package_name 例如，要安装requests库，可以执行以下命令：\n1 pip install requests 如果需要安装特定版本的包，可以使用以下语法：\n1 pip install package_name==version_number 例如，要安装requests库的 2.26.0 版本：\n1 pip install requests==2.26.0 当你在虚拟环境中安装包时，这些包会被安装到虚拟环境的site-packages目录中。具体路径如下：\n1 2 3 4 5 # macOS/Linux myenv/lib/python3.x/site-packages/ # Windows myenv/Lib/site-packages/ 这个目录是虚拟环境隔离的关键，确保包不会被安装到系统全局环境中。\n更新包 当需要更新虚拟环境中的包时，可以使用以下命令：\n1 pip install --upgrade package_name 例如，要更新requests库到最新版本：\n1 pip install --upgrade requests 卸载包 当某个包不再需要时，可以使用以下命令卸载：\n1 pip uninstall package_name 例如，要卸载requests库：\n1 pip uninstall requests 卸载时，pip 会提示你确认是否卸载该包，输入y确认即可。\n管理依赖列表 对于较大的项目，管理所有依赖和它们的版本可能会变得复杂。pip允许你使用requirements.txt文件来跟踪这些依赖。\n生成依赖列表： 1 pip freeze \u0026gt; requirements.txt 这将在当前目录下创建一个requirements.txt文件，其中包含所有已安装包及其版本号\n安装依赖列表： 1 pip install -r requirements.txt 这将安装requirements.txt文件中列出的所有包及其指定版本。\n更新依赖列表： 当你安装或更新包后，需要重新生成requirements.txt文件：\n1 pip freeze \u0026gt; requirements.txt requirements.txt文件的格式通常如下：\n1 2 3 4 5 certifi==2025.7.14 charset-normalizer==3.4.2 idna==3.10 requests==2.32.4 urllib3==2.5.0 VS Code 集成 VS Code 对 Python 虚拟环境提供了良好的支持，以下是在 VS Code 中使用 venv 的步骤：\n安装 Python 扩展： 打开 VS Code，按下Ctrl+Shift+X（Windows/Linux）或Cmd+Shift+X（macOS）打开扩展市场，搜索并安装 \u0026ldquo;Python\u0026rdquo; 扩展。\n打开项目目录： 使用File \u0026gt; Open Folder打开包含虚拟环境的项目目录。\n创建新终端： 当你创建新终端时，VS Code 会自动激活当前选择的虚拟环境，命令提示符前会显示环境名称。\n总结 在本文中，我们详细介绍了 Python 虚拟环境（venv）的使用方法和工作原理。\n虚拟环境基本使用 操作类型 macOS/Linux 命令 Windows（命令提示符）命令 Windows（PowerShell）命令 创建虚拟环境 python -m venv myenv python -m venv myenv python -m venv myenv 激活虚拟环境 source myenv/bin/activate myenv\\Scripts\\activate.bat myenv\\Scripts\\Activate.ps1 停用虚拟环境 deactivate deactivate deactivate 验证 Python 版本 python --version python --version python --version 验证 Python 解释器路径 which python where python where python 验证 pip 版本 pip --version pip --version pip --version 验证环境变量 echo $VIRTUAL_ENV echo %VIRTUAL_ENV% echo $env:VIRTUAL_ENV 依赖管理基本使用 操作类型 操作说明 执行命令 安装包 安装指定第三方包（默认最新版本） pip install package_name 安装特定版本包 安装指定版本的第三方包，避免版本冲突 pip install package_name==version_number 更新包 将已安装的包更新到最新版本 pip install --upgrade package_name 卸载包 移除已安装的第三方包 pip uninstall package_name 生成依赖列表 将当前环境中所有已安装包及版本信息导出到 requirements.txt 文件 pip freeze \u0026gt; requirements.txt 安装依赖列表 根据 requirements.txt 文件安装所有指定包及对应版本 pip install -r requirements.txt 查看已安装包 列出当前环境中所有已安装的第三方包及版本 pip list 查看包详情 显示指定包的详细信息（如版本、依赖、安装路径等） pip show package_name 检查可更新的包 列出当前环境中可更新的包及最新版本 pip list --outdated 最佳实践建议 项目结构 在项目根目录下创建名为.venv的虚拟环境 这样可以保持项目结构的清晰，并方便激活虚拟环境 依赖管理 使用requirements.txt文件记录项目依赖 在提交代码时，包含requirements.txt文件，而不是整个虚拟环境目录 使用pip freeze \u0026gt; requirements.txt生成依赖列表 环境激活 在开发过程中，始终激活虚拟环境后再执行 Python 命令或安装包 版本控制 将虚拟环境目录添加到.gitignore文件中 提交requirements.txt文件，确保其他开发者可以复现相同的环境 虚拟环境是 Python 开发中不可或缺的工具，它帮助开发者保持环境的整洁和项目的可维护性。随着你的项目规模和复杂性的增加，熟练掌握虚拟环境的使用将成为一项重要的技能。\n","date":"2025-07-01T16:21:43+08:00","permalink":"https://mahaoliang.tech/p/python-%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86venv-%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97/","title":"Python 虚拟环境管理：venv 实用指南"},{"content":"作为 Python 初学者，使用 pyenv 来管理多版本 Python 环境是一个明智的选择。pyenv 允许你轻松安装、切换和管理多个 Python 版本，同时保护系统自带的 Python 不被修改。本指南将详细介绍在 macOS 上安装和使用 pyenv 的全过程，包括安装步骤、管理不同 Python 版本、保护系统 Python 等操作方法。\n准备工作 检查 Homebrew 安装 在开始安装 pyenv 之前，首先需要确保你已经安装了 Homebrew。Homebrew 是 macOS 上的包管理器，方便我们安装各种工具和依赖。可以在终端中运行以下命令检查：\n1 2 $ brew --version Homebrew 4.5.8 安装 Xcode 命令行工具 macOS 上编译安装 Python 需要一些开发工具，这些工具可以通过 Xcode 命令行工具提供。运行以下命令安装：\n1 $ xcode-select --install 这会弹出一个安装窗口，按照提示完成安装即可。\n安装 pyenv 使用 Homebrew 安装 pyenv 在 macOS 上安装 pyenv 最简单的方法就是使用 Homebrew。在终端中运行以下命令：\n1 $ brew install pyenv 这一步会下载并安装 pyenv 及其依赖。安装完成后，你可以通过以下命令验证安装是否成功：\n1 2 $ pyenv --version pyenv 2.6.3 如果看到版本号，说明安装成功。\n配置环境变量 安装完成后，需要将 pyenv 添加到你的 Shell 配置文件中，以便在任何终端会话中都能使用。根据你使用的 Shell 类型（zsh 或 bash），打开相应的配置文件。以 zsh 为例，打开 ${HOME}/.zshrc 文件，在文件末尾添加以下内容：\n1 2 3 export PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34; [[ -d $PYENV_ROOT/bin ]] \u0026amp;\u0026amp; export PATH=\u0026#34;$PYENV_ROOT/bin:$PATH\u0026#34; eval \u0026#34;$(pyenv init - zsh)\u0026#34; 这些配置将：\n设置 pyenv 的根目录 将 pyenv 的二进制目录添加到系统 PATH 中 初始化 pyenv 环境 保存文件后，在终端中运行以下命令使配置生效：\n1 source ${HOME}/.zshrc 管理 Python 版本 查看可用 Python 版本 安装 pyenv 后，你可以查看所有可用的 Python 版本：\n1 2 3 4 5 6 7 8 $ pyenv install --list Available versions: 2.1.3 2.2.3 2.3.7 2.4.0 2.4.1 ... 这会列出所有可通过 pyenv 安装的 Python 版本，包括最新版本和旧版本。\n安装特定版本的 Python 要安装特定版本的 Python，使用以下命令：\n1 $ pyenv install \u0026lt;version\u0026gt; 例如，要安装 Python 3.13.5，可以运行：\n1 $ pyenv install 3.13.5 安装过程可能需要一些时间，因为 pyenv 会从源代码编译 Python。你会看到类似以下的提示：\n1 2 3 4 5 6 7 8 python-build: use openssl@3 from homebrew python-build: use readline from homebrew Downloading Python-3.13.5.tar.xz... -\u0026gt; https://www.python.org/ftp/python/3.13.5/Python-3.13.5.tar.xz Installing Python-3.13.5... python-build: use readline from homebrew python-build: use zlib from xcode sdk Installed Python-3.13.5 to /Users/haoliangma/.pyenv/versions/3.13.5 查看已安装的 Python 版本 安装完成后，可以使用 pyenv versions 查看已安装的 Python 版本：\n1 2 3 4 5 $ pyenv versions system 2.7.18 * 3.11.8 (set by /Users/haoliangma/.pyenv/version) 3.13.5 第一行 system 表示系统自带的 Python 版本，通常由操作系统预装。当前未被激活（没有 * 标记）\n第二行 2.7.18 表示通过 pyenv 安装的 Python 2.7.18 版本。当前未被激活（没有 * 标记）。\n第三行 * 3.11.8 (set by /Users/haoliangma/.pyenv/version) 表示当前激活的 Python 版本，该版本是通过全局配置文件 /Users/haoliangma/.pyenv/version 设置的默认版本。\n第四行 3.13.5 表示通过 pyenv 安装的 Python 3.13.5 版本，同样没有被激活。\n设置全局 Python 版本 要设置系统默认的 Python 版本，可以使用以下命令：\n1 $ pyenv global \u0026lt;version\u0026gt; 例如，要将 Python 3.13.5 设置为全局默认版本：\n1 $ pyenv global 3.13.5 再次使用 pyenv versions 查看已安装的版本：\n1 2 3 4 5 $ pyenv versions system 2.7.18 3.11.8 * 3.13.5 (set by /Users/haoliangma/.pyenv/version) 发现 3.13.5 版本已经被激活。\n可以通过以下命令验证：\n1 2 $ python --version Python 3.13.5 设置局部 Python 版本 在项目目录中，你可以设置特定于该项目的 Python 版本。进入项目目录，运行：\n1 $ pyenv local \u0026lt;version\u0026gt; 这会在当前目录下创建一个名为.python-version的文件，记录当前项目使用的 Python 版本。当你进入该目录时，pyenv 会自动切换到指定的 Python 版本。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ cd pythonprojects $ pyenv local 3.11.8 $ pyenv versions system 2.7.18 * 3.11.8 (set by /Users/haoliangma/works/pythonprojects/.python-version) 3.13.5 $ cd .. $ pyenv versions system 2.7.18 3.11.8 * 3.13.5 (set by /Users/haoliangma/.pyenv/version) 临时使用特定版本 如果你只需要在当前终端会话中临时使用某个 Python 版本，可以使用：\n1 $ pyenv shell \u0026lt;version\u0026gt; 例如：\n1 2 3 4 5 6 7 8 9 10 11 $ pyenv versions system 2.7.18 3.11.8 * 3.13.5 (set by /Users/haoliangma/.pyenv/version) $ pyenv shell 3.11.8 $ pyenv versions system 2.7.18 * 3.11.8 (set by PYENV_VERSION environment variable) 3.13.5 这会覆盖全局和局部设置，仅在当前终端会话中生效。要恢复到之前的设置，可以使用：\n1 $ pyenv shell --unset 卸载 Python 版本 当你不再需要某个 Python 版本时，可以使用以下命令卸载：\n1 $ pyenv uninstall \u0026lt;version\u0026gt; 保护系统 Python macOS 系统自带了一个 Python 解释器，通常位于/usr/bin/python3。这个 Python 版本是系统正常运行所必需的，修改或删除它可能导致系统不稳定或某些功能无法正常工作。因此，保护系统 Python 非常重要。\npyenv 不会自动管理或修改系统 Python。当你安装新的 Python 版本时，它们会被安装在~/.pyenv/versions目录下，而不是系统路径中。这意味着系统 Python 始终保持不变，不会受到 pyenv 安装的版本的影响。\n你可以通过以下命令验证系统 Python 是否未被修改：\n1 2 $ which python /Users/haoliangma/.pyenv/shims/python 可以看出，你当前使用的是 pyenv 管理的 Python 版本。\n为了确保系统 Python 不被覆盖，你应该避免使用sudo安装或升级 Python。此外，在设置全局 Python 版本时，应确保不将系统 Python 设置为全局版本。\n如果你不小心覆盖了系统 Python 的某些行为，你可以通过重新安装 Xcode 命令行工具来恢复。运行：\n1 $ xcode-select --install 这会重新安装系统工具，包括系统 Python。\n总结 通过本指南，你应该已经掌握了在 macOS 上使用 pyenv 管理 Python 版本的基本技能。以下是关键点回顾：\n安装 pyenv：使用 Homebrew 安装 pyenv 和相关插件，确保正确配置环境变量。\n管理 Python 版本\n命令 描述 pyenv install -list 查看可安装版本 pyenv install 安装指定版本 pyenv versions 查看已安装版本 pyenv global 设置全局默认版本 pyenv local 为当前目录设置版本 pyenv shell 为当前 Shell 设置版本 pyenv uninstall 卸载版本 保护系统 Python：pyenv 不会自动管理系统 Python，确保系统 Python 不被覆盖是使用 pyenv 的重要原则。 通过使用 pyenv，你可以在保持系统 Python 完整的同时，灵活地管理多个 Python 版本，使你的 Python 开发更加高效和安全。\n","date":"2025-06-14T13:18:30+08:00","permalink":"https://mahaoliang.tech/p/%E5%9C%A8-macos-%E4%B8%8A%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8-pyenv/","title":"在 macOS 上安装和使用 pyenv"},{"content":"\n","date":"2025-05-05T10:38:49+08:00","permalink":"https://mahaoliang.tech/p/ai-%E5%B7%A5%E5%85%B7%E6%80%BB%E7%BB%932025-%E5%B9%B4-5-%E6%9C%88%E7%89%88/","title":"AI 工具总结（2025 年 5 月版）"},{"content":"Zsh 是一种专门为交互式使用而设计的 Shell，同时也是一种强大的脚本语言，集成了 bash、ksh 和 tcsh 的许多有用特性，并添加了许多独特的功能。\n本文将指导您在 macOS 和 Linux 系统上安装 Zsh、Oh My Zsh 以及其常用插件，并展示如何配置 Oh My Zsh，以打造一个高效的命令行工作环境。\n安装 Zsh macOS brew install zsh Ubuntu sudo apt install zsh RHEL sudo yum update \u0026amp;\u0026amp; sudo yum -y install zsh 验证安装的 Zsh 版本 1 2 $ zsh --version zsh 5.8.1 (x86_64-ubuntu-linux-gnu) 设置 Zsh 为缺省 shell 1 $ chsh -s $(which zsh) 退出并重新登录。\n安装 Oh My Zsh Oh My Zsh 是一个开源、社区驱动的 Zsh 配置管理框架，，提供了 300 多个可选插件和 140 多个主题，并且内置了自动更新工具。\n使用下面的命令安装：\n1 $ sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; 安装 zsh-autosuggestions zsh-autosuggestions 为 zsh shell 提供了类似 Fish shell 的自动建议功能的插件，该插件可以根据历史记录和自动补全来为用户提供命令建议。\n将插件 clone 到 $ZSH_CUSTOM/plugins：\n1 $ git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions 然后在 ${HOME}/.zshrc 启用插件：\n1 plugins=(git zsh-autosuggestions) 在命令行输入命令时，zsh-autosuggestions 会根据命令历史或命令补全进行建议提示。那么如何接受建议呢？\nBash 和 Zsh 这样的 Unix shell 提供了两种主要的编辑模式：Emacs 模式和 Vi 模式，也就是说可以使用 Emacs 或 Vi 的快捷键来编辑命令行。Emacs 模式是缺省模式。\n在 zsh-autosuggestions 的缺省配置文件中，定义接受建议的快捷键：\n1 2 3 4 5 6 7 8 9 10 11 12 13 ... # Widgets that accept the entire suggestion (( ! ${+ZSH_AUTOSUGGEST_ACCEPT_WIDGETS} )) \u0026amp;\u0026amp; { typeset -ga ZSH_AUTOSUGGEST_ACCEPT_WIDGETS ZSH_AUTOSUGGEST_ACCEPT_WIDGETS=( forward-char end-of-line vi-forward-char vi-end-of-line vi-add-eol ) } ... 如果命令行处于 Emacs 模式，那么：\nctrl-f 或 ctrl-e 跳到行尾接受当前的建议 option-f 向前前进一个单词并接受建议 同样，如果命令行处于 vi 模式，那么就使用对应的 vi 键盘绑定接受建议。\n配置 Oh My Zsh Oh My Zsh 有非常多的内置插件，你也可以安装第三方插件，就像上面安装的 zsh-autosuggestions。\nOh My Zsh 也内置了多个 主题 供你选择。\n我们可以编辑 ${HOME}/.zshrc，配置 Oh My Zsh 的插件、主题，以及其他一些定制化设置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ... # 设置主题 ZSH_THEME=\u0026#34;bira\u0026#34; # 启用插件 plugins=(git z zsh-autosuggestions) # 命令别名 alias mkdir=\u0026#39;mkdir -v\u0026#39; alias mv=\u0026#39;mv -v\u0026#39; alias cp=\u0026#39;cp -v\u0026#39; alias rm=\u0026#39;rm -v\u0026#39; alias ln=\u0026#39;ln -v\u0026#39; # 配置zsh-autosuggestions export ZSH_AUTOSUGGEST_HIGHLIGHT_STYLE=\u0026#34;fg=#ff00ff,bg=cyan,bold,underline\u0026#34; export ZSH_AUTOSUGGEST_STRATEGY=(history completion) ","date":"2025-01-05T21:06:07+08:00","permalink":"https://mahaoliang.tech/p/zsh-%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/","title":"Zsh 的安装和配置"},{"content":"\n计算机系统与组成原理 极客时间：深入浅出计算机组成原理 Computer Systems: A Programmer\u0026rsquo;s Perspective 从程序员的角度学习计算机系统，了解计算机系统的各个方面，包括硬件、操作系统、编译器和网络。这本书涵盖了数据表示、C 语言程序的机器级表示、处理器架构、程序优化、内存层次结构、链接、异常控制流（异常、中断、进程和 Unix 信号）、虚拟内存和内存管理、系统级 I/O、基本的网络编程和并发编程等概念。这些概念由一系列有趣且实践性强的实验室作业支持。 Computer Systems: A programmer\u0026rsquo;s Perspective 视频课 Computer Science from the Bottom Up 采用“从下到上”的方法，从最基础的二进制、数据表示开始，逐步深入计算机内部工作原理，目的是帮助读者真正掌握计算机科学的基础知识。 Putting the “You” in CPU 深入探讨了计算机系统的工作原理，包括 CPU 的基本操作、系统调用、多任务处理、内存管理以及程序的执行过程。 编码 从二进制编码、数据表示到计算机体系结构、操作系统等多个重要主题，从根本上理解计算机的工作原理。 漫画计算机原理 趣话计算机底层技术 计算机底层的秘密 穿越计算机的迷雾 嵌入式 C 语言自我修养 C 语言 征服 C 指针 彻底理解和掌握指针的各种用法和技巧 C 专家编程 Sun 公司编译器和 OS 核心开发团队成员，对 C 的历史、语言特性、声明、数组、指针、链接、运行时、内存等问题进行了细致的讲解和深入的分析 C from Scratch 一个学习 C 语言的从零开始的路线图，包括推荐的课程、项目和资源，以及进阶到 x86-64 汇编语言和操作系统内部的指导。 极客时间：深入 C 语言和程序运行原理 cdecl 将 C 语言声明转换为英文描述，例如将这样复杂的声明 void (\\*signal(int, void (\\*)(int)))(int) 转换为文字描述：“declare signal as function (int, pointer to function (int) returning void) returning pointer to function (int) returning void” 程序运行原理 Online Compiler, Visual Debugger 独特的逐步可视化调试工具，强烈推荐！ 程序是怎样跑起来的 程序员的自我修养：链接、装载与库 如何从对象文件中导入和执行代码 part1 part2 part3 x86/x64 CPU architecture: the stack \u0026amp; stack frames x86/x64 CPU 架构中的栈（Stack）及其工作机制，包括栈的数据结构特性、CPU 中栈的管理、栈与堆的区别、栈帧的创建与销毁，以及栈的性能优势。 Driving Compilers 关于如何使用编译器创建可执行文件的深入知识，涵盖编译器驱动程序、预处理器 cpp、编译器 cc、链接器 ld 以及 Linux 加载器的概念。 Linux 使用 极客时间：Linux 实战技能 100 讲 Efficient Linux at the Command Line 像黑客一样使用命令行 Linux Foundation 的认证考试 LFCA 和 LFCS Learning Modern Linux Linux From Scratch step-by-step instructions for building your own customized Linux system entirely from source. Linux 内核 Linux 是怎么工作的 Linux 技术内幕 Linux 内核设计与实现 Linux Kernel Development 深入理解 Linux 进程与内存 极客时间：Linux 内核技术实战课 极客时间：编程高手必学的内存知识 极客时间：容器实战高手课 深入理解 Linux 网络 交互式的 Linux 内核地图 Linux 系统编程 Linux/UNIX系统编程手册 The Linux Programming Interface: A Linux and UNIX System Programming Handbook UNIX 环境高级编程 Advanced Programming in the UNIX Environment CS 341: System Programming 伊利诺伊大学香槟分校 CS 341 课程使用，介绍 C 语言和 Linux 系统编程知识。 网络 趣谈网络协议 极客时间：Web 协议详解与抓包实战 图解 TCP/IP 图解 HTTP 网络是怎样连接的 数据结构和算法 极客时间：数据结构与算法之美 极客时间：算法面试通关 40 讲 极客时间：常用算法 25 讲 极客时间：算法训练营 Hello 算法 动画图解、一键运行的数据结构与算法教程 通过动画可视化数据结构和算法 算法刷题 Leetcode 一个广受欢迎的在线编程题库 Neetcode 另一个在线编程练习平台 代码随想录 LeetCode 刷题攻略 算法通关手册 850+ 道「LeetCode 题目」详细解析 综合 计算机自学指南 (GitHub 仓库) YouTube 视频课：Crash Course Computer Science Preview 计算机教育中缺失的一课 Developer Roadmaps 为开发者提供学习路线图和指南 Online Coding Classes – For Beginners 3000 小时的免费课程，涵盖了编程涉及到的方方面面 交互式教程 Grep by example 如何使用命令行工具 grep 进行文本搜索的交互式指南 Learn Git Branching 一个交互式的在线教程，帮助用户学习并练习 Git 的基本使用方法 在线课程 educative 为开发者提供交互式在线课程，重点关注技术领域的知识与技能 edX 由麻省理工学院（MIT）和哈佛大学共同创立的在线教育平台 exercism 专注于通过有趣且具有挑战性的练习问题、支持建设性同行评审机制来促进积极参与和技能提升，从而培养对各种现代计算范式的熟练掌握。 技术面试 Cracking the coding interview book 一本深受程序员喜爱的面试指南书 编程面试大学 涵盖了算法、数据结构、面试准备和工作机会等主题，帮助你准备大公司的技术面试 interviewing.io 一个提供模拟技术面试的平台 Pramp 一个模拟面试平台 Meetapro 一个可以找到专业人士进行模拟面试的网站 PPResume 一个基于 LaTeX 的简历生成器，目标是帮助人们在几分钟内创建一份精美的简历，并提供极高质量的排版和 PDF 输出。 大语言模型 Learn Prompting 一个开源的、多元化社区构建的课程，旨在提供完整、公正的提示工程知识。 提示工程指南 介绍大语言模型（LLM）相关的论文研究、学习指南、模型、讲座、参考资料、大语言模型能力及其与其他工具的对接。 面向开发者的大模型手册 基于吴恩达大模型系列课程的翻译和复现项目，涵盖了从 Prompt Engineering 到 RAG 开发的全部流程，为国内开发者提供了学习和入门 LLM 相关项目的方式。 LLM 应用开发实践笔记 作者在学习基于大语言模型的应用开发过程中总结出来的经验和方法，包括理论学习和代码实践两部分。 动手学大模型应用开发 面向小白开发者的大模型应用开发教程，基于阿里云服务器，结合个人知识库助手项目，通过一个课程完成大模型开发的重点入门。 iOS 开发 iOS \u0026amp; Swift - The Complete iOS App Development Bootcamp The 100 Days of SwiftUI Stanford CS193p - Developing Apps for iOS iOS and SwiftUI for Beginners Meta iOS Developer Develop in Swift Tutorials 苹果官方教程 SwiftUI Tutorials 苹果官方教程 计算机科学史 信息简史 ","date":"2024-08-24T15:51:30+08:00","permalink":"https://mahaoliang.tech/p/%E5%A4%A7%E5%AD%A6%E7%94%9F%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E4%B8%8D%E5%AE%8C%E5%85%A8%E5%88%97%E8%A1%A8/","title":"大学生计算机专业学习资源不完全列表"},{"content":"本文是我参加《第一期傲来操作系统（EulixOS）训练营》的项目实习报告。\n傲来操作系统（EulixOS）是由中科院软件所 / 中科南京软件技术研究院团队基于 openEuler 打造的操作系统发行版。\n任务目标 Helm 是一个 Kubernetes 的包管理工具，它可以帮助用户定义、安装和升级运行在 Kubernetes 上的应用程序。\nHelm chart test 是一个用于测试 Helm 图表的 CLI 工具，用于测试 Helm chart 的拉取请求，能自动检测与目标分支相比已经更改的 chart。\n本任务计划在 ARM 和 RISC-V 架构上运行 Helm 和 Helm chart Test 的测试，以此来对比这两种平台上云原生软件的成熟度。\nHelm 的单元测试 分析 Helm 的 Makefile 文件，发现 test-unit 目标是用来运行单元测试的：\n1 2 3 4 5 .PHONY: test-unit test-unit: @echo @echo \u0026#34;==\u0026gt; Running unit tests \u0026lt;==\u0026#34; GO111MODULE=on go test $(GOFLAGS) -run $(TESTS) $(PKG) $(TESTFLAGS) 可以看出，helm 的单元测试可以直接通过 go test 命令来执行。\n查看 go.mod 文件，确定该项目使用的 Go 版本是 1.22.0：\n1 2 3 4 5 6 7 8 module helm.sh/helm/v3 go 1.22.0 require ( github.com/BurntSushi/toml v1.3.2 ... } Helm chart test 的单元测试 Helm chart test 项目使用 build.sh 脚本进行构建发布。分析 build.sh 发现，在每次构建前，会使用 go test -race ./... 运行单元测试：\n1 2 3 4 ... go test -race ./... goreleaser \u0026#34;${goreleaser_args[@]}\u0026#34; ... 查看go.mod文件，确定该项目使用的 Go 版本是 1.22.0：\n1 2 3 4 5 6 module github.com/helm/chart-testing/v3 go 1.22.0 toolchain go1.22.4 ... 自动化执行测试 helm 和 helm chart test 的单元测试都可以直接通过 go test 命令来执行。我们可以使用 bash 编写脚本使测试过程自动化。\n这个脚本的主要流程为：\n自动识别和配置：脚本首先检测硬件平台，然后自动下载并在测试目录下安装 Go，不干扰系统中的其他设置或版本。 环境设置：配置必要的环境变量，确保测试在适当的环境下执行。 代码仓库管理：自动从配置的 Git 仓库地址克隆代码到本地指定目录。 测试执行：运行单元测试，并将结果输出到报告文件中。 性能数据收集：通过调用 performance_counter_920.sh 收集和记录测试期间的性能指标。 为了避免网络环境对测试影响，脚本可以自定义配置：\n通过 GO_BASE_URL 定义 go 安装包下载网址 通过 REPO_URL 定义项目源码仓库的地址。可以提前将项目从 GitHub 同步到 Gitee。 通过 GOPROXY 定义 Go 镜像地址。缺省设置为 GOPROXY=https://goproxy.cn，从国内镜像下载 moudle。 另外，helm 在测试插件功能时，会访问 https://github.com/adamreese/helm-env 。由于网络环境问题，涉及的测试经常会失败。所以我将 helm-env 项目同步到了 gitee，并修改了测试案例 vcs_installer_test.go，让它从 gitee 下载插件，保证了测试运行的稳定。\n两个项目的自动化测试脚本都已经提交到了 gitee，分别为：\nhttps://gitee.com/mahaoliang/helm-test https://gitee.com/mahaoliang/helm-chart-test 测试结果 下面是在 ARM 上运行的 helm 的测试结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Avg 10 times duration time: 24558698523 Avg 10 times task clock: 130738.640 Avg 10 times cpu-cycles: 290310169669 Avg 10 times instructions: 269526073395 Avg 10 times cache references: 95029267023 Avg 10 times cache misses: 764800123 Avg 10 times branches: \u0026lt;not Avg 10 times branch misses: 647318994 Avg 10 times L1 dcache loads: 95029267023 Avg 10 times L1 dcache load misses: 764800123 Avg 10 times LLC load misses: 480961351 Avg 10 times LLC load: 1156309818 Avg 10 times IPC: 0.928 helmrequiresearchfil.txt has been deleted Number of packages: 50 Analyzing test results in /home/cloud2/helm-test/reports/20240628135442/test_result Total tests: 1327 Passed tests: 1327 Failed tests: 0 一共运行了 1327 个测试用例，全部通过。\n在 ARM 上运行的 helm chart test 的测试结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Avg 10 times duration time: 704309016 Avg 10 times task clock: 5337.880 Avg 10 times cpu-cycles: 10071410187 Avg 10 times instructions: 8342502186 Avg 10 times cache references: 3300195194 Avg 10 times cache misses: 34476953 Avg 10 times branches: \u0026lt;not Avg 10 times branch misses: 39065962 Avg 10 times L1 dcache loads: 3300195194 Avg 10 times L1 dcache load misses: 34476953 Avg 10 times LLC load misses: 15971288 Avg 10 times LLC load: 50427439 Avg 10 times IPC: 0.828 Number of packages: 8 Analyzing test results in /home/cloud2/helm-chart-test/reports/20240628140012/test_result Total tests: 92 Passed tests: 92 Failed tests: 0 一共运行了 92 个测试用例，全部通过。\n测试过程详细输出的原始文件如下：\nhelm 的 go test 输出 helm 测试过程的 perf 统计指标 helm chart test 的 go test 输出 helm chart test 测试过程的 perf 统计指标 到测试截止时间，RISC-V 机器还未准备好，因此没有运行 RISC-V 架构的测试。不过我们已经提供了自动化测试脚本，可以直接在 RISC-V 架构的机器上运行，为后面在 RISC-V 上的测试做好了准备。\n","date":"2024-06-29T10:44:41+08:00","permalink":"https://mahaoliang.tech/p/helm-%E5%92%8C-helm-chart-test-%E5%9C%A8-arm-%E5%92%8C-risv-v-%E4%B8%8A%E7%9A%84%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/","title":"Helm 和 Helm chart Test 在 ARM 和 RISV-V 上的自动化测试"},{"content":"文本准备 要创建一个以 .py 扩展名结尾的文本文件，你可以按照以下步骤进行操作：\n打开文本编辑器。\n你可以使用操作系统自带的文本编辑器（如记事本、TextEdit 等），或者使用专业的代码编辑器（如 Visual Studio Code、Sublime Text、Atom 等）。\n在文本编辑器中创建一个新文件。\n将你的 Python 代码复制粘贴到新文件中。 保存文件时，指定文件名并确保使用 .py 作为文件的扩展名。 例如，你可以将文件命名为 plot_example.py。\n代码示例（画的是一个 y=x^2 的图）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import numpy as np # 准备数据 x = np.linspace(-10, 10, 100) # 在 -10 到 10 之间生成 100 个均匀分布的点 # 计算对应的 y 值 y = x ** 2 # 创建图形并设置标题 plt.figure() plt.title(\u0026#34;Plot Example\u0026#34;) # 绘制折线图 plt.plot(x, y) # 显示图形 plt.show() repr 运行 我在 Mac 上用的是visual studio code， 点击文件，用 VS code 打开。 打开后，假如没装 python 的话，会显示要 install python 编译器，下载就可以运行了了 ","date":"2023-09-15T15:07:58+08:00","permalink":"https://mahaoliang.tech/p/%E5%A6%82%E4%BD%95%E7%94%A8%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%BB%98%E5%88%B6%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0/","title":"如何用命令行绘制数学函数"},{"content":"视频逐帧提取 运用脚本来完成手动工作，即 n 秒一次截屏 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 tell application \u0026#34;IINA\u0026#34; reopen activate delay 0.5 tell application \u0026#34;System Events\u0026#34; keystroke (ASCII character 32) delay 0.1 end tell repeat tell application \u0026#34;System Events\u0026#34; keystroke \u0026#34;s\u0026#34; using {command down} end tell delay 0.5 end repeat end tell 这里的第三个 delay 是来控制截屏间隙的，delay 0.5就是 0.5 秒截一次屏。\n这个脚本只试用于 Mac 电脑，因为 command+s 是 Mac 上的快捷键。\npng 转 svg，再转 png svg 就是字符矢量图，但是它不能导入视频软件来制作视频，所以还要再转回 png。 copy pics to tencent server 1 scp $HOME/Pictures/Screenshots/*.png mahaoliang:/home/ubuntu/works/pics/ 这里我是上传到服务器上搞的， install app 1 2 3 sudo apt-get install caca-utils sudo apt-get install librsvg2-bin sudo apt install imagemagick run script 1 ./run.sh 因为有很多张图片，所以写了一个脚本 run.sh，就是将 png-\u0026gt;svg-\u0026gt;png 的动作重复，命令如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # clean mkdir -p \u0026#34;${svg}\u0026#34; mkdir -p \u0026#34;${output}\u0026#34; # 使用循环遍历目录中的每个文件 for file in \u0026#34;${pics}\u0026#34;/*.png; do # 检查文件是否是普通文件 if [[ -f $file ]]; then filename=$(basename \u0026#34;$file\u0026#34;) filename=\u0026#34;${filename%.*}\u0026#34; echo \u0026#34;processing $file\u0026#34; # 这里可以添加你的处理逻辑 img2txt -W 200 -f svg \u0026#34;$file\u0026#34; \u0026gt;${svg}/\u0026#34;${filename}\u0026#34;.svg fi done for file in \u0026#34;${svg}\u0026#34;/*.svg; do # 检查文件是否是普通文件 if [[ -f $file ]]; then filename=$(basename \u0026#34;$file\u0026#34;) filename=\u0026#34;${filename%.*}\u0026#34; echo \u0026#34;processing $file\u0026#34; # 这里可以添加你的处理逻辑 rsvg-convert \u0026#34;${file}\u0026#34; \u0026gt;${output}/\u0026#34;${filename}\u0026#34;.png fi done #convert ${output}/*.png output.gif tar -zcvf output-${timestamp}.tar.gz ${output} 这里面#convert ${output}/*.png output.gif,可以把#去掉，这样就可以直接制作字符动图了（gif）。 download 1 scp mahaoliang:/home/ubuntu/works/output.png . 动画制作 我用的是苹果的 final cut pro，试用期是 90 天，假如到期了话就可以输入一下代码，重置时限： 1 2 3 4 cd ~ cd Library/Application\\ Support ll -a rm .ffuserdata 将所有图片依次导入软件，建立复合片段，再调整速度就好了。 结尾 最后送大家一个 GIF\n视频 https://cdn.mahaoliang.tech/images/202309082303161.mp4\n","date":"2023-09-08T20:06:23+08:00","permalink":"https://mahaoliang.tech/p/%E5%AD%97%E7%AC%A6%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C/","title":"字符动画制作"},{"content":"网址 直接推网站：DeGraeve.com\n步骤 图片要准备 URL 的链接，例如图中的https://www.degraeve.com/images/lcsm.gif 可以看到下面一行有三个选项，ASCII art、color HTML 和 RTTY。以下分别是这三个的效果图 可以发现三者，有颜色的还原度最高，第一个还原度第二，第三个最差。 怎么选就看个人了。\n总结 这个网站还有很多其他的功能，因该是为数不多的优良的免费工具了。\n","date":"2023-09-04T12:57:56+08:00","permalink":"https://mahaoliang.tech/p/%E5%9B%BE%E7%89%87%E8%BD%AC%E5%AD%97%E7%AC%A6%E7%BD%91%E7%AB%99%E5%88%86%E4%BA%AB/","title":"图片转字符网站分享"},{"content":"0. 前言 北京时间晚上 11:43，舍友，唯一一个舍友，上床睡觉去了。现在是琪亚娜时间！！！\n$MUSIC=I Really Want to Stay at Your House$\n1. 我的世界树 我这个人性格上跟村上春树很像，不折不扣的个人主义。用中国人的一个词来讲，就是天性凉薄的人。我之前写过，由于我这十八年从没有表达过自己，导致了我的情感淡化，麻木，成了无感之人。看医生的时候，问我在某个特定的情境下有什么感受，或是对我之前的经历有什么感受，我完全回答不上来，“我母鸡呀”。我写这篇文章呢，并不是要给谁看的，而是在练习自我表达，缓解病情，虽然这“天性凉薄”可能已经刻在了我的 DNA 里啦。\n我并不了解我的父母，在这十几年里，感觉像跟陌生人一样生活在一起，这一切大部分是我的问题，谁叫我天性凉薄呢。我妈每次都唠叨，假如我现在回家，她一定会先叫我洗手，但我一定不会洗。我爸好沉默的，以前我小学的时候，他辅导我数学经常发火，于是我家买了一个一米厚的木头长桌。别问发火与买桌子有啥关系，问就是一米厚拍不断。我呢，就基本上在打游戏啦。在学校里累成狗，回家最快，最容易上手的就是打游戏啊。初二时，我晚上两点偷偷爬起来打平板。平板就在客厅的长桌上，我每次都得摸着过去，因为要经过爸妈的房间，每次拿到 iPad 回到房间里心都得猛跳两下。高三时，高考前两个月，我白天在家里打游戏，晚上和爸妈出去吃饭，吃饭时打游戏，吃完饭后在 Star Bark 里打游戏。真就游戏人生咯。\n有人可能要发：逆天作者，天天打游戏。但这就要谈及价值观啦。前面跑题那么多哦，终于要谈我的世界树，也就是价值观了。直接上结论，一个人想怎么活就怎么活，不论是读一辈子书，还是打一辈子游戏。自己觉得值了，那便值了。人生嘛，哪里有什么高低贵贱。人死后，他生前做了什么都与他无关了。至于他人的评论嘛，那就当是放屁。一辈子就几十年，减去睡觉吃饭，有 B 站 up 算过，9795 天，@元菜 Yilia，还能有时间在意别人的看法吗？\n就用乔布斯的话来总结吧：JUST DO IT！！！\n$MUSIC=Golden Hour$\n2. 怎么看待人生：躺着看呀 我现在就想躺平。《20 年读书，余生还房贷，死后给医院做贡献》，真实不？我想大多数人多是这样的人生吧，我假如不啃老也是。\n我高三的最后两个月，看了一些视频，彻底把我打醒了。\n一个大学生，好歹是 211 的，考了两年的研，考不上，也没工作，发视频自嘲。\n上海的一家药工厂，一个月三千，只招 985 的。\n学习是为了什么？就为了那三千块钱，或是压根找不到工作？人生是为了什么？就是为了卷一辈子？我好像一出生就走上了一条路：拼了命的考了一个好的高中，拼了命的考了一个好大学，拼了命的考研，工作后拼了命的加班是为了拼了命的还房贷。我为什么要走这条路？这条路真的非走不可吗？中国人真是“勤奋”啊，吃苦一辈子，劳累一辈子。我当然知道小镇做题家，这是他们唯一出路，但这绝不是我的唯一。\n是的，当时我觉望了，我躺了，我选择躺看人生。对了，这也不应该叫躺。我们这里，只要不加班了，只要一娱乐了，只要不上进了，就叫躺平，凭什么呀？吃苦吃的。别地的健康青年到我们这里就叫躺平了。我“躺”了两个月，即使是现在，我也是在躺平。\n《保安少走 40 年弯路，躺平少走 60 年弯路，人生何其短暂，走自己喜欢的路就行。》\n","date":"2023-09-03T20:55:25+08:00","permalink":"https://mahaoliang.tech/p/%E9%A9%AC%E7%9C%BC%E7%9C%8B%E4%B8%96%E7%95%8C/","title":"马眼看世界"},{"content":"1. 文本部分 1.1 斜体和粗体，删除线 使用 * 和 ** 表示斜体和粗体。\n示例：这是斜体，这是粗体。\n使用 ~~ 表示删除线。\n这是一段错误的文本。\n1.2 分级标题 使用 === 表示一级标题，使用 \u0026mdash; 表示二级标题。\n效果图 你也可以选择在行首加井号表示不同级别的标题 (H1-H6)，例如：# H1, ## H2, ### H3，#### H4。\n效果图\n1.3 常用 Emoji \u0026amp; Font-Awesome 2.0 常用布局 2.1 无序列表 使用 *，+，- 表示无序列表。\n示例：\n无序列表项 一 无序列表项 二 无序列表项 三 2.2 有序列表 使用数字和点表示有序列表。 示例：\n有序列表项 一 有序列表项 二 有序列表项 三 2.3 行内代码块 使用 代码 表示行内代码块。(就是这个``)\n2.4 插入图像 使用：\n! [描述]\n$+$\n( 图片链接地址 )\n$+$\n插入图像。\n2.5 表格支持 项目 价格 数量 计算机 $1600 5 手机 $12 12 管线 $1 234 \u0026gt; ce ce 效果图\n2.6 常用数学符号 ","date":"2023-09-03T19:58:18+08:00","permalink":"https://mahaoliang.tech/p/markdown-%E8%AF%AD%E6%B3%95/","title":"Markdown 语法"},{"content":"前言 本文借鉴了多位米游社大佬的文章，大佬名单：\n@为胡桃坐牢火本、@110000cc、@光坂镇守镇人、怎么能沉迷游戏 QAQ、@枕香半縷夢。\n好了，正片开始。\n1:2 的暴击与暴伤比最优的由来 为什么会常说暴击率：暴击伤害为 1:2 时最优？双暴很大一部分来源于圣遗物词条，而圣遗物每一个有效词条均有四个档位，如下图。\n可以发现，在同档位下，暴击与暴伤的比值正好为 1:2。这个 1:2 与最优解的比值一样，并不是巧合，解释如下。\n就拿暴击率为 3.9% 加成每词条，暴击伤害 7.8% 每词条，同一档位的为例。（取不同档位比较是无意义的）\n假设双暴有效词条数共有 x 条，暴击率有 a 条，则暴击伤害有 (x - a) 条\nx：常数，是一个已知的固定值\na：变量，由圣遗物决定，也由玩家选取决定\nx - a：变量，由圣遗物决定，也由玩家选取决定\n即此时\n$ 暴击区间=1+3.9%\\times a\\times 7.9%\\times(x-a)=1+30.42%\\times a\\times(x-a)$\n初中知识，简单的抛物线函数， 当 a 为 0.5x 时，值最大，那么此时（x - a）同样为 0.5x 即 40 个有效双暴词条，暴击率暴伤个 20 词条时（20 个有效，则各有 10 个时），伤害值最大 故在其他条件不变下，暴击词条数：暴伤词条数 = 1:1 最优 又由于每个暴伤有效词条是暴击率词条加成的 2 倍，故暴击率：暴击伤害 = 1:2 时伤害最高\n精通与双爆的分配 不同武器的被动不同，所以即使是总面板一样，伤害还是有差异的。那么按照武器分类，不同武器的最终面板与伤害的图如下： 圣遗物的选择 圣遗物选择：\n4 魔女（输出上限） 2 件套：获得 15％火元素伤害加成。 4 件套：超载、燃烧、烈绽放反应造成的伤害提升 40%，蒸发、融化反应的加成系数提高 15%。施放元素战技后的 10 秒内，2 件套的效果提高 50%，该效果最多叠加 3 次。\n4 追忆（输出相对稳定） 2 件套：攻击力提高 18%。 4 件套：施放元素战技时，如果角色的元素能量高于或等于 15 点，则会流失 15 点元素能量，使接下来的 10 秒内，普通攻击、重击、下落攻击造成的伤害提高 50%，持续期间内该效果不会再次触发。\n2 魔女 2 乐团 乐团 2 件套：元素精通提高 80 点。（寄）\n配队 胡行钟夜 yyds。\n分析：胡桃主 C，夜兰副 C 加辅助增伤加挂水，行秋减 15% 水抗加高频挂水，钟离给盾加减抗。另外，双水加 25% 生命，提高了夜兰和胡桃的输出，钟离的盾量。完美！！！\n注：夜兰带终末不仅增加了充能，还给胡桃提供了额外的精通。钟离还可以尝试一下教官套，也可以加精通。\n","date":"2023-09-01T21:16:54+08:00","permalink":"https://mahaoliang.tech/p/%E8%83%A1%E6%A1%83%E7%9A%84%E6%9C%80%E4%BC%98%E9%9D%A2%E6%9D%BF/","title":"胡桃的最优面板"},{"content":"为啥有人称我们为水军呢？我们明明还是训练的，只是不多而已。以下是我们的军训时间表。\n8.29 开始训练 7 天，10 月份再训练 7 天。\n就我亲生经历来讲，整体上不太累。我在巡时每天早上 6:40 起床，7:30 在食堂吃完早餐，7:50 集合，8:00 开训，练到 9 点多开始坐着唱军歌（休息），10:20 结束早上训练。下午 1:40 集合，听讲座，又是睡觉时间，听到 4:30 结束下午的训练。要说累，就是晚上训练很累，6:00 集合，到二期校园的体育场练习。走过去 20 分钟回来 20 分钟，期间要训练两个多小时，9 点多回到一期校园，结束训练。\n训练时手机要上交，训练完了就会拿回来。训练过程中的休息时间的长短要看教官。我的教官就挺好的，休息训练五五开\n重点来了，军训累不累还要看老天爷的心情。停训的时间是不会补回来的！！！\n最后，在提一点注意要点。\n军训的穿的军鞋质量很烂，站久了脚会很疼，而且鞋底还会掉色，白袜子会被染黑的，我舍友就是例子。所以，为了您的脚和袜子，一定要买一双鞋垫，塞进去垫着。\n","date":"2023-09-01T19:23:19+08:00","permalink":"https://mahaoliang.tech/p/uic-%E7%9A%84%E5%86%9B%E8%AE%AD%E6%97%A5%E8%AE%B0/","title":"UIC 的军训日记"},{"content":"生成新的文章 进入到本机网站目录\n1 cd documents/works/mahaoliang/mahaoliang.stack.tech 在保存本机网站的目录下运行下面的命令，生成一篇文章\n1 hugo new post/kubernetes-overview.md 编辑文章 使用 Typora 打开新建的文章，首先编辑文章的元信息：\n1 2 3 4 5 title: \u0026#34;Kubernetes工作原理概述\u0026#34; date: 2022-07-22T17:30:11+08:00 draft: false tags: [docker,linux,kubernetes] categories: [tech] 然后编辑文章内容，图片使用 picGo 上传到图床。\n将文章发布到 GitHub 在保存本机网站的目录下运行下面的命令，将文章发布到 GitHub\n1 2 3 git add . git commit -m \u0026#34;kubernetes\u0026#34; git push 网站内容更新 登录到腾讯云主机，\n1 ssh guangzhou-tencent 进入 ~/works/mahaoliang.stack 目录，执行命令：\n1 git pull 完成\n","date":"2023-08-31T20:54:03+08:00","permalink":"https://mahaoliang.tech/p/%E6%96%87%E7%AB%A0%E5%8F%91%E8%A1%A8/","title":"文章发表"},{"content":"雅思学习｜心得 ∆口语 口语对于我来说是最难的部分，不仅仅有社恐的因素，还有 oral speaking 的语速问题（over 120 words per minute is standard）。\n特别要注意的是口音并不太重要，重要的是 例子！！栗子！！真实例子！！ 不管是 part 1，part 2，还是 part 3，都需要例子，只不过每个 part 的例子各有区别罢了。区别后面讲。 不能有太多的停顿，一个问题最多两个停顿，多了就有可能扣分，再多些考官就会 cut you out，然后你的心态就可能会崩，导致后面答题的状态，引发蝴蝶效应。 注意时间限制，part one 和 part three 一个问题回答一分钟左右，没有准备时间！！！part two 有一分钟的准备时间，给一张纸让考生写提示词，讲述时可以看。 Part 1 在 part 1 时，举例子只用举自己的轻身例子，并用第一人称讲述。讲述时，时态讲错了不用管，因为不重要，重要的是要讲述具体细节\n例如，说出去吃饭时要说清楚和谁去（who），为什么去（why），什么时候去的（when），去哪里吃（where），吃的什么类型的餐厅的什么菜 (what)，心情如何（how），即6W 要素。\neg. I usually hang out with my best friend, Tom, to have a big meal on weekends in a shopping centre like the Coastal City in Nan Shan district in Shenzhen because the food in school\u0026rsquo;s canteen is tasteless. And we always go to the Green Tea restaurant there, a famous chinese restaurant, for having some crispy chicken, spicy tofu, or the boiled Fish with Sichuan pickles. That really makes me a happy mood to face the disgusting food next week in school.\n在上面的例子中，我每提出一个概念都给出了解释或是具体的事物。例如，friend 是 Tom，a shopping centre 是深圳南山区的海岸城 the Coastal City，the Green Tea restaurant 是一家中餐厅。以上就是我所说的具体细节，就是得分要点。没有细节口语就不可能上 6 分，只有 5 分。\nPart 2 在 part 2 时，也要举亲身经历，题目怎么问就怎么答，讲出具体细节就可以了。准备时，最好四十秒写提示词，二十秒再在脑中过一遍，以免出现太多停顿。\n例如题目：Describe a daily routine that you enjoy\nYou should say, what you do, when it became your daily routine, whether you will change it in the future, and explain why you enjoy it\neg.\nThe daily routine I am going to tell you about is that I jog at around 6 pm nearly every afternoon. At that time, the sun has already started sinking, but the darkness has not fallen, creating a mild temperature. Right next to my community is a huge park stretching over 2 kilometers, which is a perfect place to run around. Sometimes, I would also go to the stadium several blocks away. It has a standard 400-meter track, enabling me to precisely calculate the distance and speed.\nI developed this habit several months ago when I accidentally caught Xiaoming doing some warm-up activities at the side of the street on my way back from the library. I pulled over my electric bicycle and asked what he was doing. He said he was going to start his daily jogging and invited me to jog together. I thought, why not. But what I did not expect was that I overestimated my stamina. After one kilometer, I felt I could not breathe and had to give up. But since then, I have jogged with him almost every day.\nI do not think I will change this daily routine in the near future because it has substantially improved my health. Before I jogged, I suffered from sub-health resulting from my sedentary lifestyle. I felt tightness in my chest now and then, my back and waist ached, and I could not help panting after climbing only two floors. But now, these problems rarely surface.\nApart from the health benefits I have mentioned, I enjoy jogging because it can soothe my tense nerves. You know, it helps me concentrate on my breaths and steps and thus temporarily escape from all the troubles.\n在上面的回答中，加粗的字是要写在纸上的，其他部分就只能自由发挥了\nPart 3 在 part 3 时，回答的方式和 part 1 差不多，只是第一人称要换成第三人称，如 the people，they，them 和 their。讲述时按照OREO的方法就可以了，讲述时间不能超过 1 分钟，40 秒就足够了。举例的时候，只用举一个就可以了，越详细越好。\n例如：Q do people\u0026rsquo;s routines differ on weekdays and weekends?\neg.\nThe routines on weekdays are regular but routines on weekends are not. (opinion) (reason) For example, people have to go to school or go to the workplace at 7 am every weekday and have lunch at 12 o’clock, and they had to stay at school or work place until at about 6 PM. But on the weekends, people can get up late at around 11am if they want, having a big meal in the restaurant like the green tea restaurant for the crispy chicken and spicy tofu. And they can go home at any time they want.(example) so the routines on weekends are flexible but on weekdays are stricted.(opinion) 如何讲好例子 背景（一句话）\n4、5 个 actions ————\u0026gt;good example\n2、3 个 actions————\u0026gt;small example\n结果，the end\n例如：Describe the word \u0026ldquo;patience\u0026rdquo; .\neg.\nIf a naughty kid is making noise, laughing and shouting, in a public place like the library named the Book City in Nan Shan district in Shenzhen and his parents are not there （background）, I will not get mad at him or punch on his face. I will first grab his hands to stop him from running around the whole place, and tell him to be quiet. （actions） Then, I will send him to his parents to tell about his bad behavour, to make sure that he will not make noise in the public any more. （the end） 注：加粗为 actions ∆阅读 题型 顺序题：\n填空题（sentence completion 和 short answer question） 判断题（T/F/NG 和 T/N/NG） 选择题（multiple choice） 乱序题：\n人物匹配题（matching features） 中心句匹配题（matching headings） 信息匹配题（matching information） 注意 做信息匹配题时，要先读题，思考题目中的关键词会替换成什么。 人物匹配题和信息匹配题出现 NB 时，代表有一个人会被选两次，或有一段会选两个信息。 ∆写作 类型 小作文\n流程图 数据图 大作文\n内容 小作文\n信息转述（Introduction） 总述（overview） 特点一（feature one） 特点二（feature two） 大作文\n信息转述（introduction） 一方面（1st side） 另一方面（2nd side） 总结（conclusion） ∆听力 题型 填空题 选择题（单/多） 地图题 注意 同义替换，一般与答案一起出现 ","date":"2023-08-29T21:59:36+08:00","permalink":"https://mahaoliang.tech/p/ielts-learning/","title":"IELTS Learning"},{"content":"SDKMAN！是用于管理多个软件开发工具包的并行版本的工具。\n可安装的软件列表\n列出当前可安装的软件列表 1 sdk list 列出当前可安装的 Java 版本列表 1 sdk list java 安装指定的版本 1 sdk install java 17.0.7-tem 查看当前使用的版本 1 2 sdk current sdk current java 设置缺失使用的版本 1 sdk default java 17.0.7-tem 设置当前 shell session 使用的版本 1 sdk use java 8.0.332-tem 更新软件仓库 1 sdk update sdk 软件本身的更新 1 sdk selfupdate ","date":"2023-06-24T18:08:17+08:00","permalink":"https://mahaoliang.tech/p/sdkman-%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"Sdkman 的使用"},{"content":"1Password的最大优势是跨平台，不管是苹果系的 iOS，macOS，还是 Google 的 Android，微软的 Windows，以及 Linux，都能完美支持。让你在 macOS 上使用 Safari，在 Windows 上使用 Edge，Android 上使用 Chrome 时，都能无缝得到密码管理的支持。这是苹果自带的密码管理所不能满足的。\n如何使用好 1Password，官方文档Get started with 1Password是最好的学习素材，快速浏览一遍，基本会使用完全没有问题。\n这篇文档我只重点介绍在 macOS 和 iOS 上使用 1Password 时的一些注意事项。\n安装 macOS 和 iOS 上的 Safari，都需要打开1Password 插件。\nmacOS 的 Safari 插件直接在 App Store 安装。\niOS 上插件的安装请参考：Get to know 1Password for Safari on your iPhone or iPad\nOpen Safari to any website and tap in the address bar.\nIf you\u0026rsquo;re using an iPad, tap in the address bar.\nTap Manage Extensions and turn on 1Password, then tap Done.\n关掉内置密码管理 由于 Safari 自带了密码管理，同时打开会和 1Password 有冲突，因此需要关闭内置的密码管理。\n具体操作参考官方文档Turn off the built-in password manager in your browser。\nmacOS 上 Safari 的设置 To stop Safari from asking to save your passwords:\nClick the Safari menu and choose Preferences. Click the AutoFill icon. Turn off all the AutoFill web forms settings: “Using info from my contacts”, “User names and passwords”, “Credit cards”, and “Other forms”. iOS 上 Safari 的设置 To stop Safari from asking to save your passwords:\nOpen Settings, then tap Passwords \u0026amp; Accounts. Tap AutoFill Passwords. Turn off iCloud Keychain. iOS 上 App 的支持 iOS 上 app 使用 1Password 的体验和 Safari 是一致的，只要 app 使用 iOS 系统标准键盘，在需要输入用户名和密码的地方，会自动出现钥匙图标，点击钥匙，会呼出 1Password 进行自动填充。\n然而很多国产银行类 app，基本都不使用 iOS 内置键盘，所以没法呼出 1Password。这时候只能自己进入 1Password 进行拷贝密码或创建密码的操作。\nUniversal Autofill Universal Autofill 是 1Password 8的一个重大升级功能。Universal Autofill 实现了真正的「全局填充」。你只要记住一个快捷键 Command + \\ ，就可以在浏览器、应用程序、终端和系统提示等任何地方，让 1Password 帮你自动完成密码填充。\n两步认证 对于已经支持 1Password 两步认证 的网站，都已经迁移到了 1Password。\n如果 1Password 本身开启了两步认证，它就需要一个第三方的 Authenticator，因为它不可能自己保存自己的 one-time password，官方文档已经说明。我选择了微软的Authenticator。\n微软的 Authenticator 可以备份和恢复，在换手机时非常方便。另外使用 Authenticator 登录微软账号时体验很好，不需要输入密码，只用在手机上确认即可，体验类似扫码登录，但又不用扫码:)\n管理 SSH Keys 1Password 可以保存你的 SSH keys，并作为 SSH agent ，无缝整合 SSH 和 Git 工作流。同时，在 GitHub 等平台需要填写公钥的地方，自动帮你拷贝填充公钥。\n生成或导入 SSH keys 我们可以使用 ssh-keygen 命令自己生成 SSH key，也可以在 1Password 中创建 SSH key 项目时自动生成。 开启 1Password SSH Agent 1Password SSH Agent 使用你保存在 1Password 中的 SSH Key，与你的 Git 和 SSH 工作流程无缝集成。它可以验证你的 Git 和 SSH 客户端，而这些客户端永远无法读取你的私钥。\n首先要打开 1Password SSH Agent，让它在后台运行，为你的 SSH 客户端处理认证。打开 1Password \u0026gt; 偏好设置 \u0026gt; 开发者，勾选“使用 SSH agent”和“授权连接时显示密钥名称”。 为了确保 SSH Agent 在后台持续运行，需要在 1Password \u0026gt; 偏好设置 \u0026gt; 通用 中，勾选“在菜单栏中保留 1Password”。\n配置 SSH 客户端 为了让 SSH 客户端能使用 1Password SSH agent，需要将 IdentityAgent 配置添加到 ~/.ssh/config文件中：\n1 2 Host * IdentityAgent \u0026#34;~/Library/Group Containers/2BUA8C4S2C.com.1password/t/agent.sock\u0026#34; 你也可以在 Shell 中设置 SSH_AUTH_SOCK 环境变量：\n1 export SSH_AUTH_SOCK=~/Library/Group\\ Containers/2BUA8C4S2C.com.1password/t/agent.sock 如果觉得 agent 路径过于复杂，可以先创建一个符号链接：\n1 mkdir -p ~/.1password \u0026amp;\u0026amp; ln -s ~/Library/Group\\ Containers/2BUA8C4S2C.com.1password/t/agent.sock ~/.1password/agent.sock 这样就可以在设置环境变量时直接引用符号链接：\n1 export SSH_AUTH_SOCK=${HOME}/.1password/agent.sock 现在 SSH 客户端就可以在登录远程主机时使用 1Password SSH agent。\n可以使用下面的命令查看，1Password SSH agent 帮我们管理的 SSH Keys：\n1 ssh-add -L 发布 SSH 公钥 我们需要使用某种方式，将 SSH 公钥发布到远程服务器，以便对方能利用公钥验证你的身份。\n一种方式是把公钥上传到服务提供者的网站，将公钥和你的账号绑定。例如你可以在GitHub SSH key settings页面上传公钥。腾讯云的管理控制台也可以上传你的公钥，然后将公钥和你购买的服务器绑定。在页面填写公钥时，1Password 会像填充密码一样进行自动填充。\n另一种方式就是使用 ssh-copy-id 命令，直接将公钥拷贝到远程服务器。\n不管使用哪种方式，一定要记住你发布的是公钥，千万不能不小心泄漏了私钥。\n管理 Github Signing Key Github 开始支持使用 SSH Key 来签名提交，也就是说，我们可以用 1Password 管理的 SSH key 来签名 git commit。\n设置过程可以参考Sign your Git commits with 1Password，主要包含两个步骤：\n在 GitHub 上生成 Signing Key。访问https://github.com/settings/keys ，选择“New SSH Key”，选择 key 的类型为“Signing Key”，然后填入 1Password 管理的 SSH 公钥。 配置本地的 .gitconfig。在 1Password 中，选择 SSH Key，并在上面显示的横幅中选择 \u0026ldquo;配置 \u0026ldquo;选项： 按照提示，配置你的 .gitconfig文件。这些设置选择了你的 SSH key，并在 git commit 的时候使用 SSH Key 签名。\n“Sign in with” anything 1Password 将记住用户使用的第三方登录服务（如 sign in with Google）。看官方消息，这个功能很快会上线。\n","date":"2022-09-29T22:06:18+08:00","permalink":"https://mahaoliang.tech/p/1password-%E5%9C%A8-macos-%E5%92%8C-ios-%E4%B8%8A%E7%9A%84%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/","title":"1Password 在 macOS 和 iOS 上的使用技巧"},{"content":"基本设置 打开终端的偏好设置，点击描述文件tab，将 Pro 设置为默认描述文件。\n然后对 Pro 进行配置。\nTab 页 设置 文本 勾选“平滑文本”。可自定义背景透明度。 窗口 窗口大小：行 120 列 30 窗口 选择 将行数限制为：10000 shell 当 shell 退出时，选择当 shell 完全退出后关闭 键盘 勾选 将 option 键当 Meta 键 高级 确认终端为 xterm-256 color 安装Xcode Command Line Tools Xcode Command Line Tools 包含了clang编译器，git客户端等命令行常用的工具。使用下面的命令安装：\n1 xcode-select --install 安装Oh My Zsh 参照 Oh My ZSH! 的官方文档进行安装。\n1 2 3 4 5 #确认zsh版本 zsh --version #执行安装 sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; 如出现连接问题，请在终端设置科学上网。\n1 export https_proxy=http://127.0.0.1:7890 http_proxy=http://127.0.0.1:7890 all_proxy=socks5://127.0.0.1:7890 安装和配置 Powerlevel10k powerlevel10k 是一个 Zsh 的主题，具体很强的灵活性，并且非常美观。\n首先安装 Powerlevel10k 所推荐的字体 Meslo Nerd Font，可以在命令行终端显示一些特殊符号。下载并安装下列字体：\nMesloLGS NF Regular.ttf MesloLGS NF Bold.ttf MesloLGS NF Italic.ttf MesloLGS NF Bold Italic.ttf 然后更改终端的字体，在终端的偏好设置的的描述文件中，选择我们使用的 Pro，设置字体为MesloLGS NF，字体大小为 14。\n由于我们使用的是 Oh My Zsh，可以把 Powerlevel10k 作为一个主题，安装到 Oh My Zsh 中：\n1 git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k 如果遇到网络问题，可以参考上面，设置终端科学上网代理。\n编辑 Oh My Zsh 的配置文件 ~/.zshrc，设置主题为 Powerlevel10k\n1 ZSH_THEME=\u0026#34;powerlevel10k/powerlevel10k\u0026#34; 重新开启终端，按照提示进行 Powerlevel10k 的样式配置，完成后，我们漂亮的命令行终端配置就大功告成了。\n如果你在使用VSCode，需要在配置文件settings.json中设置下面两个配置，你就可以让VSCode的终端同样适配 Powerlevel10k。\n1 2 \u0026#34;terminal.integrated.fontSize\u0026#34;: 14, \u0026#34;terminal.integrated.fontFamily\u0026#34;: \u0026#34;MesloLGS NF\u0026#34; 安装 zsh-autosuggestions 插件 Oh My Zsh 在安装完成后，已经自动配置了 git 插件。为了在命令行终端更快捷的工作，还可以为 Oh My Zsh 安装zsh-autosuggestions 插件。\nzsh-autosuggestions 提供类似于Fish shell 自动建议功能，它会根据历史记录，在你键入命令的时候，提供非侵入式的自动建议。\n安装zsh-autosuggestions 的命令：\n1 git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions 在 ~/.zshrc 中配置插件：\n1 plugins=(git zsh-autosuggestions) 在键入命令时，会有灰色的提示信息，按 → 或 ctrl-f 自动完成。是不是非常方便，用过后就完全离不开了。\n命令别名配置 使用命令行操作非常快速便捷，但有时候你不知道命令具体干了什么，例如我输入的 rm *到底删除了哪些文件。\n其实这些命令都有参数，详细的输出该命令影响的文件。但每次都输入这些参数实在太麻烦，我们可以在 Zsh 的配置文件 ~/.zshrc 中为这些命令设置别名：\n1 2 3 4 5 alias mkdir=\u0026#39;mkdir -v\u0026#39; alias mv=\u0026#39;mv -v\u0026#39; alias cp=\u0026#39;cp -v\u0026#39; alias rm=\u0026#39;rm -v\u0026#39; alias ln=\u0026#39;ln -v\u0026#39; ","date":"2022-07-23T15:26:12+08:00","permalink":"https://mahaoliang.tech/p/%E6%89%93%E9%80%A0%E4%B8%80%E4%B8%AA%E6%BC%82%E4%BA%AE%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%BB%88%E7%AB%AF/","title":"打造一个漂亮的命令行终端"},{"content":"拿到一台新的 MacBook，总是要经过一些设置才能让它变得顺手好用。这些配置也许对你也有启发和帮助。\n触控板设置 单指轻点代表单击。拿到 MacBook 第一个要设置的，对它轻柔一些，单击毫不费力 四指左右轻扫，桌面间切换 四指向上，进入调度中心。“调度中心”提供了一个鸟瞰图，平铺了所有应用的窗口、桌面空间，可以轻松地在它们之间切换。点击右上角“+”号可以创建新桌面。 四指向下，App Expose，查看正在使用的 App 的所有窗口。 设置三指拖移：苹果菜单  \u0026gt; “系统偏好设置” \u0026gt; “辅助功能” \u0026gt; “指针控制” \u0026gt; “触控板选项” \u0026gt; “启用拖移”，然后从菜单中选取“三指拖移”。 词典配置 在触控板设置中，勾选“三指轻点”触发查询与数据检查器。\n在词典应用的偏好设置中，除了缺省选择的词典，建议增加勾选一个英英词典。用英语解释英语单词，就想我们查汉语词典一样，是一种更地道的学习英语方式。\n在任何应用中，都可以通过“三指轻点”，触发词典。因为是系统原生支持，体验丝滑顺畅。\n原生词典方便好用，但是没有单词本功能。网上找到一种利用“自动操作”生成单词本的方法，可以尝试。\n系统偏好设置 滚动条设置\n始终显示滚动条 跳至点按的位置 触发脚设置\n右下角显示桌面 左上角屏幕保护。在安全和隐私中设置：进入屏幕保护后立即要求输入密码。这样既是屏保又可以锁屏 移除 Dock 上大部分可以用 Spotlight 唤出的应用。\n设置 Dock 自动隐藏，扩大桌面可用区域。\nF1~F12 保持缺省设置，作为特殊功能键。如果需要使用 F1~F12 的标准功能，需要配合fn\n关闭英文自动补全。系统偏好设置 \u0026gt; 键盘 \u0026gt; 文本 \u0026gt; 取消自动纠正拼写。\n控制中心设置 为了节省菜单栏的空间，可以让不用的控制项只在控制中心显示，不在占用菜单栏，如蓝牙，隔空投送等。\nSpotlight 可以用快捷键呼出，不用在菜单栏显示。\n时间可以用 24 小时制，缩短占用的菜单栏。\n按住 Cmd 键，可以用指针拖动菜单栏图标，按你喜欢的顺序排列。\nFinder 设置 Finder 中的“个人收藏”可以偏好设置中进行定制，可以把常用的文件夹拖到个人收藏 Finder 的四种显示模式，个人喜欢分栏模式 可自定义 Finder 工具栏 显示路径栏 网络设置 为系统设置 DNS，选择可靠 DNS 服务：\n阿里云 DNS 223.5.5.5 和 223.5.5.5。 腾讯云 DNS 119.29.29.29 ","date":"2022-07-23T14:51:18+08:00","permalink":"https://mahaoliang.tech/p/macos-%E7%9A%84%E7%B3%BB%E7%BB%9F%E5%BB%BA%E8%AE%AE%E8%AE%BE%E7%BD%AE/","title":"macOS 的系统建议设置"}]